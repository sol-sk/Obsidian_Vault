#philosophy 
#cogs 

[[PHIL 351]]

10-10-23 

### Pre class readings
Chapter 2, sections 4-5 [[The Philosophy of Cognitive Science (Cain, Mark J) (Z-Library).pdf]] 
Chapter 5, [[The Promise Of Artificial Intelligence Reckoning And Judgment (Brian Cantwell Smith) (Z-Library).pdf]]
## Lecture

## Review
### Fodor
**Fodor** hybridizes vertical and horizontal models; these organizations both apply to different cognitive systems
	Input/peripheral systems = vertical
	"Central"/cognitive = horizontal
Modules moderate sensory input and output info that central cognition can deal with ("early stages" of cognition)
#### Modular vs non-modular systems
i) **Cognitively impenetrable/informationally encapsulated**
	Internal operations are restricted to the *incoming input* and *internally stored information* (proprietary database)
		"Reflex-like" because it does not integrate, it can only work within itself
		It cannot receive or employ information from other parts of the cognitive system!
	Accessibility relations: central cognition has access to output, but it cannot perceive the internal system/computational states/operations (black box)
**Example:**
![[Screen Shot 2023-10-15 at 12.39.57 PM.png]]

ii) **Domain-specificity**
	The inputs to a module are restricted to a particular subject matter which constrains the range of questions to which the module can potentially provide an answer
	If a computational system can answer a wide range of questions, it is domain *general*
iii) **Mandatory**
	Once the inputs are received, the module's algorithms operate automatically and involuntarily
iv) **Speed: Fast**
v) **Specific breakdown patterns**
vi) **Specific developmental pace and sequencing**

Modules do not need to have *all* of these features, but have most or all of them, and encapsulation and domain specificity are the most important

**Central systems**
- Responsible for belief-fixation via non-demonstrative (non-deductive) inference: abduction/inference to the best explanation–informational "un-encapsulation"
- Central systems are non-modular, because they are informationally un-encapsulated/cognitively penetrable (promiscuous). They are domain general, slow, sequential, and under conscious control
- *Person-level cognition*

#### Assessing LOT
Distinguish: 
- Questions about LOT as an account of central cognition
- Questions about LOT as an account of peripheral cognition: the elvel that it is natural to associate with sub personal subsystems
Modularity offers a way, at least in principle, to explain the seeming ineffability of peripheral (perceptual and sensorimotor) systems, since the internal states of modules are inaccessible to central cognition and so to verbal report and reasoning

"trust me, these inaccessible modules are computing, we just can't see it"

Option 1: Bring back LOT
Young, new scientists are still in favor of LOT: resurgence 

Option 2: Non-classical computational architectures
- artificial neural networks
- connectionism
- parallel distributed processing
- the sub-symbolic paradigm in artificial intelligence

### Artificial neural networks
#### Connectionism
> In 1940s, as knowledge of brain was advancing and computationalist models of the mind were being formulated, certain questions about the brain were being explored:
- what are neurons doing (computationally?)
- how do synapses affect neural activity?
- how do neural networks perform useful cognitive tasks?
- how do neural networks evolve and learn?

**Churchland**: We should be ready to accept that revisions to cognitive models may be necessary

#### Historical roots of ANNs
> McCulloch & Pitts (1949)
> 	proof that neural nets can perform any Turing computable task
> 	simple binary threshold model that suggested account of what neurons do and how synapses work
> Rosenblatt's (1958)
> 	two layer net called "perceptron"
> 	*Learning*, drawing from Hebb "neurons that fire together wire together"
> Minsky & Papert (1969)
> 	Showed that Rosenblatt's "perceptrons" couldn't compute ⊕ (XOR) and it wasn't obvious that this could be solved by adding more layers


#### Rise of connectionism
> Many nodes interact via weighted connections, which can be inhibitory or excitatory 
> 	Further modeling from brain
> No central controller or memory/storage system
> 	Short term memory is "stored" in a unity's changing state of activation
> 	Long term memory is stored in the relatively enduring strength of the connection weights between units


