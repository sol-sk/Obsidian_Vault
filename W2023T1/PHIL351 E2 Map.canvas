{
	"nodes":[
		{"id":"94be040cd3971c65","type":"group","x":-5798,"y":-1552,"width":4307,"height":1668,"label":"Solving the Frame problem"},
		{"id":"c349cf951cf986e2","type":"group","x":3120,"y":2013,"width":3619,"height":1539,"label":"Intentionality, Computationalism, Searle"},
		{"id":"25e10d2bc508fadd","type":"group","x":1755,"y":-2231,"width":3496,"height":1247,"label":"ANNs"},
		{"id":"569b52153133f25f","type":"group","x":2235,"y":-3000,"width":4092,"height":596,"label":"ANN vs LOT"},
		{"id":"84d83fe9bf0df6af","type":"group","x":4589,"y":-9,"width":1465,"height":658,"label":"Cappellen and Dever"},
		{"id":"230ee30239a6cac4","type":"group","x":-725,"y":-5080,"width":3620,"height":1844,"label":"LOT"},
		{"id":"145eaf3afd5a5a86","type":"group","x":-725,"y":-2120,"width":2091,"height":1136,"label":"Smith on 1st and 2nd wave AI"},
		{"id":"db596b5b9cff9861","type":"group","x":-2854,"y":1485,"width":1356,"height":1056,"label":"Phenomenal overflow"},
		{"id":"974858e448a679ca","type":"group","x":-4729,"y":1977,"width":1383,"height":564,"label":"Hard problem of consciousness"},
		{"id":"8a726e184cac3d67","type":"group","x":-1498,"y":476,"width":766,"height":858,"label":"Concepts of consciousness"},
		{"id":"5728d2ec6a9200ab","type":"group","x":-8920,"y":-1120,"width":2210,"height":953,"label":"Frame Problem"},
		{"id":"3f35b010e56fba0a","type":"group","x":-6468,"y":-2647,"width":1889,"height":527,"label":"The Robot Parable"},
		{"id":"ddf328247a0afe3d","type":"group","x":-8960,"y":-2120,"width":1227,"height":701,"label":"Central Systems"},
		{"id":"32228c7ef8f1573d","type":"group","x":320,"y":320,"width":2233,"height":2028,"label":"Theories of mental content"},
		{"id":"aead94c25c2cc967","type":"text","text":"## Intentional systems theory and interpretivism\n- Anchors our account of intentional content in the 3rd-personal perspective of an *interpreter*\n- Content is determined by the *most charitable/rational* interpretation of an agent's behavior \n- Mental states aren't unobservable internal causes of behavior but observable patterns in prediction and explanation\n\t- Not speculation: determining best explanation\n- Strongly **externalist**: depends on individual's environment\n\nSometimes the distinction between original and derived is challenged (Dennett)\n","x":1085,"y":1571,"width":669,"height":357,"color":"#5a8cdd"},
		{"id":"7bf460247d96c310","type":"text","text":"#### Historical roots of ANNs\n- McCulloch & Pitts (1949)\n\t- proof that neural nets can perform any Turing computable task\n\t- simple binary threshold model that suggested account of what neurons do and how synapses work\n- Rosenblatt's (1958)\n\t- two layer net called \"perceptron\"\n\t- *Learning*, drawing from Hebb \"neurons that fire together wire together\"\n- Minsky & Papert (1969)\n\t- Showed that Rosenblatt's \"perceptrons\" couldn't compute ⊕ (XOR) and it wasn't obvious that this could be solved by adding more layers","x":2195,"y":-1687,"width":630,"height":424,"color":"3"},
		{"id":"4aa89f6313a21e1c","type":"text","text":"#### Connectionism\n> In 1940s, as knowledge of brain was advancing and computationalist models of the mind were being formulated, certain questions about the brain were being explored:\n- what are neurons doing (computationally?)\n- how do synapses affect neural activity?\n- how do neural networks perform useful cognitive tasks?\n- how do neural networks evolve and learn?\n\n**Churchland**: We should be ready to accept that revisions to cognitive models may be necessary\n","x":1835,"y":-2087,"width":630,"height":372,"color":"#8cff00"},
		{"id":"0e88e211636533f3","type":"text","text":"#### Morals of 2nd wave AI: The good\n> Successes of ML architectures suggest that a vastly rich and likely ineffable web of statistical relatedness weaves the world together into an integrated 'subconceptual' whole. That is the world with which intelligence must come to grip. \n> p. 64\n- Reality itself surpasses an intelligent being's capacity to conceptualize and verbally articulate it\n- Intelligent systems must somehow 'come to grips' with world on non-conceptual level\n- Intelligent systems must somehow abstract concepts from basic nonconcepts\n- Objects, or ontologically determinate entities, are emergent from a richer metaphysical 'plenum' (the philosophical idea that no part of reality is empty of matter)\n","x":350,"y":-1826,"width":996,"height":331,"color":"#4258ff"},
		{"id":"d4815f7d587250c9","type":"text","text":"#### Phenomenal intentionality\n- anchors our concept of intentionality in first-person reflection/introspection on one's conscious states\n- the intentional content of a mental state **is given by a mental state's subjective or phenomenal character**\n- often strongly 'internalist' about mental content (ie. the representational content of a mental state is 'narrow', not constitutively dependent on features of the individual's environment)\n- Morals for the Chinese room argument?","x":1085,"y":732,"width":668,"height":312,"color":"#d57cfe"},
		{"id":"10b827ad822048e0","type":"text","text":"##### Standard worries for these views:\n- Anti-realism? Realistic to just say \"it makes most sense for system to be thinking\"?\n- What is the status of this interpreter? Theory does not include the perspective of theorist","x":360,"y":1557,"width":668,"height":192},
		{"id":"cc71e41f93173e1a","type":"text","text":"#### Morals of 2nd wave AI: The bad\n- Similar to GOFAI error; cannot explain how a system comes to *understand* what it is representing. To achieve understanding (and genuine intelligence) Smith suggests the system would need to be capable of assuming a stance of deference to the world: \n> At the outset I said that semantics, in order to be semantics, must be *deferential.* At the moment, althought we may design our systems with deferential semantics, *the deference is ours*, not theirs. If we are going to build a system that is itself genuinely intelligent, that knows what it is talking about, we have to build one that is *itself* deferential–that *itself* submits to the world it inhavits, and does not merely behave in ways that accord with our human deference. To do that, it will have to know i) that there is a world, ii) that its representations are about that world, and iii) that it and its representations must defer to the world that they represent\n> p.79\n","x":350,"y":-1454,"width":996,"height":367,"color":"#4258ff"},
		{"id":"a08c5ab18c9635ea","type":"text","text":"### Classical vs. nonclassical views of cognitive architecture\n- LOT\n- ANN/connectionism\n\t- Respects in which ANNs and LOT differ\n- The argument from systematicity for the claim that the human mind has a LOT structure.\n- One reply to the argument from systematicity. (Note: there are multiple coherent strategies open to an ANN theorist. You are likely to be asked to outline one coherent strategy).\n- Theoretical morals Smith draws from 1st and 2nd wave AI\n\n","x":607,"y":-2653,"width":743,"height":340,"color":"#006eff"},
		{"id":"e532edd14e38a572","type":"text","text":"Subsystems = black box (agent lacks direct access)\nHowever, within these subsystems, there are computations over mental representations (symbols)\n> If so, LOT theorists can accept that, in general, knowing how to do *x* doesn't imply knowing how to *verbally explain* how to do *x*, while also affirming that knowledgeable *x*-ing consists in the formal manipulation of symbols","x":1339,"y":-3676,"width":427,"height":336,"color":"4"},
		{"id":"118a0cd89d43c717","type":"text","text":"Distinction between deciding what to eat for lunch (which has identifiable premises and inferences) and the inferences the motor system executes to adjust your grip and movement towards a target\n\n**Distinction between two kinds of cognitive state/process**","x":436,"y":-3821,"width":354,"height":273,"color":"#527dff"},
		{"id":"922f14d55acaef03","type":"text","text":"> Refers to the language of thought as \"Mentalese\" to denote its separation from public language (internal language)\n","x":341,"y":-4857,"width":286,"height":157,"color":"#527dff"},
		{"id":"f0cdc8be1a3b9cc0","type":"text","text":"#### **Tacit knowledge**\nKnowledge (aka representation) which is *inaccessible* to the person/agent ","x":933,"y":-4440,"width":293,"height":184,"color":"4"},
		{"id":"88b7629776002716","type":"text","text":"##### F1: Neurological \n> \"The brain does not work the way that GOFAI does. AI is extremely fast, but consists of relatively few serial loci of activity / Brain operations are roughly 50 million times slower, deriving their power from massive parallelism.\"\n","x":1353,"y":-4920,"width":680,"height":148,"color":"2"},
		{"id":"4b9e0ed24c9c4175","type":"text","text":"## Worries about classicism\nPaul Churchland: \n> Classicism overstates the centrality of language to intelligence. Consequently, it explains only a small fraction of human intelligence and approximately none of nonhuman intelligence","x":673,"y":-4952,"width":536,"height":241,"color":"2"},
		{"id":"fd44d4d65cd61c32","type":"text","text":"##### F3: Epistemological\n> \"Thinking and intelligence, on the GOFAI model, consisted of rational, articulated steps, on its founding model of logical inference ... in many situations a more accurate characterization of intelligence is one of skillful coping or navigation*–of being 'thrown' into individual and social projects in which we are embedded and enmeshed* ... emerges a horizon of *ineffable* knowledge and sense-making.\" (p.27)","x":1353,"y":-4586,"width":613,"height":245,"color":"2"},
		{"id":"783d72379c42b8fd","type":"text","text":"# Objections to LOT","x":1148,"y":-5060,"width":245,"height":100,"color":"2"},
		{"id":"08085372c4c18013","type":"text","text":"Modularity offers a way, at least in principle, to explain the seeming ineffability of peripheral (perceptual and sensorimotor) systems, since the internal states of modules are inaccessible to central cognition and so to verbal report and reasoning\n","x":2295,"y":-4267,"width":480,"height":156,"color":"5"},
		{"id":"cfa2a4de29eebe6c","type":"text","text":"## Classicism response to these objections:\n\n","x":753,"y":-4552,"width":395,"height":89,"color":"4"},
		{"id":"9dff61e9d777011c","type":"text","text":"Personal: conscious, \"common sense,\" top-level states/propositional attitudes","x":368,"y":-4440,"width":305,"height":127,"color":"#527dff"},
		{"id":"1a6482f901407ca3","type":"text","text":"##### F2: Perceptual \n> \"Perception would merely involve determining what objects were 'out there,' what properties they manifested, what types or categories they belong to, and so on. It did not work out that way.\" (p.24)","x":1353,"y":-4757,"width":613,"height":147,"color":"2"},
		{"id":"d5ec8fcfc077b755","type":"text","text":"#### Expertise / \"know how\"\nHow do we express non-verbal knowledge in LOT? ie tying shoes is easily accessed and difficult to describe verbally","x":836,"y":-4683,"width":487,"height":114,"color":"2"},
		{"id":"69fdbb8b8e8f967c","type":"text","text":"Interpretivism → Systems reply","x":1253,"y":1964,"width":334,"height":66,"color":"#5a8cdd"},
		{"id":"2b0637cc5d3fb18f","type":"text","text":"# Theories of mental content","x":1645,"y":593,"width":437,"height":75,"color":"#ffbb00"},
		{"id":"275be856562795f1","type":"text","text":"##### Phenomenal intentionality vs causal theorists\nCausal theorists (tracking) would argue w/ phenom. about the order of explanation between content and consciousness\n- Consciousness-first theorists: the representational properties of a mental state are intrinsically properties of the mental state → consciousness explains content/aboutness (Searle)\n- Content-first theorists: the phenomenal properties of a mantal state are just representational properties of a mental state that satisfy further causal conditions → content explains consciousness","x":1140,"y":1120,"width":558,"height":350,"color":"#d57cfe"},
		{"id":"01b15abf1c71845e","type":"text","text":"### Chinese Room experiment response\n- Systems reply; system does understand bc this is most useful interpretation\n- Other interpretivists may insist on more context which are absent in thought experiment (historical, env, causal-functional factors)\n\n","x":1420,"y":2152,"width":669,"height":159},
		{"id":"a8299bd5a11ba753","type":"text","text":"##### Charles Peirce 'abduction' (inference to the best explanation) \n-> Contrast with deduction and induction (narrowly understood)\n- Reasoning (whether practical or theoretical, scientific, everyday) is a temporally extended process of inquiry motivated by a practical aim\nStages of abduction in reasoning: \n- Notice an anomaly, violation of expectation, obstacle to action\n- Pose a question, framed in relation to one's interests\n- Make an inference: hypothesize (leaps to instinctive conclusion)\n- Experiment in trial and error manner\n- If all goes wall, one arrives at a workable but still defeasible (defn: open in principle to revision) explanation that facilitates more fluent interaction with the world","x":-8089,"y":-930,"width":643,"height":426,"color":"3"},
		{"id":"447d2e7d31a27e21","type":"text","text":"##### Later: Quine, Goodman\n- There is no perfect experiment to test an isolated hypothesis. We use all of our beliefs to interpret an observation\n- Depending on how integrated a belief is within belief system, we may treat a false prediction as evidence against the belief in question or as evidence of measurement error\n- **Moral: theory confirmation is holistic and un-codifiable**\n","x":-7373,"y":-840,"width":643,"height":240,"color":"3"},
		{"id":"670effcf6311bd1d","type":"text","text":"### The frame problem \n\n- The phrase 'frame problem' is contentious (Shanahan)\n- Originally referred to a technical problem in GOFAI abt modelling non-monotonic (revisable, mutable) reasoning without requiring the system to update every single belief: only updating relevant beliefs\n- Later, 'frame problem' = problem of explaining 'common-sense' and sensitivity to relevance\n- Dietrich et al: computer scientists are reluctant to admit they are dealing w/ a philosophical problem \n\n**Purports to explain why AI research has been unsuccessful so far in creating truly intelligent systems**\n\nWhat can and can't computers do? ","x":-8900,"y":-1100,"width":625,"height":421,"color":"5"},
		{"id":"f12151ee4b8f48ae","type":"text","text":"##### Paradigm shift #1: ANNs\n- May seem more promising architecture than classical GOFAI for addressing *frame problem*\n\t- Greater ease dealing w context and global properties of stim.i\n\t- Memory is an evolving state of the network implicitly in the connection weights\n- However, insufficient to solve the frame problem, esp. relevance problem\n- ANNs tend to assimilate novel cases to pre-existing categories: \"overfitting\" (see: adversarial examples)\n\t- Product of their reliance on statistics \n","x":-5778,"y":-1377,"width":695,"height":342,"color":"#3a3fe4"},
		{"id":"332e37206e4984f7","type":"text","text":"##### Critiques of enactive and ecological approaches\n(a new kind of) Frame problem:\n- **How does the organism become attuned to only the relevant affordances?**\n- Is biological agency sufficient for cognitive agency?\n\t- Intelligence: when something that matters to an organism is not causally coupled to it \n\t- Agents engage w world as objective, not just an arena for action (?)\n\t\t- → Whatever sense that we make of the world should be true to 'reality'\n\n","x":-4520,"y":-1524,"width":673,"height":329,"color":"#d45454"},
		{"id":"a869e009b395a194","type":"text","text":"##### Dennett's parable doesn't even raise the issue of *non-effects*\n- **Non-effects:** things that have not been changed do not change if something else changes (my hair color does not change when I enter a new room)","x":-5608,"y":-2627,"width":428,"height":213,"color":"#ffb22e"},
		{"id":"7d671eb0069bfd43","type":"text","text":"##### Karl Popper\n- Scientific method is process of conjecture and refutation (**falsification**)\n- Conjecture is a creative leap of the imagination (unsupported guess-work), followed by a rigorous empirical investigation (logically derive a bold prediction, then test it via controlled experimentation)\n- Fallibilism: empirical theories can never be conclusively confirmed but only disconfirmed. Science progresses through the incrememntal elimination of falsity.","x":-8089,"y":-470,"width":643,"height":283,"color":"3"},
		{"id":"00189d154640f69f","type":"text","text":"##### Application to everyday reasoning\n- The ease and speed of an inference is not an indication of its triviality\n- A special case of everyday abductive reasoning: interpreting another agent's actions, including communicative intentions. \n- We need to have the same background of relevant info in order to understand what a person says or does (see: pragmatics)\n\n","x":-7306,"y":-551,"width":510,"height":309,"color":"3"},
		{"id":"704953c9e174f61d","type":"text","text":"### Historical background: The role of abduction in scientific reasoning\n\n\n\n\n\n","x":-8117,"y":-1051,"width":477,"height":76,"color":"3"},
		{"id":"ae60f19007a26ece","type":"text","text":"Relationship of abduction to the frame problem?","x":-8440,"y":-651,"width":250,"height":60,"color":"1"},
		{"id":"528d0efb8fd09954","type":"text","text":"### Paradigm shift #2: 4E cognitive science\n> **4E: embodied, embedded, extended, and enactive** \n\n4E cog scientists: **temporal dynamics** are one of most important properties of cognition\n→ \"off-loading\" cognitive labor to agent's *body* and environment\n- More radical 4E (enactivism, ecological psych): reject the idea that cognition can be understood as computation bc this misinterps the temporal structure of cognition\n","x":-5778,"y":-1001,"width":695,"height":312,"color":"#db57ff"},
		{"id":"5750fa742951ed19","type":"text","text":"#### Situated robotics\nRodney Brooks' 'subsumption architecture': provides alternative to iterated sequences like SHAKEY\n- Brooks' robots: sensors are directly wired to various activity layers (e.g. wander, avoid, explore) which would compete via mutual inhibition\n- When an activity layer wins out, the sensors directly and continuously drive behavior proprietary to that layer, no mediating model/plan\n\nResult: smoother in cluttered env., flexible in goal directed ways\n\nBrooks claimed his robots did not have any use for representations","x":-5778,"y":-645,"width":695,"height":354,"color":"#e2ca32"},
		{"id":"f5a519e577d24f87","type":"text","text":"### Dynamical systems theory\nA branch of mathematics used to model a system whose behavior changes continuously over time, including patterns of stability, instability, and meta-stability.\nAn alternative set of formal tools for desc the activity of ANNs and autonomous robots\nApplications to human + animal cognition: \n- outfielder problem;\n- the diving gannet, whose behavior can be described mathematically with tau\n- modelling cognitive development in infancy","x":-5778,"y":-231,"width":695,"height":327,"color":"#914040"},
		{"id":"bb9452899c8c72cc","type":"text","text":"### Thinking through coupling\nMind/cognititon spans the gap between agent and environment\n\n> It is only for convenience (and from habit) that we think of the organism and environment as separate; in fact, they are best thought of as comprising just one system” \n> Chemero 2001, p.142\n\n\n> “The emergence of mind takes place in the medium of patterns of activation across neuronal cell assemblies in conjunction with the interaction of their attached sensors (eyes, ears, etc.) and effectors (hands, speech apparatus, etc.) with the environment in which they are embedded. Make no mistake about it, *that* is the stuff of which human minds are made: brains, bodies, and environments.” \n> Spivey 2007, 33, emphasis original\n","x":-5024,"y":-645,"width":695,"height":414,"color":"#b06363"},
		{"id":"d1e3d3e834f4cd7d","type":"text","text":"> One feels that there should be some economical and principled way of succinctly saying what changes an action makes, without having to explicitly list all the things it doesn’t change as well; yet there doesn’t seem to be another way to do it. That is the frame problem.” \n> \n> Hayes, quoted in Dietrich et al., p. 147","x":-5108,"y":-2520,"width":509,"height":200,"color":"#ffb22e"},
		{"id":"f6436a15f4388f7e","type":"text","text":"## A change of paradigm? ","x":-5610,"y":-1479,"width":360,"height":55,"color":"2"},
		{"id":"1506e361ebee45d3","type":"text","text":"#### Enactive and ecological cognitive science (radical 4E)\n- Both meaning and relevance are 'enacted' through active self-maintenance (autopoiesis)\n- Ecological psychologists: the info organisms pay attention to is 'ecological information', which is relative to its morphology\n- **Common theme**: properties like meaning and relevance do not belong to internal states of an agent (e.g. representations of mind-independent objects and properties) but to an organism's *milieu*: agent and environment are *complementary aspects of an integrated system*\n- If so, cognitive agents do not need to solve the frame problem since, in virtue of being alive, they already inhabit a world of salience and significance\n\t- Elements of the env stand out as 'affordances' with an hinherent bio significance to the organism","x":-4977,"y":-1157,"width":602,"height":468,"color":"#fe20eb"},
		{"id":"08821d630bd96602","type":"text","text":"In his The Mind Doesn’t Work That Way (2000), Fodor argues (among other things) that the frame problem re-arises for these accounts: when a situation has features that are consistent with many different task domains, how is the relevant module selected? That is, how does the system determine what context it is in?  \n• Cf. Dreyfus’ *What Computers Can’t Do*","x":-3825,"y":-802,"width":425,"height":254,"color":"6"},
		{"id":"b2e31e5c745a3185","type":"text","text":"#### Key assumptions made by GWS and HOT theorists\n- Central task for theory of consciousness is to abstractly explain what consciousness is and show how that abstraction could be realized architecturally and physically (Marr)\n- In formulating an abstract description of what consciousness is/does, theorists should take as their primary evidence subject's verbal reports about their conscious states \n- Consciousness is a higher cognitive phenomenon\n\n**But! These are not all universally accepted**\n","x":-2157,"y":-305,"width":621,"height":326,"color":"#fe9467"},
		{"id":"ddc4d99044254a1f","type":"text","text":"#### Synthesizing GWT and HOT\n**Content that gains entry to the global workspace becomes objects of meta-representation**\n- States can orchestrate activity within the broader cognitive system while remaining a potential site of metacognitive intervention","x":-2125,"y":-581,"width":556,"height":249,"color":"#fe9467"},
		{"id":"173194fc7fbf1865","type":"text","text":"##### Higher order (HO) theories of consciousness and self-modeling \nA conscious state is a mental state that one is conscious of \n- The higher order representation (meta-representation) might be a thought (HOT) or perception (HOP)\n- The higher order representation might be the same as first-order representation (self-representing), or distinct \n\nRelated: conscious states are embedded with a representation of the self within the content\n\nCommon theme: consciousness is not a mere 2-place relation between X (a representation) and Y (a represented object) but a 3-place relation between X, Y, and the subject to or for whom Y is represented via X","x":-3010,"y":-659,"width":680,"height":405,"color":"#d6fe67"},
		{"id":"e848588e02ec0b68","type":"text","text":"#### The robot parable (Dennett)\n- R1: needs to rescue its spare battery. Removes it from room with bomb, but pulls the bomb out with the battery. \n- R1D1: programmed to understand all implications of its actions. needs to rescue its spare battery. bomb explodes before it can figure out all of the possible implications\n- R2D1: programmed to understand all implications and ignore irrelevant ones. needs to rescue its spare battery. bomb explodes before it can figure out what implications are relevant. \n- R2D2: ?\n","x":-6448,"y":-2627,"width":501,"height":414,"color":"#ffb22e"},
		{"id":"1ad4827e79575aee","type":"text","text":"#### Robot parable cont. \n- Cannot understand the **effects** of its actions\n- **Cannot ignore what it knows**\n- Search space for real-world problems is vast → need to have a selective search strategy (heuristics and biases) to avoid **'combinatorial explosion'**\n\n","x":-5828,"y":-2364,"width":580,"height":224,"color":"#ffb22e"},
		{"id":"26ea681d9e88549e","type":"text","text":"### Central Systems\n","x":-8940,"y":-2100,"width":241,"height":50,"color":"4"},
		{"id":"7215fab98f9e5a15","type":"text","text":"##### **Fodor gives us two morals from this:**\n1. The closer we get to higher levels of cognitive capacity, the more global properties like isotropy show up → global properties are necessary for paradigmatic intelligence\n**\"Fodor's first law of the nonexistence of cognitive science\":**\n2. The more global (isotropic) a cognitive process is, (e.g. analogical reasoning,) the less anybody understands it.","x":-8216,"y":-2050,"width":463,"height":332,"color":"4"},
		{"id":"35ba2c7ffc27482a","type":"text","text":"### Fodor\n- While modules are plausibly computational, *central systems are not* - Narrow vs general AI ","x":-8900,"y":-2008,"width":496,"height":124,"color":"4"},
		{"id":"941ad4efecdac94b","type":"text","text":"#### **Two entertwined properties of central cognitive states which resist computational explanation**\n1. Being '**Quinean**': epistemic/rational properties of an attitude that are relative to a larger set of attitudes: \n\t1. Coherence, simplicity, plausibility (all relative to whole belief system) → Generates the 'globality problem': computational operations are sensitive ot a mental state's local (syntactic) properties, but not to its global/Quinean properties\n2. Being **Isotropic**: any member of an attitude set is potentially relevant to any other\n\t1. Analogical reasoning (metaphor/simile)\n\t2. Isotropism underlies 'the relevance problem': computational operations are insensitive to relevance","x":-8900,"y":-1860,"width":620,"height":421,"color":"4"},
		{"id":"4d1b3cb7b46d0616","type":"text","text":"### The problem of relevance (the frame problem)\n- Dennett’s parable of the robot\n- Relation of the frame problem to central systems and the holistic character of abduction/inference to the best explanation\n- One attempted solution to the frame problem: e.g., connectionism, 4E cognitive science, massive modularity (which retains classical computationalism but denies that the mind includes informationally unencapsulated processes), global workspace theory","x":-7297,"y":-1764,"width":743,"height":288,"color":"3"},
		{"id":"180b775877b67619","type":"text","text":"**Phenomenal conscious seems fundamentally different from other theories of consciousness**\n- We understand it in *first person terms*: all other theories thus far (and scientific theories in general) are expressed in *third person*","x":-2834,"y":1969,"width":421,"height":245,"color":"#ff6f0f"},
		{"id":"3c61bba1129e7740","type":"text","text":"**Mind as a 'bag of tricks' or 'Swiss army knife'**\n**No domain-general cognition**\n\n**Content specific reasoning:** \n- We are much better at using the exact same logic if it is presented in a domain we are more familiar with (2/7/E/Z puzzle)\n\n","x":-3832,"y":-1105,"width":438,"height":240,"color":"6"},
		{"id":"808e04b0777d04ef","type":"text","text":"## Massive Modularity\n","x":-3766,"y":-1228,"width":306,"height":50,"color":"6"},
		{"id":"f014c3cfa372c879","type":"text","text":"## Consciousness","x":-2750,"y":-1136,"width":270,"height":51,"color":"#67fe78"},
		{"id":"3fabae7c28b7717b","type":"text","text":"These two challenges have opened the door to some surprising scientific theories of phenom. consc.\n- Integrated information (Φ) theories (Tononi, Koch)\n- 'Axioms' (Intrinsicality, Information, Integration, Exclusion, Composition), and corresponding physical postulates\n\t- Has specific mathematical and physical postulates about where consciousness is\n\t- \"Back of the head\" theory\n- Implies that all intrinsic physical systems possess some degree of consciousness (panpsychism)\n- May have even more radical implications about which entities exist; only those with maximal Φ, with all parts of the system existing only in a relative sense (namely, as objects for some conscious subject)\n\t- Idealism? ","x":-4709,"y":1997,"width":572,"height":468,"color":"1"},
		{"id":"4ef503b7716408bc","type":"text","text":"#### Responses to the hard problem\n**Non-physicalist responses:** \n\tAccept the explanatory gap between physical and phenomenal properties and infer a metaphysical gap. Consciousness is fundamentally nonphysical\n\n**Physicalist rejoinders:** \n\ti) reject the putative explanatory gap\n\tii) accept the explanatory gap but deny the metaphysical gap. ","x":-4038,"y":2231,"width":626,"height":293,"color":"1"},
		{"id":"208d9773b7e55dd2","type":"text","text":"### The problem of consciousness  \n- Global Workspace (GWS) theory  \n- Higher Order (HO) theories  \n- Concepts of consciousness, especially phenomenal consciousness, access consciousness, and higher order consciousness (/self-consciousness)  \n- How GWS and HO theories see consciousness as straightforwardly computational (e.g., as ‘access’ and/or higher order representation)  \n- The methodological challenge posed by the possibility of ‘phenomenal overflow’  \n- The hard problem of consciousness  \n- Responses to the hard problem (both physicalist and anti-physicalist)","x":-3549,"y":444,"width":743,"height":347,"color":"6"},
		{"id":"030c66b93843455e","type":"text","text":"#### **The hard problem of consciousness:** No theory which uses purely third-personal, objective terms can explain what is essentially first-personal and subjective","x":-3990,"y":2044,"width":529,"height":149,"color":"1"},
		{"id":"cbd2d6660af198af","type":"text","text":"#### Phenomenal overflow\nWhen the bandwidth of phenomenal consciousness exeeds/outstrips the bandwidth of cognitive access. \nSimilarly, if phenomenal consciousness goes beyond the content of self-awareness\n","x":-2360,"y":1686,"width":471,"height":183,"color":"#ff6f0f"},
		{"id":"99950eedd92995ff","type":"text","text":"#### But what about phenomenal overflow?","x":-2110,"y":1544,"width":443,"height":72,"color":"#ff6f0f"},
		{"id":"271d6f0f4b6c32e9","type":"text","text":"### **Block's 'methodological challenge'**\n- How do you scientifically investigate whether phenomenal consciousness overflows access consciousness when out methods for detecting consciousness depend on access consciousness?\n\nGWS's challenge: how to justify restriction of phenomenal consciousness to access conscious without begging the question (eg. relying on verbal reports)\n\nBlock's moral: GWS theories fail because they link phenom. consc. with central cogntition\n- We perceive a whole stim but can only consciously access part of it\n","x":-2280,"y":1969,"width":656,"height":382,"color":"#ff6f0f"},
		{"id":"d35d552f9de029c3","type":"text","text":"HO's challenge? \n- Immersion and flow state: phenom. consc. experiences which occur without self-consciousness","x":-2010,"y":2374,"width":386,"height":148,"color":"#ff6f0f"},
		{"id":"ead1f047bfcfa232","type":"text","text":"![[Screen Shot 2023-11-27 at 8.03.48 PM.png]]","x":-2210,"y":-1504,"width":603,"height":241,"color":"#28c893"},
		{"id":"c98c4ffc61be521d","type":"text","text":"##### Global workspace theory (GWT)\nHumans do engage in domain general cognition (against massive modularity)\n\n**Avoiding combinatorial explosion**\n\n- Parallel processes: global processing thru modules\n- 'Strongest' response (because of its relevance) gets transmitted back to global workspace\n\n- Cycle of flexible cognitive processing bc result of \"winning\" module gets globally broadcast, processed, then result of next module, etc. \n\n- *Globality* is explained by the integrative coordinative effects of global broadcasting (thru 'small-world' neural architecture)\n\n- *Relevance* is explained by *competitive* interactions between modules in response to the globally broadcast content\n\n- **By what process is the competition's 'winner' determined and allowed access to the global workspace?**\n\n- The standard answer invokes '**attentional mechanism**' as gatekeeper: but attention must be able to *detect relevance*. How?","x":-2249,"y":-1204,"width":680,"height":529,"color":"#28c893"},
		{"id":"9acde8d96af4a4bf","type":"text","text":"#### Potential relevance to the frame problem? \nCognitive agents often fall victim to the frame problem when they fail to notice what's relevant about their situation: often bc of functional fixedness/being locked into a certain framing\n\nEmbedding a rep. of oneself/one's cog perspective on a problem within one's overall rep of the problem might afford opportunities to reflectively step back from one's cog framing to assess its appropriateness to task at hand and adjust their perspective: cog. restructuring → cog flexibility\n","x":-3335,"y":-1085,"width":529,"height":344,"color":"#67fe78"},
		{"id":"2886e85b67b21719","type":"text","text":"Human cognitive architecture is peripheral and central - We have the disctinctive flexibility of central systems bc of *consciousness*","x":-3290,"y":-1219,"width":438,"height":103,"color":"#67fe78"},
		{"id":"8af2d9e3b45be1e7","type":"text","text":"#### State consciousness\n- Consciousness is a property of a mental state\n\n","x":-1478,"y":800,"width":602,"height":90,"color":"6"},
		{"id":"636e2c7b7888483d","type":"text","text":"## Smith's conclusions on 1st and 2nd wave AI","x":165,"y":-2100,"width":371,"height":80,"color":"#4258ff"},
		{"id":"fadb274fa0f8336f","type":"text","text":"#### Types of state consciousness:\n##### Self-consciousness: being conscious of a mental state as such\n##### Access consciousness: a mental state available for the global control of action (reasoning, intentional action)\n##### Phenomenal consciousness: a mental state that there is something it is like for the subject to be in; the phenomenal character of this state is how it 'feels' or 'what it is like' ('POV:')","x":-1478,"y":920,"width":664,"height":374,"color":"6"},
		{"id":"e5670bc1ea732ab0","type":"text","text":"**HO theories seem to explain self-consciousness, GWS explains access consciousness, HO-GWS explains both** ","x":-1469,"y":519,"width":416,"height":95,"color":"6"},
		{"id":"5a560fdd4913d758","type":"text","text":"##### Systematicity\nLOT is *generative*:\n*Systematicity*: if someone understands a complex expression, they will understand a diff complex expression which comprises the same constituents","x":-272,"y":-4264,"width":558,"height":189,"color":"#527dff"},
		{"id":"949d457d33203b69","type":"text","text":"##### Creature consciousness\n- Consciousness is a property of a creature/organism/person","x":-1478,"y":681,"width":602,"height":88,"color":"6"},
		{"id":"8d9858d9937fa77e","type":"text","text":"### Fodor","x":301,"y":-4927,"width":135,"height":50,"color":"#527dff"},
		{"id":"880f78e590b56199","type":"text","text":"#### Morals of 1st wave AI (GOFAI): The good\n> **Representational Mandate:** The proper functioning of any world-directed system–any system that is thinking about or representing or processing information about the world–must be governed by **normative criteria**, applying to its **mechanical operations**, that are framed in terms of situations and states of affairs **in the world** that the system is **representing or reasoning about**, which situations and states of affairs will not, in the general case, be within effective (causal) reach. \n> Smith p. 17\n\n**→ Intentional system: is able to reason about something that is distal** \n","x":-705,"y":-1826,"width":996,"height":295,"color":"#4258ff"},
		{"id":"ac301320efcfdb28","type":"text","text":"#### Morals of 1st wave AI (GOFAI): The bad\n**Took objects for granted**\n> It is naive to assume that \"the world comes chopped up into neat, ontologically discrete objects. In fact one of the primary theses of this book is that the misconception of the world in first-wave AI is the 'smoking gun' that explains its ultimate inadequacy... Representations, descriptions, models, and the like all interpret or picture or filter the world through abstractions or idealizations–conceptual 'frames' that highlight or privilege some aspects of what is represented, minimize (or even distort) others, and ignore or abstract away from a potentially unbounded amount of in-the-world detail\"\n> pp. 28-29\n\n\n\n\n\nHow does a system parse a world? How does it discover what the world is?\n→ An object ontology (the world as we understand it) is a cognitive achievement: something constructed rather than pre-given\n- Corollary: we cannot assume that any intelligent system that we build will parse reality as we do. As an intelligent system, it will have to make its own sense of things\n","x":-705,"y":-1487,"width":996,"height":483,"color":"#4258ff"},
		{"id":"440d543df58c217b","type":"text","text":"> S thinks that P if and only if S is cognitively related to a sentence (in S's internal LOT)","x":-196,"y":-4887,"width":327,"height":94,"color":"#527dff"},
		{"id":"32070ac556495a4d","type":"text","text":"# Language of Thought Hypothesis","x":-200,"y":-4977,"width":361,"height":50,"color":"#527dff"},
		{"id":"3c9e81a20f7611a2","type":"text","text":"### Motivations for LOT\n* LOT has a finite set of atomic symbol-types (e.g. words), which have a set meaning\n* There are strict syntactic rules for how to concatenate one atomic symbol to another to form a complex expression\n* These rules are recursive\n* The meaning of a complex LOT expression is a systematic function of the semantic values of the constituent elements and their syntactic organization (**compositionality**)","x":-455,"y":-4517,"width":756,"height":261,"color":"#527dff"},
		{"id":"b4ad59cc8b742566","type":"text","text":"##### All thoughts possess two constituents: \n> Propositional content: The thought expressed\n\n> Propositional attitude: How the subject is thinking or cognitively related to the thought","x":-392,"y":-4700,"width":648,"height":164,"color":"#527dff"},
		{"id":"cc65732ba9531041","type":"text","text":"* All thinking and reasoning takes place in a language\n\n* The language in which thinking occurs *should not be confused with natural language*; it *presupposes* natural language; it is ***computational***","x":-560,"y":-4977,"width":307,"height":259,"color":"#527dff"},
		{"id":"14761507d1108b01","type":"text","text":"Exploiting such parallelism between syntactic and semantic structure (in the manner of a Turing machine,) LOT shows how everyday episodes of personal level reasoning can be fundamentally a mechanical process","x":-392,"y":-3445,"width":342,"height":189,"color":"#527dff"},
		{"id":"74d775f4f23a639f","type":"text","text":"#### Cappelen & Denver; practicality of philosophy of ANNs mental content \nWhat's the point of this philosophizing around mental content? They respond: \n1. If ANNs are to have practical application, then we must be able to interpret their outputs\n2. To interpret ANN's outputs, we must have a theory of content for ANNs\n3. We have no theory of content for ANNs\n4. So, we can't interpret an ANN's outputs\n5. So, ANNs have no practical application to society\n\n\n\n","x":4609,"y":11,"width":705,"height":336,"color":"#7e6e20"},
		{"id":"2345c2d2de90cdb6","type":"text","text":"**Cannot interpret ANNs internal process**\n> \"Alchemy\": Alchemists' theorizing about chemistry was not based on understanding of science, and their theories had to be abandoned in Enlightenment + progress. ANN systems are currently like alchemy.\n> \n> Ali Rahimini, qted in Cappelen & Denver\n","x":5090,"y":367,"width":448,"height":262,"color":"#7e6e20"},
		{"id":"c88aa29f33ae4c1d","type":"text","text":"Cappelen & Denver don't think it's going to take another scientific revolution to understand ANNs, but just to recognize that it is a philosophical problem about content ascription, that we don't understand what AI are doing or saying\n","x":5594,"y":268,"width":440,"height":158,"color":"#7e6e20"},
		{"id":"dca9021d82a717a6","type":"text","text":"- Mood or emotion: non-representational mental states\n- If cognitive states are computational, we can combine a mechanical, purely causal story about how it *works* with a *semantic interpretation* of what the agent is doing that reveals the significance of the agents internal states, including in relation to phenomena beyond the agent's causal reach \n- Syntax = mechanical \n- Behavior = semantics, ascribe intentionality \n\n**In classical terms:**\n> The syntactic rules that govern the system’s transitions form one symbolic state to another are as such to sustain a semantic interpretation of the system's state-transitions as rationally coherent.\n","x":3990,"y":2954,"width":648,"height":402,"color":"#ff8a8a"},
		{"id":"65b695a50178216a","type":"text","text":"#### Deep reinforcement learning\n> Neither supervised nor unsupervised: **reward based**\n> AlphaGoZero\n","x":4304,"y":-2167,"width":468,"height":120,"color":"#3719a3"},
		{"id":"005d4fd86dfddde8","type":"text","text":"### Unsupervised learning ","x":4755,"y":-2018,"width":324,"height":50,"color":"#5e1ba1"},
		{"id":"6e93861da2087c4d","type":"text","text":"#### Ramsey, Stich and Garon:\n1. If human thought is systematic, then it must have vehicles whose structure maps on to the structure of their contents\n2. If ANNs are good models of human thought, then human thought cannot have vehicles whose structure maps on to the structure of their contents\n3. ANNs are good models of human thought\n4. Therefore: Human thought cannot have vehicles whose structure maps on to the structure of their contents.\n5. (And) Human thought is not systematic\n\n**Grant that human thought is systematic, but deny that systematicity requires vehicle structural mapping**\n-> Construct an ANN that exhibits systematicity in its responses!\n- Given that this structure finds no echo at the computation level in the structure of connection weights, activation functions, etc. systematicity qualifies as an 'emergent' property of network activity ","x":4327,"y":-2980,"width":687,"height":502,"color":"#4a36dd"},
		{"id":"9751151890317215","type":"text","text":"##### **LOT Response**\n- Just as easy to train a systematic network as it is unsystematic; systematicity is an *accidental* feature rather than *essential*: **ANNs don't explain why systematicity is essential** \n- An ANN trained to display systematicity would be implementing LOT: ANNs do not contribute a *computational* explanation, only an implementational explanation\n","x":5087,"y":-2980,"width":476,"height":306,"color":"#527dff"},
		{"id":"fc87289b1e0b3afa","type":"text","text":"**ANN theorists face a dilhemma:**\n- If an ANN can be trained to display systematicity, it vindicates classical approach\n- If it cannot, it fails as a general account of human cognition","x":5087,"y":-2607,"width":476,"height":183,"color":"#4a36dd"},
		{"id":"76fbeb9855976bd0","type":"text","text":"**We can think about:**\n- non-existent entities (e.g. unicorns, santa)\n- entities outside space and time (eg. numbers)\n- future events\n- counterfactual scenarios (how things might have been)\n- entities otherwise causally isolated from us (eg. events we can't see)\n- impossible states of affairs (eg. that violate the laws of nature)","x":5502,"y":2737,"width":507,"height":316,"color":"#fed12f"},
		{"id":"f3c49c8eb77c2828","type":"text","text":"##### Wake-Sleep algorithm: \n> alternates between perceiving, *recognizing patterns* (awake) and classifying, *generating the training set* (asleep)\n\nKnowledge \"bootstrapping\"\nNo innate knowledge of the target mapping","x":4809,"y":-1944,"width":422,"height":244,"color":"#5e1ba1"},
		{"id":"a949407787b7214b","type":"text","text":"## Intentionality","x":4564,"y":2244,"width":250,"height":60,"color":"#fed12f"},
		{"id":"383166a3414d85f6","type":"text","text":"#### **Franz Brentano**\n> Intentionality: having a subject matter (\"of-ness\", \"about-ness\"), being about something by representing it","x":4472,"y":2311,"width":479,"height":154,"color":"#fed12f"},
		{"id":"bea5a43142b0cf01","type":"text","text":"#### **Questions for this response:**\n- Doesn't this trivialize LOT? \n- Could it be the LOT who is confused about levels of explanation? According to behaviorist theories, prop. attitude psych is not a theory of hidden causes of overt behavior but a description of directly observable behavioral patterns (**Dennett**) → Questions about brain's sub-personal computational architecture are *separate* from system's 'personal' level competences \n\t- Architecture of brain just needs to produce requisite behavior\n\t- Common-sense psych picks up on directly observable behavioral patterns \n\t- Computational question of what architecture underlies these is a completely separate issue\n","x":5627,"y":-2980,"width":680,"height":391,"color":"#527dff"},
		{"id":"460871a7024b9855","type":"text","text":"##### Sparse connectivity\n- Unlike traditional ANNs, each unit projects forward only to a small number of units further down (functionally akin to a retinotopic field)\n##### Shared weights\n- Unlike traditional ANNs, multiple units either share the same weight or have weights that are a systematic function of other units (non-isolated learning)\n##### Invariance under translation\n- Solves the selectivity/invariance problem: e.g. a visual representation of a samoyed must be selective enough to distinguish samoyed from a wolf, but invariant enough to register many different individual samoyeds or perspectives on a samoyed\n","x":3975,"y":-1931,"width":657,"height":504,"color":"#3719a3"},
		{"id":"54cdc85b9a1f7097","type":"text","text":"#### Convolutional networks (ConvNets)","x":3975,"y":-2011,"width":411,"height":60,"color":"#3719a3"},
		{"id":"87184330deddaefa","type":"text","text":"##### **Intentional relations:**\n- S may believe that H2O is poisonous, but not water, even though they are names for the same thing\n- Can be only objects of thought and not physically exist","x":5003,"y":2785,"width":439,"height":220,"color":"#fed12f"},
		{"id":"fd432745ca43fcbc","type":"text","text":"#### Notable features of intentionality\n\n\n\n\n","x":4973,"y":2505,"width":420,"height":63,"color":"#fed12f"},
		{"id":"9cc49cfa8732441d","type":"text","text":"##### **Brentano's thesis: All, and only, mental states have intentionality**","x":4420,"y":2527,"width":354,"height":82,"color":"#fed12f"},
		{"id":"7495c8a8d0ce9333","type":"text","text":"## Argument from Systematicity","x":2535,"y":-2538,"width":444,"height":55,"color":"#527dff"},
		{"id":"445a7acc978deb72","type":"text","text":"### Deep learning and hierarchical multilayer networks\n> Biologically inspired by the hierarchical organization of the mammalian brain, where hidden layers closer to the input layer are ‘earlier’ in the processing hierarchy than those closer to the output layer. Different layers thus correspond to different levels of complexity, abstraction, and scale.\n\n**more biologically realistic/evolutionarily sensible -> more computationally powerful**\n\nFed with raw data, automatically discover representations\n\n**Hierarchy of representations:**\nOne layer provides inputs to another, which correspond to levels in a representational hierarchy\n","x":3311,"y":-2167,"width":591,"height":423,"color":"#3719a3"},
		{"id":"ecd2f28be69d2eb2","type":"text","text":"### LOT argument against ANNs\n1. **If human thought is systematic, then it must have vehicles whose structure maps on to the structure of their contents**\n2. **If ANNs are good models of human thought, then human thought cannot have vehicles whose structure maps on to the structure of their contents**\n\t- Extremely context sensitive, not just discrete, clearly identifiable representation\n3. Human thought is systematic\n4. Human thought must have vehicles whose structure maps on to the structure of their contents\n5. Therefore: Artificial neural networks are not good models of human thought\nSee Fodor & Pylyshyn","x":3129,"y":-2980,"width":598,"height":469,"color":"#527dff"},
		{"id":"e80409daa6b0d3a7","type":"text","text":"#### Classic multilayer ANNs\n- Relies on \"supervised\" learning algorithms that use labelled pre classified data to calculate the error signal/cost function\n\t- \"feedback\" on how well it's doing","x":2895,"y":-1707,"width":416,"height":192,"color":"3"},
		{"id":"30a0ee2f37b367f2","type":"text","text":"#### Rise of connectionism\n- Many nodes interact via weighted connections, which can be inhibitory or excitatory \n\t- Further modeling from brain\n- No central controller or memory/storage system\n\t- Short term memory is \"stored\" in a unity's changing state of activation\n\t- Long term memory is stored in the relatively enduring strength of the connection weights between units","x":2600,"y":-2179,"width":591,"height":315,"color":"#8cff00"},
		{"id":"4ad5430d20f70f61","type":"text","text":"Even where the object of thought is real, it isn't easy to see it can explain behavior, yet we do explain behavior by reference to thought\n- Does the object of one's thought exert a magnetic pull on one's brain and body? \n- Can you build a device that alerts you whenever someone thinks about you? \n","x":6053,"y":2576,"width":500,"height":237,"color":"#fed12f"},
		{"id":"8711f1b20b9547cd","type":"text","text":"##### **Ordinary physical relations:** \n- Hold regardless of how the relata (the things which relate to each other) are described or considered\n- Relata exist \n\n","x":5003,"y":2576,"width":439,"height":188,"color":"#fed12f"},
		{"id":"eead5873c153a618","type":"text","text":"##### Autoencoders:\n> reproduce the input state after a process of data-compression – encoding and decoding\n> \"charades\"\n> \"bottleneck\" facilitates the construction of more abstract representations of the stimulus\n\n\n","x":4809,"y":-1679,"width":422,"height":232,"color":"#5e1ba1"},
		{"id":"9599c584e5157d2e","type":"text","text":"We could use this distinction to modify Brentano's thesis to say ","x":3412,"y":2527,"width":317,"height":58,"color":"#feaa34"},
		{"id":"216deafbbf88d8cf","type":"text","text":"All and only mental states possess intrinsic, original, or non-derived intentionality.","x":3808,"y":2640,"width":364,"height":113,"color":"1"},
		{"id":"a35f1ecd4b7ee14c","type":"text","text":"### Language and propositional thought (LOT vs ANNs)\n- Long seen as the main strength of classical cognitive science. However, machine learning is emerging as a serious rival. Even early ANNs were remarkably resilient in the face of noisy, incomplete linguistic data, produced human-like errors, etc.  \n- More recently, ConvNets have been applied to natural language processing (large language models) with considerable success. After all, ConvNets excel at information presented in a grid-like format, and a written sentence is just a 1-D grid!\n\nANNs lack a language-like structure: LOT theorists argue ANNs cannot explain the most central feature of human thought\nNothing to prevent an ANN from being a \"punctate mind\"\n","x":2299,"y":-2960,"width":703,"height":376,"color":"#527dff"},
		{"id":"3f4fe2aed46d4686","type":"text","text":"#### Functional indeterminacy problem: \n**What can the frog distinguish visually?** \nExplanation pf proper function framed in terms of selection history generate some counterintuitive results \n\t→ \"Swampman\" who just emerges spontaneously can think, but without **history**, they cannot, according to theory of proper function\n- Representation is a stand-in for the represented (tracking which is causally distal)\n- Representation tracks objects, implying a self-world distinction","x":1863,"y":1640,"width":669,"height":324,"color":"#4d8f5d"},
		{"id":"5c887f202bbdc277","type":"text","text":"Causal → Robot reply","x":2073,"y":2000,"width":250,"height":60,"color":"1"},
		{"id":"7ce4a3c0a34fed10","type":"text","text":"## **Causal theories**\n- Take \"theories\" more seriously\n- Believe mental representations are causally efficacious internal states of a subject whose nature is not settled by interpretive practices of a linguistics community → is this required to be a realist?\n- Relate to: About-ness is doing , robots reply\n- Response to Chinese room: could claim there is understanding (because about-ness is determined by interrelation of symbols), or claim that Searle's system is not *functionally* equivalent to a native Chinese speaker (despite being *behaviorally identical*)\n#### **'Tracking'**\n- Tracking theorists ask about relationships between parts of internal system and aspects of **environment** (how do they causally co-vary)\n- Moral of the Chinese room: we must ask about the systems *causal relationships* with entities *outside the system*\n##### **Teleofunctional indicator semantics**\nX represents Y if and only if X has the proper function of reliably indicating Y (**indexical**, altho more to it than that)\nBiological/evolutionary framing: X has the effect Y because those Xs that didn't have the function of Y died out\n- e.g. certain neural populations in the mammalian visual system are said to be \"feature detectors\" → these neurons detect various classes of stimuli BUT they can malfunction and misrepresent\n\n**Worries for Tracking**\n\"functional indeterminsey problem\"","x":1863,"y":716,"width":669,"height":882,"color":"#4d8f5d"},
		{"id":"3995698315da7cfe","type":"text","text":"### Chinese Room Argument\n1. Computer programs are purely formal or syntactic: behavior is only sensitive to \"shapes\" of symbols\n2. Genuine understanding and thought is sensitive to the meaning (semantics) of the symbols\n3. Form (or syntax) can never constitute, or be sufficient for, meaning \n4. Therefore: running a computer program can never be sufficient for understanding or thought\n","x":3170,"y":2843,"width":700,"height":278,"color":"#057a0d"},
		{"id":"e26ad44a4326cba7","type":"text","text":"### **Searle:**\n- No automated syntactic system (simply as such) will be sufficient for semantic content\n> \"As long as it simulates only the formal structure, of the system, it won't have simulated what matters about the brain, namely its causal properties, its ability to produce *intentional* states\"\n- We (cognitive agents) *attribute* meanings to a computer's internal states and outputs but what meanings they possess *derive* from the *intrinsic* meaning of our mental states","x":3170,"y":3192,"width":700,"height":340,"color":"#057a0d"},
		{"id":"1d269826e200aef9","type":"text","text":"##### ANN advocate response\n**Radically: deny that human thought is systematic**\n- If this is inconsistent with folk psychology, that's just an argument against folk psychology! We should replace it with something better\n→ **\"Eliminative materialism\"**","x":3787,"y":-2980,"width":476,"height":291,"color":"#4a36dd"},
		{"id":"6e7251a18399b3cf","type":"text","text":"***Successes:*** \n- solves pattern detection problems that would be wildly difficult for a serial processor, given the large number of parameters of variation\n- fault tolerance and graceful degradation; good with ambiguous data, identify similarities and differences between different examples in training set\n- mesh with prototype theory of concepts","x":3055,"y":-1447,"width":614,"height":236,"color":"4"},
		{"id":"4407522c48dcd521","type":"text","text":"***Worries:***\n- technical limitations\n- conceptually: supervised learning seems problematic from the perspective of real-world learning–is it learning if the solution is already given? ","x":3055,"y":-1188,"width":614,"height":184,"color":"4"},
		{"id":"56c25b2126a21ffb","type":"text","text":"# Artificial neural networks (ANNs)\n\n\n","x":1775,"y":-2211,"width":511,"height":64,"color":"3"},
		{"id":"8f089fcff3fa366f","type":"text","text":"### The problem of meaning\n- Traditional puzzles of intentionality/mental representation (focusing on the causal properties of mental representations);\n- Computationalist response in terms of a syntax/semantics distinction.\n- Searle’s Chinese room argument against Strong AI/computational theories of mind.\n- Derived versus. nonderived meaning/content\n- Theories of mental content: interpretivism, causal theories (e.g., indicator telosemantic theories), phenomenal intentionality\n- Conseuences of these theories for the Chinese Room (e.g., interpretivism goes with the system reply, causal theories go with robot reply)\n- Problems with interpreting ANNs (Cappelen & Denver)\n","x":3292,"y":286,"width":743,"height":398,"color":"2"},
		{"id":"3d3892f26a220510","type":"text","text":"### Approaches to Intentionality","x":3140,"y":2033,"width":380,"height":50,"color":"2"},
		{"id":"019cb51041542b6c","type":"text","text":"**Original vs Derived Intentionality**\n> \"Intentionality, however, is not all created equal. **At least some outward symbols (for instance, a secret signal that you and I have agreed on) have their intentionality only derivatively (the agreement)**. Derivative intentionality must derive eventually from something that is not similarly derivative (original). This original intentionality is the real metaphysical problem; for the possibility of delegating content, once there is some to delegate, is surely less puzzling than how there can be any in the first place.\" (\"Intentionality All Stars\" 385)\n> Haugeland (abbreviated somewhat)\n","x":3140,"y":2111,"width":589,"height":350,"color":"#feaa34"},
		{"id":"84a07f50c8543bd0","type":"text","text":"\nOption 1: Bring back LOT\nYoung, new scientists are still in favor of LOT: resurgence \n\nOption 2: Non-classical computational architectures\n- artificial neural networks\n- connectionism\n- parallel distributed processing\n- the sub-symbolic paradigm in artificial intelligence","x":2147,"y":-3676,"width":508,"height":273,"color":"5"},
		{"id":"c462a56de783cbce","type":"text","text":"##### Productivity\nHuman thinkers have capacity to understand a large number of sentences without having encountered them before ","x":-160,"y":-4075,"width":461,"height":159,"color":"#527dff"},
		{"id":"69dd01aef379a96b","type":"text","text":"##### Rational coherence\nProponents argue LOT is unique because it explains how thought can both cause and justify action\n> A thought both causes and justifies a transition whenever the **syntactic relations between the LOT sentences are structurally isomorphic to the rational relations between the propositional contents of the sentences**","x":-134,"y":-3883,"width":410,"height":343,"color":"#527dff"},
		{"id":"1dd415f435c06a05","type":"file","file":"W2023T1/W2023T1 Files/Screen Shot 2023-10-19 at 8.58.17 AM.png","x":-647,"y":-4177,"width":344,"height":400,"color":"#527dff"},
		{"id":"a62df1f2ed4ec5a4","type":"text","text":"##### Translation between sub-personal and personal vocabularies\nWe can translate the theoretical taxonomy of common-sense psych into the theoretical taxonomy of cognitive science: Belief, desire, reasoning, planning, decision making, etc. ","x":313,"y":-4180,"width":530,"height":185,"color":"#527dff"},
		{"id":"3c75769948bac75d","type":"text","text":"Chomskian themes throughout Fodor's Mentalese writing","x":327,"y":-4515,"width":371,"height":61,"color":"#527dff"},
		{"id":"dce545eb687d38ed","type":"text","text":"Sub-personal: non-conscious, tacit, can be difficult or even seem impossible to represent in language","x":595,"y":-4340,"width":305,"height":126,"color":"1"},
		{"id":"c57c9f86904eded4","type":"text","text":"Situates this idea historically: Previous philosophers attempted to describe thinking and reasoning in mechanical terms, causal sequence of ideas/representations\n","x":327,"y":-4679,"width":463,"height":143,"color":"#527dff"},
		{"id":"bccfbeb6809dccdb","type":"text","text":"#### Assessing LOT\n**Distinguish:** \n- Questions about LOT as an account of central cognition\n- Questions about LOT as an account of peripheral cognition: the elvel that it is natural to associate with sub personal subsystems\n\n\"trust me, these inaccessible modules are computing, we just can't see it\"\n","x":1971,"y":-3991,"width":664,"height":261,"color":"5"},
		{"id":"64f433d0de80e4c5","type":"text","text":" **Play on homunculus argument**","x":1435,"y":-4056,"width":250,"height":61,"color":"4"},
		{"id":"444ad8f097b57272","type":"text","text":"#### Fodor, on tying one's shoes (tacit knowledge): \n> Homunculus in our head has a library of information, including a book of step by step instructions on tying shoes. He presses buttons that activate our motor (in his description, literal cogs and levers) pathway which does the action, ie take the left free end of the shoelace in the left hand. This continues until the instructions end with the word \"end\" at which point he puts the book back on the shelf\n","x":1017,"y":-4193,"width":336,"height":461,"color":"4"},
		{"id":"e95f5d67456f1c51","type":"text","text":"Theoretical \"reduction\" of **personal** psychology into **sub-personal, aka computational** terms","x":368,"y":-3963,"width":360,"height":94,"color":"#527dff"},
		{"id":"a814d83173cc167d","type":"text","text":"##### F4: Ontological\n> One of the ultimate failures of GOFAI is the assumption that reality possesses a pre-given structure of which our representations are a mirror. We exist in a *non-conceptual world*\n","x":1353,"y":-4317,"width":413,"height":206,"color":"2"},
		{"id":"0fe561b718d0acd2","type":"text","text":"C is caused by the \"syntactic\" properties of P1 and P2: their shape or form","x":-705,"y":-3740,"width":433,"height":77,"color":"#527dff"}
	],
	"edges":[
		{"id":"33856b9897941a5a","fromNode":"32070ac556495a4d","fromSide":"right","toNode":"783d72379c42b8fd","toSide":"left"},
		{"id":"57c6c0d643f3d357","fromNode":"118a0cd89d43c717","fromSide":"right","toNode":"444ad8f097b57272","toSide":"left"},
		{"id":"f9ef6adc04c71dde","fromNode":"e95f5d67456f1c51","fromSide":"bottom","toNode":"118a0cd89d43c717","toSide":"top"},
		{"id":"cd98d40748d55836","fromNode":"922f14d55acaef03","fromSide":"left","toNode":"c57c9f86904eded4","toSide":"left"},
		{"id":"ef7ccd09773134bd","fromNode":"8d9858d9937fa77e","fromSide":"right","toNode":"922f14d55acaef03","toSide":"top"},
		{"id":"b47bb60472e4dee7","fromNode":"4b9e0ed24c9c4175","fromSide":"bottom","toNode":"d5ec8fcfc077b755","toSide":"left"},
		{"id":"90c7ea4a040867da","fromNode":"cfa2a4de29eebe6c","fromSide":"bottom","toNode":"f0cdc8be1a3b9cc0","toSide":"top"},
		{"id":"35cbdbf2a49f2146","fromNode":"f0cdc8be1a3b9cc0","fromSide":"bottom","toNode":"444ad8f097b57272","toSide":"top"},
		{"id":"c5210dfdb8b2d3bb","fromNode":"e95f5d67456f1c51","fromSide":"right","toNode":"444ad8f097b57272","toSide":"left"},
		{"id":"ab1ff421482c2f78","fromNode":"3c9e81a20f7611a2","fromSide":"bottom","toNode":"5a560fdd4913d758","toSide":"top"},
		{"id":"2afff2b4cce3b301","fromNode":"444ad8f097b57272","fromSide":"right","toNode":"64f433d0de80e4c5","toSide":"left"},
		{"id":"d9d4c3235fe90a94","fromNode":"14761507d1108b01","fromSide":"top","toNode":"69dd01aef379a96b","toSide":"bottom"},
		{"id":"e552ba476d682b42","fromNode":"14761507d1108b01","fromSide":"left","toNode":"0fe561b718d0acd2","toSide":"right"},
		{"id":"1486141b38b0744b","fromNode":"1dd415f435c06a05","fromSide":"bottom","toNode":"0fe561b718d0acd2","toSide":"top"},
		{"id":"fdd2c4990c533cd4","fromNode":"69dd01aef379a96b","fromSide":"left","toNode":"1dd415f435c06a05","toSide":"right"},
		{"id":"933fddde2b5e4e56","fromNode":"783d72379c42b8fd","fromSide":"bottom","toNode":"88b7629776002716","toSide":"left"},
		{"id":"46802e90042f5508","fromNode":"783d72379c42b8fd","fromSide":"left","toNode":"4b9e0ed24c9c4175","toSide":"top"},
		{"id":"bb5fd4e5252690d0","fromNode":"bccfbeb6809dccdb","fromSide":"top","toNode":"08085372c4c18013","toSide":"bottom"},
		{"id":"3bda2a2aacc7ce84","fromNode":"bccfbeb6809dccdb","fromSide":"bottom","toNode":"84a07f50c8543bd0","toSide":"top"},
		{"id":"1cfb2610d4b47ac2","fromNode":"a08c5ab18c9635ea","fromSide":"top","toNode":"230ee30239a6cac4","toSide":"bottom","label":"LOT"},
		{"id":"53052326b8ef0493","fromNode":"a08c5ab18c9635ea","fromSide":"right","toNode":"56c25b2126a21ffb","toSide":"left","label":"ANNs"},
		{"id":"72e31e3b386d6b81","fromNode":"56c25b2126a21ffb","fromSide":"bottom","toNode":"4aa89f6313a21e1c","toSide":"top"},
		{"id":"4bf6408fb73ac89c","fromNode":"4aa89f6313a21e1c","fromSide":"right","toNode":"30a0ee2f37b367f2","toSide":"left"},
		{"id":"a7f2de778d474728","fromNode":"7bf460247d96c310","fromSide":"left","toNode":"4aa89f6313a21e1c","toSide":"bottom"},
		{"id":"f8678249ea2567bb","fromNode":"e80409daa6b0d3a7","fromSide":"right","toNode":"6e7251a18399b3cf","toSide":"top"},
		{"id":"3bf2138d032718ba","fromNode":"7bf460247d96c310","fromSide":"right","toNode":"e80409daa6b0d3a7","toSide":"left"},
		{"id":"52b6680c4ca1c616","fromNode":"30a0ee2f37b367f2","fromSide":"right","toNode":"445a7acc978deb72","toSide":"left"},
		{"id":"8be2d2f3c02e6c61","fromNode":"e80409daa6b0d3a7","fromSide":"top","toNode":"445a7acc978deb72","toSide":"left"},
		{"id":"476c69dd4f3c3412","fromNode":"445a7acc978deb72","fromSide":"right","toNode":"54cdc85b9a1f7097","toSide":"left"},
		{"id":"d8c420b4f2539fe9","fromNode":"005d4fd86dfddde8","fromSide":"left","toNode":"f3c49c8eb77c2828","toSide":"left"},
		{"id":"de58565f1600de76","fromNode":"005d4fd86dfddde8","fromSide":"left","toNode":"eead5873c153a618","toSide":"left"},
		{"id":"8074613753288922","fromNode":"230ee30239a6cac4","fromSide":"bottom","toNode":"569b52153133f25f","toSide":"left"},
		{"id":"114ea438d6287205","fromNode":"56c25b2126a21ffb","fromSide":"top","toNode":"569b52153133f25f","toSide":"left"},
		{"id":"1b9510a266667c0f","fromNode":"ecd2f28be69d2eb2","fromSide":"right","toNode":"1d269826e200aef9","toSide":"left"},
		{"id":"bc16ca9f97c2f596","fromNode":"1d269826e200aef9","fromSide":"right","toNode":"6e93861da2087c4d","toSide":"left"},
		{"id":"5b8bd630290d5573","fromNode":"6e93861da2087c4d","fromSide":"right","toNode":"9751151890317215","toSide":"left"},
		{"id":"e5b241164e19d781","fromNode":"9751151890317215","fromSide":"bottom","toNode":"fc87289b1e0b3afa","toSide":"top"},
		{"id":"a35198ed7006fd22","fromNode":"9751151890317215","fromSide":"right","toNode":"bea5a43142b0cf01","toSide":"left"},
		{"id":"75a8faf01ffe8b1f","fromNode":"636e2c7b7888483d","fromSide":"left","toNode":"880f78e590b56199","toSide":"top"},
		{"id":"38bc701c89a57191","fromNode":"636e2c7b7888483d","fromSide":"right","toNode":"0e88e211636533f3","toSide":"top"},
		{"id":"9cb54466af044120","fromNode":"a08c5ab18c9635ea","fromSide":"left","toNode":"145eaf3afd5a5a86","toSide":"top","label":"Smith"},
		{"id":"146192cc10f32cd7","fromNode":"7495c8a8d0ce9333","fromSide":"right","toNode":"ecd2f28be69d2eb2","toSide":"left"},
		{"id":"b0e706fddefbfd3e","fromNode":"383166a3414d85f6","fromSide":"bottom","toNode":"9cc49cfa8732441d","toSide":"top"},
		{"id":"8c2f234e37c072d6","fromNode":"87184330deddaefa","fromSide":"right","toNode":"76fbeb9855976bd0","toSide":"left"},
		{"id":"fd84f2684fe93164","fromNode":"8711f1b20b9547cd","fromSide":"right","toNode":"76fbeb9855976bd0","toSide":"left"},
		{"id":"3554cea95f0b7127","fromNode":"76fbeb9855976bd0","fromSide":"top","toNode":"4ad5430d20f70f61","toSide":"left"},
		{"id":"32abc1093d893a8c","fromNode":"fd432745ca43fcbc","fromSide":"left","toNode":"87184330deddaefa","toSide":"left"},
		{"id":"c0a9072b2745d64f","fromNode":"fd432745ca43fcbc","fromSide":"left","toNode":"8711f1b20b9547cd","toSide":"left"},
		{"id":"b44cdea2a44d55ff","fromNode":"9cc49cfa8732441d","fromSide":"bottom","toNode":"dca9021d82a717a6","toSide":"top"},
		{"id":"b60dfc328989e31b","fromNode":"dca9021d82a717a6","fromSide":"left","toNode":"3995698315da7cfe","toSide":"right"},
		{"id":"6dd87d0f854d74a2","fromNode":"3995698315da7cfe","fromSide":"bottom","toNode":"e26ad44a4326cba7","toSide":"top"},
		{"id":"3d663143384eb7f6","fromNode":"9cc49cfa8732441d","fromSide":"right","toNode":"fd432745ca43fcbc","toSide":"left"},
		{"id":"cec8a6c5bd135856","fromNode":"9cc49cfa8732441d","fromSide":"left","toNode":"216deafbbf88d8cf","toSide":"right"},
		{"id":"e3db523537dbb727","fromNode":"019cb51041542b6c","fromSide":"bottom","toNode":"9599c584e5157d2e","toSide":"top"},
		{"id":"f20eb59d5c2df61b","fromNode":"9599c584e5157d2e","fromSide":"bottom","toNode":"216deafbbf88d8cf","toSide":"left"},
		{"id":"4c0be093e8a9f50e","fromNode":"7ce4a3c0a34fed10","fromSide":"bottom","toNode":"3f4fe2aed46d4686","toSide":"top"},
		{"id":"5250d5aa3dc098a4","fromNode":"aead94c25c2cc967","fromSide":"bottom","toNode":"01b15abf1c71845e","toSide":"top"},
		{"id":"d2d722c10dd3481f","fromNode":"275be856562795f1","fromSide":"right","toNode":"7ce4a3c0a34fed10","toSide":"left"},
		{"id":"af78a03fed298e41","fromNode":"d4815f7d587250c9","fromSide":"bottom","toNode":"275be856562795f1","toSide":"top"},
		{"id":"2a7b2fa0a43fb047","fromNode":"74d775f4f23a639f","fromSide":"bottom","toNode":"2345c2d2de90cdb6","toSide":"left"},
		{"id":"a25b7700d935a46e","fromNode":"2345c2d2de90cdb6","fromSide":"right","toNode":"c88aa29f33ae4c1d","toSide":"left"},
		{"id":"357ac182932d709f","fromNode":"8f089fcff3fa366f","fromSide":"left","toNode":"32228c7ef8f1573d","toSide":"right","label":"Theories of mental content"},
		{"id":"982f2cbce842ccf9","fromNode":"8f089fcff3fa366f","fromSide":"right","toNode":"84d83fe9bf0df6af","toSide":"left","label":"Alfred Debate"},
		{"id":"0d2b4eb5570f5d7c","fromNode":"8f089fcff3fa366f","fromSide":"bottom","toNode":"c349cf951cf986e2","toSide":"top","label":"Intentionality + Searle"},
		{"id":"64de1dbea325efd5","fromNode":"aead94c25c2cc967","fromSide":"left","toNode":"10b827ad822048e0","toSide":"right"},
		{"id":"5fe537778fb1e2d0","fromNode":"01b15abf1c71845e","fromSide":"right","toNode":"3995698315da7cfe","toSide":"left"},
		{"id":"072b6fc69a7384a1","fromNode":"5c887f202bbdc277","fromSide":"right","toNode":"3995698315da7cfe","toSide":"left"},
		{"id":"f3f58a790c1db0b2","fromNode":"4d1b3cb7b46d0616","fromSide":"top","toNode":"e848588e02ec0b68","toSide":"left","label":"Robot parable"},
		{"id":"90199cce2eace4e1","fromNode":"e848588e02ec0b68","fromSide":"right","toNode":"1ad4827e79575aee","toSide":"left"},
		{"id":"4e79c68e916b67e7","fromNode":"1ad4827e79575aee","fromSide":"top","toNode":"a869e009b395a194","toSide":"bottom"},
		{"id":"56fb03f8afee7846","fromNode":"d1e3d3e834f4cd7d","fromSide":"left","toNode":"a869e009b395a194","toSide":"right"},
		{"id":"ed3e76f9c1461596","fromNode":"704953c9e174f61d","fromSide":"bottom","toNode":"a8299bd5a11ba753","toSide":"top"},
		{"id":"0e410e6f50151ab0","fromNode":"a8299bd5a11ba753","fromSide":"bottom","toNode":"7d671eb0069bfd43","toSide":"top"},
		{"id":"4f5985771ab6e936","fromNode":"7d671eb0069bfd43","fromSide":"right","toNode":"447d2e7d31a27e21","toSide":"left"},
		{"id":"4dcd18de69cba7f4","fromNode":"ae60f19007a26ece","fromSide":"right","toNode":"a8299bd5a11ba753","toSide":"left"},
		{"id":"1820f4f86b9ef300","fromNode":"ae60f19007a26ece","fromSide":"left","toNode":"670effcf6311bd1d","toSide":"bottom"},
		{"id":"1717d370100881f4","fromNode":"35ba2c7ffc27482a","fromSide":"right","toNode":"7215fab98f9e5a15","toSide":"left"},
		{"id":"8edf8724fec88925","fromNode":"941ad4efecdac94b","fromSide":"right","toNode":"7215fab98f9e5a15","toSide":"left"},
		{"id":"00bd898b83f35e5f","fromNode":"4d1b3cb7b46d0616","fromSide":"top","toNode":"ddf328247a0afe3d","toSide":"right","label":"Central systems"},
		{"id":"3a6cd304d1d80365","fromNode":"332e37206e4984f7","fromSide":"left","toNode":"1506e361ebee45d3","toSide":"top"},
		{"id":"b69f6bf0f20ca3ad","fromNode":"528d0efb8fd09954","fromSide":"right","toNode":"1506e361ebee45d3","toSide":"left"},
		{"id":"af51e89a7bb76107","fromNode":"808e04b0777d04ef","fromSide":"bottom","toNode":"3c61bba1129e7740","toSide":"top"},
		{"id":"425a45c1bf66e17d","fromNode":"f6436a15f4388f7e","fromSide":"bottom","toNode":"f12151ee4b8f48ae","toSide":"top"},
		{"id":"5afbbd830c058a92","fromNode":"f014c3cfa372c879","fromSide":"bottom","toNode":"c98c4ffc61be521d","toSide":"left"},
		{"id":"75c43fe96beaf59e","fromNode":"3c61bba1129e7740","fromSide":"bottom","toNode":"08821d630bd96602","toSide":"top"},
		{"id":"6028472c43b63862","fromNode":"f014c3cfa372c879","fromSide":"left","toNode":"2886e85b67b21719","toSide":"right"},
		{"id":"d70c767eeecc9ea8","fromNode":"f014c3cfa372c879","fromSide":"bottom","toNode":"173194fc7fbf1865","toSide":"top"},
		{"id":"1a130e91cd56cf9e","fromNode":"2886e85b67b21719","fromSide":"bottom","toNode":"9acde8d96af4a4bf","toSide":"top"},
		{"id":"f5f000d2e6c60e85","fromNode":"ead1f047bfcfa232","fromSide":"bottom","toNode":"c98c4ffc61be521d","toSide":"top"},
		{"id":"7e092b7930a269c3","fromNode":"173194fc7fbf1865","fromSide":"right","toNode":"ddc4d99044254a1f","toSide":"left"},
		{"id":"3d7feb081fa8e191","fromNode":"c98c4ffc61be521d","fromSide":"bottom","toNode":"ddc4d99044254a1f","toSide":"top"},
		{"id":"8f8159a7677ae4ae","fromNode":"4d1b3cb7b46d0616","fromSide":"bottom","toNode":"5728d2ec6a9200ab","toSide":"right","label":"Frame problem"},
		{"id":"cde899690056e5df","fromNode":"447d2e7d31a27e21","fromSide":"right","toNode":"94be040cd3971c65","toSide":"left"},
		{"id":"011ada957fb8096d","fromNode":"ddc4d99044254a1f","fromSide":"bottom","toNode":"b2e31e5c745a3185","toSide":"top"},
		{"id":"252d9ca992a24e07","fromNode":"208d9773b7e55dd2","fromSide":"top","toNode":"173194fc7fbf1865","toSide":"bottom","label":"Higher order/Global Workspace"},
		{"id":"976ee02ee4678009","fromNode":"ddc4d99044254a1f","fromSide":"right","toNode":"e5670bc1ea732ab0","toSide":"top"},
		{"id":"abf61a30e99ec06b","fromNode":"8af2d9e3b45be1e7","fromSide":"left","toNode":"fadb274fa0f8336f","toSide":"left"},
		{"id":"bb97de4328017319","fromNode":"e5670bc1ea732ab0","fromSide":"right","toNode":"949d457d33203b69","toSide":"right"},
		{"id":"68166e8eb427e91d","fromNode":"e5670bc1ea732ab0","fromSide":"right","toNode":"8af2d9e3b45be1e7","toSide":"right"},
		{"id":"a57aad1501d0ebbe","fromNode":"e5670bc1ea732ab0","fromSide":"left","toNode":"99950eedd92995ff","toSide":"top"},
		{"id":"15bb0581ee6497e2","fromNode":"99950eedd92995ff","fromSide":"bottom","toNode":"cbd2d6660af198af","toSide":"top"},
		{"id":"6a5031ee55b188ab","fromNode":"cbd2d6660af198af","fromSide":"bottom","toNode":"271d6f0f4b6c32e9","toSide":"top"},
		{"id":"8e7342cf54960a7a","fromNode":"271d6f0f4b6c32e9","fromSide":"right","toNode":"d35d552f9de029c3","toSide":"right"},
		{"id":"da5e325132ceeb51","fromNode":"cbd2d6660af198af","fromSide":"left","toNode":"180b775877b67619","toSide":"right"},
		{"id":"80b63bcde82d4ef0","fromNode":"180b775877b67619","fromSide":"bottom","toNode":"030c66b93843455e","toSide":"top"},
		{"id":"d91f5aa65d15d77f","fromNode":"030c66b93843455e","fromSide":"bottom","toNode":"4ef503b7716408bc","toSide":"top"},
		{"id":"a325fc4bcd7e62a5","fromNode":"4ef503b7716408bc","fromSide":"left","toNode":"3fabae7c28b7717b","toSide":"right"},
		{"id":"20dd6c8a3934b90e","fromNode":"208d9773b7e55dd2","fromSide":"bottom","toNode":"db596b5b9cff9861","toSide":"left","label":"Phenomenal overflow"},
		{"id":"316bc6b6352514b0","fromNode":"208d9773b7e55dd2","fromSide":"bottom","toNode":"974858e448a679ca","toSide":"top","label":"The Hard problem "},
		{"id":"45e26fb595d2eb9a","fromNode":"4d1b3cb7b46d0616","fromSide":"right","toNode":"94be040cd3971c65","toSide":"left","label":"Solving frame problem"},
		{"id":"de946779b903e34c","fromNode":"208d9773b7e55dd2","fromSide":"right","toNode":"e5670bc1ea732ab0","toSide":"left","label":"Concepts of consciousness"},
		{"id":"239beb0aae61d1c4","fromNode":"447d2e7d31a27e21","fromSide":"top","toNode":"7215fab98f9e5a15","toSide":"bottom"}
	]
}