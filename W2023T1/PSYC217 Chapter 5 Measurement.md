1. LO1 Define the reliability of a measure and describe the differences among test-retest, internal consistency, and inter-rater reliability. 
2. LO2 Compare and contrast reliability and validity. 
3. LO3 Describe how a researcher can build a case for construct validity, including predictive validity, concurrent validity, convergent validity, and discriminant validity. 
4. LO4 Describe the problem of reactivity for a measure and discuss ways to minimize reactivity. 
5. LO5 Describe the properties of the four scales of measurement: nominal, ordinal, interval, and ratio.

### Reliability
True score + measurement error
correlation coefficient
##### Test-retest reliability 
- giving many people the same measure twice
- reliability of a test could be assessed by giving it to a group of people on one day and then again a week later -> calculate a correlation coefficient to determine the relationship between the first test score and the second test score (i.e., the “retest”): strong positive correlation between the two sets of scores = high level of reliability. 
Alternate forms reliability: diff versions of same test
##### Internal consistency 
- how successfully diff items on scale measure same construct
- Cronbach's alpha: averages all inner-item correlations
	- Moving towards coefficient omega
**Split-half reliability**
- Randomly splitting items on scale in half and administering both halves to group
- Measure correlation between: should be strongly correlated
##### Inter-rater reliability
Person or group of people who rate/judge participants
*Cohen's kappa*

### Reliability, accuracy, and validity of measures
##### Construct validity 
- is operationalization accurate
	- Indicators:
	- Face
	- Content
	- Predictive
	- Concurrent
	- Convergent
	- Discriminant

### Reactivity of measures
- People changing behavior based on act of measurement
