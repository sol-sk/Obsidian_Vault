[[PHIL 351]]

12-05-23
### Pre class readings
Smith Ch.7-9 [[The Promise Of Artificial Intelligence Reckoning And Judgment (Brian Cantwell Smith) (Z-Library).pdf]]
### Lecture
#### Recap of Smith's project
- Arguing for a distinction between two kinds of intelligence
	- Ours 
	- Computers
- Only ours is "genuine" intelligence, because only it manifests the capacity for *judgement* vs *reckoning* (calculative reasoning)
	- There is something intentional about reckoning
	- Judgement *capacity* (rather than action)
- Consider intelligence based on AI's failures
##### Ethical dimensions
- How should we relate to these systems? 
- We assume that we can trust them more than we should 
> Two things do terrify me i) that we will rely on reckoning systems in situations that require genuine judgement; ii) that, by being unduly impressed by reckoning prowess, we will shift our expectations on human mental activity in a reckoning direction
> Smith xix-xx

- Potential errors are fueled by false assumptions about the nature of reality, intelligence, and the relationship between them 

#### Morals of 1st wave AI (GOFAI): The good
> **Representational Mandate:** The proper functioning of any world-directed system–any system that is thinking about or representing or processing information about the world–must be governed by **normative criteria**, applying to its **mechanical operations**, that are framed in terms of situations and states of affairs **in the world** that the system is **representing or reasoning about**, which situations and states of affairs will not, in the general case, be within effective (causal) reach. 
> Smith p. 17

**→ Intentional system: is able to reason about something that is distal** 

#### Morals of 1st wave AI (GOFAI): The bad
**Took objects for granted**
> It is naive to assume that "the world comes chopped up into neat, ontologically discrete objects. In fact one of the primary theses of this book is that the misconception of the world in first-wave AI is the 'smoking gun' that explains its ultimate inadequacy... Representations, descriptions, models, and the like all interpret or picture or filter the world through abstractions or idealizations–conceptual 'frames' that highlight or privilege some aspects of what is represented, minimize (or even distort) others, and ignore or abstract away from a potentially unbounded amount of in-the-world detail"
> pp. 28-29

How does a system parse a world? How does it discover what the world is?
→ An object ontology (the world as we understand it) is a cognitive achievement: something constructed rather than pre-given
- Corollary: we cannot assume that any intelligent system that we build will parse reality as we do. As an intelligent system, it will have to make its own sense of things
#### Morals of 2nd wave AI: The good
> Successes of ML architectures suggest that a vastly rich and likely ineffable web of statistical relatedness weaves the world together into an integrated 'subconceptual' whole. That is the world with which intelligence must come to grip. 
> p. 64
- Reality itself surpasses an intelligent being's capacity to conceptualize and verbally articulate it
- Intelligent systems must somehow 'come to grips' with world on non-conceptual level
- Intelligent systems must somehow abstract concepts from basic nonconcepts
- Objects, or ontologically determinate entities, are emergent from a richer metaphysical 'plenum' (the philosophical idea that no part of reality is empty of matter)
#### Morals of 2nd wave AI: The bad
- Similar to GOFAI error; cannot explain how a system comes to *understand* what it is representing. To achieve understanding (and genuine intelligence) Smith suggests the system would need to be capable of assuming a stance of deference to the world: 
> At the outset I said that semantics, in order to be semantics, must be *deferential.* At the moment, althought we may design our systems with deferential semantics, *the deference is ours*, not theirs. If we are going to build a system that is itself genuinely intelligent, that knows what it is talking about, we have to build one that is *itself* deferential–that *itself* submits to the world it inhavits, and does not merely behave in ways that accord with our human deference. To do that, it will have to know i) that there is a world, ii) that its representations are about that world, and iii) that it and its representations must defer to the world that they represent
> p.79


### Taxonomy of intentional systems?
**Nonhuman animals (creatures)**: 
> Participate in the world (in a way that makes sense for them)
> They represent/register the world in ways that mesh with how they engage, navigate, cope with, and are vulnerable to 
> The world is (presumably) intelligible to them; it matches their forms of life, biological needs, etc. 
> "Creatures" = systems whose registrational powers do not extend beyond the ways in which they conduct their lives, what they care about, what they are vulnerable to, what they are existentially involved with. 
> p.107

**Computers:**
> Most of the computational systems we construct–including the vast majority of AI–represent the world in ways that matter to us, not to them. It is because of that fact that we call them *computers,* or *information processors*, and also because of it that they have power in our lives, that they matter to us. What limits them is that, so far, nothing matters *to them.* Things will only matter to them when they develop committed and deferential existential engagement with the world. 
> p.108

![[Screen Shot 2023-12-07 at 12.41.21 PM.png]]


