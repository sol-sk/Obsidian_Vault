[[PSYC 217]]

12-06-23

### Open Science cont.
Human consequences of replication efforts?
Multi-lab collaboration → Failure to replicate → Why?
Replication causes lots of anxiety for original researchers (what if results don't replicate!!!) → resentment towards "replicators"
#### Many Labs
![[Screen Shot 2023-12-06 at 2.18.00 PM.png]]

#### Many Kids
- Developmental psych version of Many Labs
#### Many Brains
- Neurosci version of Many Labs
- Neuroscience tends to have few participants due to expense: replication very important!

### Research that has replicated well
#### False memory
#### Primacy effect + recency effect
#### Spacing effect
- Memory retention is much better if you space encoding out
### Research that has *not* replicated well
#### Power pose
#### Oxytocin effect
#### Learning styles
#### MBTI

### Effect size
- Point estimate of how large the effect is based on sample
- Focuses on the size of the difference between groups, not on p values (because $a = .05$ was set so arbitrarily)
- Could be off! → Confidence interval 
##### Confidence interval (eg. 95% confidence intervals)
- Tells us that there is a 90-95% chance that the range of values that we calculate capture the true effect size
- Indexes precision of effect size estimate
- Chance that interval captures true effect size
- If confidence interval does not include 0 = reject null hypothesis
- Example: 95% confidence interval of `[0.14, 0.46]`, a 99% confidence interval could be `[0.14, 0.90]` (larger interval = higher chance it includes population value)

### Perceptions of objectivity in quantitative research
Quantitative analysis **models** can decide if results are significant
→ **Researcher degrees of freedom (decisions made *after* data collecion)**
- What to control for? 
- Collect more people? 
- **Multiple models → inflates type 1 error**
	- Aka, keep running diff models until they show what you want to 
	- Because you keep "drawing" new models (analogy: pulling marbles from bag) without replacement, the chance of type 1 error goes up


#### One proposed solution: Split half analysis
- Split data in half
- Run *exploratory* analyses (eg. several models) on half of data, then run the chosen model(s) on *confirmatory* half
- Problem: Needs large data set
