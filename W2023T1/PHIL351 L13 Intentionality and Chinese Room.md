#philosophy 
#cogs 

[[PHIL 351]]

10-31-23

### Pre class readings
Dietrich et al Chapter 5 again
Optional: Searle, original Chinese room 

#### Historical notes on intentionality
**Franz Brentano**
- Intentionality: having a subject matter ("of-ness", "about-ness"), being about something by representing it

#### Notable features of intentionality
The semantic relation between a mental represent. and what it represents differs in striking ways from physical relations
**Ordinary physical relations:** 
- Hold regardless of how the relata (the things which relate to each other) are described or considered
- Relata exist 
**Intentional relations:**
- S may believe that H2O is poisonous, but not water, even though they are names for the same thing
- Can be only objects of thought and not physically exist
**We can think about:**
- non-existent entities (e.g. unicorns, santa)
- entities outside space and time (eg. numbers)
- future events
- counterfactual scenarios (how things might have been)
- entities otherwise causally isolated from us (eg. events we can't see)
- impossible states of affairs (eg. that violate the laws of nature)
Even where the object of thought is real, it isn't easy to see it can explain behavior, yet we do explain behavior by reference to thought
- Does the object of one's thought exert a magnetic pull on one's brain and body? 
- Can you build a device that alerts you whenever someone thinks about you? 
Brentano claimed that intentionality is the mark of the mental; 
##### **Brentano's thesis: All, and only, mental states have intentionality**

Mood or emotion: non-representational mental states
If cognitive states are computational, we can combine a mechanical, purely causal story about how it *works* with a *semantic interpretation* of what the agent is doing that reveals the significance of the agents internal states, including in relation to phenomena beyond the agent's causal reach 
Syntax = mechanical 
Behavior = semantics, ascribe intentionality 
**In classical terms**
- the syntactic rules that govern the systemâ€™s transitions form one symbolic state to another are as such to sustain a semantic interpretation of the system's state-transitions as rationally coherent.

### Chinese Room Argument
1. Computer programs are purely formal or syntactic: behavior is only sensitive to "shapes" of symbols
2. Genuine understanding and thought is sensitive to the meaning (semantics) of the symbols
3. Form (or syntax) can never constitute, or be sufficient for, meaning 
4. Therefore: running a computer program can never be sufficient for understanding or thought

We can scrutinize premise 3 the most: This is what Searle does
**Contrast**
1. A situation in which Searle answers questions expressed in a language that he is fluent in on the basis of understanding what has been asked
2. A situation in which Searle 'answers' questions posed in a language that he is not fluent in on the basis of following a program that perfectly simulates a normal conversation in that language

Systems reply: the larger system of Searle is a part would represent what is being said
Robots reply: a robot with sensors and effectors would represent what is being said
Brain simulator reply: an ANN would represent what is being said
Combination reply: the system comprised of an ANN inside a mobile robot would represent what is being said

**Searle:**
- No automated syntactic system (simply as such) will be sufficient for semantic content
> "As long as it simulates only the formal structure, of the system, it won't have simulated what matters about the brain, namely its causal properties, its ability to produce *intentional* states"
- We (cognitive agents) *attribute* meanings to a computer's internal states and outputs but what meanings they possess *derive* from the *intrinsic* meaning of our mental states

