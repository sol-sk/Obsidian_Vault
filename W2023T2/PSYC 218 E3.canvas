{
	"nodes":[
		{"id":"538416f5f1a08bc2","type":"group","x":-14229,"y":-89,"width":4289,"height":6340,"label":"E1"},
		{"id":"a1cfc5551e20c39b","type":"group","x":-9920,"y":-89,"width":3840,"height":4912,"label":"E2"},
		{"id":"afbfe30a0984b315","type":"group","x":-9680,"y":3244,"width":1552,"height":763,"label":"Review"},
		{"id":"fd718118b5c7ef94","type":"group","x":-6080,"y":-89,"width":5280,"height":4912,"label":"Final"},
		{"id":"ea936b8cf8883e73","type":"group","x":-6060,"y":2367,"width":3175,"height":1935,"label":"ANOVA"},
		{"id":"cce143c46bd5a5f0","type":"group","x":-3200,"y":800,"width":2200,"height":1340,"label":"independent t-test"},
		{"id":"169480747e632bc2","type":"group","x":-4500,"y":646,"width":1226,"height":1275,"label":"paired t-test"},
		{"id":"651a30ee44965458","type":"group","x":-6060,"y":646,"width":1474,"height":1036,"label":"confidence interval"},
		{"id":"361d946a90cf841d","type":"group","x":-7520,"y":-2080,"width":1060,"height":609,"label":"Testable material"},
		{"id":"78b8c553f8b6727c","type":"text","text":"#### Scatterplots\n**Imperfect**: Data does not fall completely linearly (as is most typical)\n\nSpearman's $\\rho/r_s$ = when one or both of the variables are of ordinal scaling\nBiserial correlation coefficient $r_b$ = when one o f the variables is at least interval and the other is dichotomous\n$\\phi$ = when each of the variables is dichotomous\n\n**Effect of range on correlation**: restrict range = lower correlation\n","x":-13419,"y":1723,"width":1140,"height":255,"color":"5"},
		{"id":"7ac582cb6a910de3","type":"text","text":"# Chapters 5 & 6","x":-13419,"y":1663,"width":300,"height":60,"color":"5"},
		{"id":"71e376d92c1a660f","type":"text","text":"### Pearson's $r$ vs Spearman's $r_s / \\rho$ \n#### Pearson's r assumes\n- Equal intervals\n- Normal distribution of X & Y\n- Absence of multivariate outliers\n- Linear relationship X~Y\n- *If assumptions violated, model may be biased*\n\n#### Spearman $r_s$ assumes: \n- *Ranked data*\n- Normal distribution unnecessary\n- Multivariate outliers are OKish\n- Monotonic relationship X~Y, but not necessarily linear\n\n**Advantages:**\n- RObust to non-normal data (skewed) outliers\n- Model non-linear\n\n**Disadvantages**: \n- If X & Y have equal intervals, ranking loses information\n- If X & Y are normally distributed, Spearman $r_s$ is less powerful (may fail to find true relationship)","x":-11289,"y":2255,"width":711,"height":761,"color":"5"},
		{"id":"d3dfcd587d1c2009","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.35.00 AM.png","x":-11997,"y":2431,"width":711,"height":273,"color":"5"},
		{"id":"b483f6d95d95d883","type":"text","text":"##### 3b: The Normal Curve\n- Symmetric\n- Unimodal \n- Perfectly variable\n68.2% of scores will fall within 1 SD of $\\mu$\n13.6% of scores will be $\\mu-2\\sigma$, 13.6% $\\mu+2\\sigma$ (27.2% will be exactly 2SD away)\n2.3% of scores will be $\\mu-3\\sigma$, 2.3% $\\mu+3\\sigma$ (4.6% will be exactly 3SD away)\n\n##### 3c: z-scores (a.k.a., standard scores)\n**z score** \n$\\frac{X_{i}-\\mu}{\\sigma}$\n- how many $\\sigma$s an observation is from $\\mu$\nz-scores *center* data: \"standardizing\"\n\nCan only use normal curve to obtain percentile rank when data is virtually identitcal to normal distribution","x":-11639,"y":1723,"width":711,"height":530,"color":"5"},
		{"id":"95722743966f4b46","type":"text","text":"# Lectures 7-10","x":-11639,"y":1663,"width":250,"height":60,"color":"5"},
		{"id":"2833eb85899fdb61","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 2.08.26 PM.png","x":-12219,"y":1958,"width":511,"height":295,"color":"5"},
		{"id":"dcd225b88d01ff77","type":"text","text":"#### Equation for the normal curve","x":-12219,"y":1751,"width":400,"height":60,"color":"5"},
		{"id":"89ecef187a8b78be","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 1.42.56 PM.png","x":-12219,"y":1811,"width":400,"height":147,"color":"5"},
		{"id":"0714ab33e5e6a7b3","type":"text","text":"### Calculating Pearson's $r$: \n- Check that relationship is linear (scatterplot)\n- Variables are normally distributed\n- Interval or ratio data\n- Absence of extreme outliers\n### Calculating $r$ rom z-scores:  $r = \\frac{\\sum z_{x}z_{y}}{N-1}$\n\nMost r values in psych research are 0.10-0.30\n\n#### Coefficient of determination: $r^2$\nr = relatedness, covariance\n$r^2$ = explained variance","x":-12709,"y":2253,"width":711,"height":527,"color":"5"},
		{"id":"60d788c739fc5eda","type":"text","text":"# Chapters 3 & 4\n","x":-13410,"y":-56,"width":277,"height":50,"color":"4"},
		{"id":"d3cec2315e3c4d43","type":"text","text":"pp. 102-148","x":-13599,"y":1663,"width":180,"height":60,"color":"5"},
		{"id":"1ec7d5c8928ff3ed","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.00.14 PM.png","x":-12282,"y":3323,"width":400,"height":120,"color":"6"},
		{"id":"aefe48b61a4435d8","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 2.59.53 PM.png","x":-12282,"y":3667,"width":303,"height":126,"color":"6"},
		{"id":"63a7fc6b7af87aa6","type":"text","text":"##### Video 5: Standard deviation and Variance\n**Variance**\nAverage of squared deviations (mean squared deviation)\n$\\frac{\\sum (X_{i}-\\bar{X})^2}{N}$\n\n**Standard deviation**\nSquare root of variance\nPopulation SD = $\\sqrt{\\frac{\\sum (X_{i}-\\mu)^2}{N}}$\nSample SD = $\\sqrt{\\frac{\\sum (X_{i}-\\bar{X})^2}{N-1}}$\n","x":-12610,"y":706,"width":711,"height":310,"color":"4"},
		{"id":"94db242de9ada138","type":"text","text":"### SS (sum of squares) = $\\sum{X^2 - \\frac{(\\sum{X})^2}{N}}$\nformulas will be on MTs; recognize but don't memorize\n### population SD = $\\sigma_{pop}$ $\\sqrt \\frac{\\sum(X_{i}- μ)^2}{N}$ \nTaking the square root gives us mean deviation = **standard deviation**\n### s (sample SD) = $\\sigma_{est}$ = $\\sqrt{\\frac{\\sum(X_{i}-\\bar X)^2}{N-1}}$\n### s (standard deviation, alt.) = $\\sqrt{\\frac{SS}{N-1}}$\n#### z score = $\\frac{X_{i} - \\mu}{\\sigma}$\n> how many $\\sigma$ an observation is from $\\mu$","x":-13420,"y":2253,"width":711,"height":548,"color":"5"},
		{"id":"493afaa5225385c3","type":"text","text":"# Chapter 7","x":-13420,"y":3263,"width":240,"height":60,"color":"6"},
		{"id":"432e8d5efd3412a1","type":"text","text":"### Calculating Pearson's $r$ from example data\n\n**Determining if two variables are related: *r***\n**Determining how much of Y variance is explained by X variance: $r^2$**\n\n\"multivariate outlier\"\n","x":-12709,"y":2778,"width":711,"height":174,"color":"5"},
		{"id":"c4a10188346a9a37","type":"text","text":"**Median**\n> Centermost score if n is odd, average between centermost scores if n is even\n> less sensitive than mean to extreme scores\n> more subject to sampling variability than mean but less than mode","x":-12766,"y":86,"width":590,"height":184,"color":"4"},
		{"id":"268cc394d83e332b","type":"text","text":"**Mode**\n> most frequent score in dist\n","x":-12766,"y":270,"width":590,"height":118,"color":"4"},
		{"id":"fb1015409115b4cd","type":"text","text":"#### Measures of central tendency","x":-13356,"y":30,"width":390,"height":56,"color":"4"},
		{"id":"65b97f620a43e86a","type":"text","text":"**Mean**\n> sensitive to exact value of all scores in dist. \n> the sum of the deviations about the mean = 0 (the deviations above the mean are equal to the deviations below)\n> sensitive to extreme scores\n> sum of squared deviations of all the scores about their mean is a *minimum* (the smallest possible sum of squared deviations about any value)\n> usually, the mean is the least subject to sampling variation compared to other measures of central tendency","x":-13356,"y":86,"width":590,"height":325,"color":"4"},
		{"id":"ac6c859dab042591","type":"text","text":"**Standard deviation**\n$\\bar{X}$ = mean of sample \nμ = mean of population\n\nDeviation score for sample data = $X - \\bar{X}$\nDeviation score for population data = $X~~–~~μ$\n\n","x":-13356,"y":411,"width":590,"height":177,"color":"4"},
		{"id":"a1ef437eeb618946","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-14 at 5.09.07 PM.png","x":-13810,"y":55,"width":400,"height":131,"color":"4"},
		{"id":"d1f2ce7f7fd0ce48","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-01-14 at 11.40.38 AM.png","x":-13810,"y":183,"width":400,"height":266,"color":"4"},
		{"id":"04c75f6125b664da","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-14 at 5.04.11 PM.png","x":-13610,"y":-6,"width":200,"height":61,"color":"4"},
		{"id":"07f3572b8cb7da25","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-01-14 at 11.45.00 AM.png","x":-14121,"y":449,"width":711,"height":392,"color":"4"},
		{"id":"edf36836b0be7d64","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-01-14 at 11.38.46 AM.png","x":-14209,"y":222,"width":399,"height":189,"color":"4"},
		{"id":"ec27e89d668d1e55","type":"text","text":"**Frequency distribution**\n- **Relative**: proportion of total number of scores that occurs in each interval \n- **Cumulative** (cum *f*): number of scores that fall below the upper real limit of each interval \n- **Cumulative percentage**: percentage of scores that fall below the upper real limit of each interval (cum *f*/N * 100)\n\n","x":-12766,"y":388,"width":590,"height":251,"color":"4"},
		{"id":"06d0e150ef7c008b","type":"text","text":"##### Video 3: Three models of variability\nRange \n- No symbol\n- max - min\nStandard Deviation \n- s, $\\sigma$, std, s.d.\nVariance\n\n**Distribution of observations**\n- Central tendency\n- Skew\n- Variability: how spread out scores are\n\n##### Video 4: Models of Variability and Sum of Squares\n**Summed Deviation**\n$\\sum (X_{i}-\\bar{X})$ = always 0\n\n**Sum of squares** (SS)\n$\\sum (X_{i}-\\bar{X})^2$\n","x":-13321,"y":706,"width":711,"height":618,"color":"4"},
		{"id":"f55c82c7242623a0","type":"text","text":"# Review: E2\n- Why not 2tail\n- Rules for binom dist\n- Normal approx\n- $\\hat{d}$ vs $d_z$ vs $d_s$ \n- When can we reject null\n- **How to increase t/z score**\n- Types of power\n- How to increase/decrease type I&II errors\n\nStandard error of the mean = $\\sigma_{\\bar{X}}, s_{\\bar{X}}$\n","x":-13960,"y":-672,"width":507,"height":371,"color":"1"},
		{"id":"4e03c2197c1465cb","type":"text","text":"# Review: E1\n- Types of data\n- Skew - measures of central tendancy\n- Freq. Dist (relative, cumulative, cumulative percentage), \n- Intervals (p.53)\n- Sum of squares? Am i stupid? \n\t- $SS = \\sum{X^2} - \\frac{\\sum{X^2}}{N}$\n\t- $SS = \\sum (X_{i}-\\bar{X})^2$\n\t- p. 93\n- Spearman's $\\rho$ aka $r_s$\n\t- Use when? vs Pearson's $r$\n- Calculating r, R\n- Normal curve SD/%s\n- standard error $s_{Y|X}$\n- Dummy var\n- Types of sampling","x":-13960,"y":-1240,"width":507,"height":568,"color":"1"},
		{"id":"fe20d0e6fc5c6e18","type":"text","text":"If *chance* accounts for all reductions/increases (50/50):\n*a priori: P = .5*\n\nObserved: \na posteriori: P = .9 (9/10 reductions)\n\nWhat are chances of observing P = .9 given P = .5 (assuming $H_0$)\n","x":-9211,"y":635,"width":378,"height":261,"color":"5"},
		{"id":"c7f50c5435865584","type":"text","text":"**Controlling for Type II: Increasing Power**\n- Larger N \n- Smaller $\\sigma$pop aka variability\n- Larger difference between means aka effect size\n- Larger $\\alpha$, or 1-tail test\nGoal: ≥80%: G * Power (Laken's Shiny)\n- Most common tool for calculating power \n- Normal distribution (or z-distribution) \n\t- What is/are the z-score for: $2-tail = .05\n","x":-7738,"y":3663,"width":398,"height":367,"color":"3"},
		{"id":"eba5f6575336ad20","type":"text","text":"### $z_{obt} = \\frac{\\bar{X} - \\mu}{\\sigma_\\bar{X}}$ \n\n\n\n","x":-8950,"y":3800,"width":179,"height":76,"color":"#3d24ff"},
		{"id":"89ecfc744392cc07","type":"text","text":"Binomial dist for P = .5, N = 10, 9 observed","x":-9211,"y":915,"width":378,"height":61,"color":"5"},
		{"id":"8b65da1914ac804b","type":"text","text":"bi\nserial\nconstant P \nmutually exclusive\nindependent\n","x":-8694,"y":511,"width":265,"height":173,"color":"1"},
		{"id":"5f7ae076a2155661","type":"text","text":"## $z = \\frac{X - \\mu}{\\sigma}$$\\mu = NP$$\\sigma = \\sqrt{NPQ}$\n","x":-9807,"y":1391,"width":227,"height":160,"color":"#fe95cb"},
		{"id":"74792d31134e2c74","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-21 at 10.15.15 AM.png","x":-9211,"y":1015,"width":399,"height":232,"color":"5"},
		{"id":"cee62814e491f32b","type":"text","text":"##### 5 Rules for Binomial Distribution \n1. Series of N trials \n2. Only 2 outcomes \n3. Outcomes are mutually exclusive \n4. Outcomes are independent \n5. Expectation for P remains consistent","x":-8694,"y":165,"width":374,"height":229,"color":"6"},
		{"id":"c4df37f11b226ce7","type":"text","text":"##### Sign testing\n- Requires repeated-measures (or within-subjects/correlated groups) research design: **paired groups**","x":-9211,"y":426,"width":378,"height":152,"color":"5"},
		{"id":"ffdd5c067e555118","type":"text","text":"**The evaluation should always be two-tailed unless the experimenter will retain $H_0$ when results are extreme in the direction opposite to the predicted direction.**","x":-9604,"y":649,"width":344,"height":162,"color":"5"},
		{"id":"9df355d35a65d031","type":"text","text":"## Equations","x":-9900,"y":1323,"width":250,"height":50,"color":"#ffffff"},
		{"id":"a458e543c10c76a4","type":"text","text":"# Lectures 4-6","x":-11630,"y":-69,"width":250,"height":60,"color":"4"},
		{"id":"b9998370f033d6fe","type":"file","file":"W2023T2/W2023T2 Files/Textbooks/Formulae.pdf","x":-10800,"y":-6,"width":840,"height":755},
		{"id":"6145bf896d56987f","type":"file","file":"W2023T2/W2023T2 Files/Textbooks/ztable.pdf","x":-10880,"y":820,"width":840,"height":596},
		{"id":"93e9dfdae57a51bc","type":"text","text":"##### **Video 1: Summation**\n$\\sum^{N}_{i=1} X_{i} = X_{1} + X_{2} + ... X_{N}$ \n##### Video 2: Summed differences\n$\\sum(X_{i}-\\bar{X}) = (X_{1}-7.8) + (X_{2}-7.8) ... + (X_{N}-7.8)$\n$\\bar{X}$ = sample mean (eg. 7.8)\n$\\bar{X} = \\frac{\\sum X_{i}}{N}$\n\n\n\n\n\n\n\n\n","x":-11630,"y":-9,"width":711,"height":351,"color":"4"},
		{"id":"d6e75322cf065a6a","type":"text","text":"Regression = prediction\nLine of best fit = regression line\n> Balances the magnitude of positive and negative errors\n> Minimizes $\\sum (X-Y')^2$\n\n**Formula for linear regression line**\n$Y' = b_Y X + a_Y$\n$Y'$ = Criterion variable\n$X$ = predictor variable\n$b_Y$ = slope of regression line\n$a_Y$ = intercept\n\n**Calculating $b_Y$**\nWhen you know r, $s_Y$, and $s_X$:\n$b_Y = r\\frac{s_Y}{s_X}$\n\n**When you only have raw data:** (screenshot)","x":-11640,"y":3323,"width":711,"height":461,"color":"6"},
		{"id":"204d9ad67272d215","type":"text","text":"##### Video 7: Alternative Formulas for _SS, s,_ & _$s^2$_  \n**Sum of squares**\n$\\sum X^2 - \\frac{(\\sum X)^2}{N}$\n\n**Standard deviation**\n$\\sqrt \\frac{SS}{N-1}$\n\n**Variance**\n$\\frac{SS}{N-1}$\n\n##### Video 8: Calculating Sum of Squares, Standard Deviation, & Variance\n**Example: Sugar in cereal**\n\n##### Video 9: Conceptual Wrap-up for SS, s, & $s^2$\nSS: all squared deviation scores \nVariance: N-1 acts as correction because sample variance tends to underestimate variance\nStandard deviation is unsquared variance: average raw deviance from mean","x":-11899,"y":706,"width":711,"height":589,"color":"4"},
		{"id":"e992a3eb13827ac1","type":"text","text":"# Lectures 11-12","x":-11640,"y":3263,"width":320,"height":60,"color":"6"},
		{"id":"2f23e8b78cce17e3","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.36.13 AM.png","x":-11999,"y":2720,"width":710,"height":361,"color":"5"},
		{"id":"8323da3e6c6a495e","type":"text","text":"##### Inferential statistics\n\nAllows us to calculate a raw score for any given z-score, its percentile, and the probability of getting that score (from area under curve)","x":-8833,"y":-69,"width":418,"height":150,"color":"3"},
		{"id":"cd8b5874f0711d60","type":"text","text":"##### Sampling Distribution of Sample Means\nMean of the distribution of sample means = population mean \n### $\\mu_{\\bar{X}} = \\mu$\n\n","x":-8320,"y":1606,"width":426,"height":209,"color":"4"},
		{"id":"d0f69278a5f8efb4","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.45.13 AM.png","x":-11979,"y":3443,"width":339,"height":293,"color":"6"},
		{"id":"65e8ef7295f20507","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-21 at 10.25.07 AM.png","x":-8038,"y":3663,"width":271,"height":189,"color":"1"},
		{"id":"57dbd2783906f41b","type":"text","text":"**Reducing Type I errors increases Type II errors** \n(Reducing Type II errors does not increase Type I errors) \n- We need to balance errors: \n\t- What are the consequences of failing to find a real effect? \n\t- What are the consequences of “finding” an effect that is not real?","x":-7318,"y":3663,"width":440,"height":280,"color":"3"},
		{"id":"1ce442acddadf833","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.35.26 AM.png","x":-11997,"y":2255,"width":708,"height":176,"color":"5"},
		{"id":"2d84e204045678ca","type":"text","text":"**When do we reject null?**\n- Reject when $z_{obt}$ gets large enough \n\t- Specifically when ±1.96 (for 2-tail tests): $z_{crit}$ is always ±1.96 or ±1.645 (for 1-tail) \n\t- Critical values change for other test statistics (e.g., t, F) \n\n##### Increasing $z_{obt}$\n- Larger N increases $z_{obt}$ \n\t- (because it reduces $σ_{\\bar{X}}$) \n- Smaller $\\sigma$pop  increases $z_{obt}$ \n\t- (b/c it reduces $σ_{\\bar{X}}$) \n- Larger difference ($\\bar{X}$– μ) \n- Larger 𝛂, b/c it reduces $z_{crit}$ \n\t- ($z_{obt}$ does not need to be as extreme to reject $H_0$) \n- Directional $H_0$/$H_1$, b/c it reduces $z_{crit}$  \n\t- ($z_{obt}$ does not need to be as extreme to reject $H_0$)","x":-7952,"y":2988,"width":560,"height":540,"color":"2"},
		{"id":"7035b2df013db212","type":"text","text":"#### $\\hat{d} = \\frac{\\bar{X}_{obt}-\\mu}{s}$\nStandardized difference/ES: for comparing a sample mean to population mean","x":-9809,"y":2251,"width":549,"height":140,"color":"3"},
		{"id":"4f375c8195dce0a8","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-21 at 4.06.10 PM.png","x":-9809,"y":1911,"width":549,"height":312,"color":"#80ff00"},
		{"id":"2d640c9fe8bbe2da","type":"text","text":"Goal: ≥80%: G * Power (Laken's Shiny)\n- Most common tool for calculating power \n","x":-8750,"y":3876,"width":560,"height":84,"color":"3"},
		{"id":"5ce44e9dbd0b55fb","type":"text","text":"Standard error\nz-obtained","x":-8038,"y":3863,"width":271,"height":60,"color":"3"},
		{"id":"cea65f16c7e6bfbe","type":"text","text":"### **Example data**","x":-11640,"y":3784,"width":711,"height":72,"color":"6"},
		{"id":"f48a6efab1b3989c","type":"text","text":"### $t_{obt} = \\frac{\\bar{X}-\\mu}{s_{\\bar{X}}}$\n","x":-9145,"y":3800,"width":179,"height":76,"color":"#80ff00"},
		{"id":"1b23f97c812bf7a8","type":"text","text":"##### Standard error of the mean\nStandard deviation of distribution of sample means = population standard deviation divided by sqr N\n### $\\sigma_\\bar{X} = \\frac{\\sigma}{\\sqrt N}$\n= Average difference between $\\bar{X}$s and $\\mu$\n\nAs $\\sigma$ (pop SD) decreases, $\\sigma_\\bar{X}$ (sample means SD) decreases \n","x":-8320,"y":1843,"width":426,"height":360,"color":"4"},
		{"id":"09da228077bb0501","type":"text","text":"As N increases, sampling dist of means approaches ***normal distribution (z)***","x":-8320,"y":2223,"width":426,"height":61,"color":"4"},
		{"id":"d4ec824180091db9","type":"text","text":"Raw effect sizes are expressed in **original units of measurement**","x":-8612,"y":2518,"width":406,"height":61,"color":"3"},
		{"id":"7005af8a6050fe75","type":"text","text":"#### $\\hat{d} = \\frac{\\bar{X}_{obt}-\\mu}{s}$\nStandardized difference/ES: for comparing a sample mean to population mean","x":-8612,"y":2678,"width":406,"height":140,"color":"3"},
		{"id":"d33da342b00a306e","type":"text","text":"Standardized effect sizes are divided by s: same scale (like z)","x":-8612,"y":2599,"width":406,"height":61,"color":"3"},
		{"id":"1baeeb57c1b41423","type":"text","text":"pp.187-220","x":-13600,"y":5583,"width":180,"height":60,"color":"3"},
		{"id":"2ec602bbbd19ac7a","type":"text","text":"##### Why sample? \n- Save time and expense\n- Ensure accuracy of measurement\n- Ensure quality/durability \nSample must be **representative** for valid generalizations to be made\n\n\n\n\n\n\n","x":-12880,"y":5643,"width":711,"height":191,"color":"3"},
		{"id":"6db61b3e97521a9b","type":"text","text":"# Lectures 13-15","x":-12880,"y":5583,"width":320,"height":60,"color":"3"},
		{"id":"1b771818fec26b8b","type":"text","text":"# Chapter 8","x":-13420,"y":5583,"width":250,"height":60,"color":"3"},
		{"id":"9c6513c157fdf84c","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-10 at 7.52.46 PM.png","x":-12711,"y":4797,"width":710,"height":483,"color":"6"},
		{"id":"92e19eee6d62a5e8","type":"text","text":"### Categorical Predictors in Regression\nArbitrary numbering can mess with regression \n**Multiple regression**\nCriterion Y: grumpiness\nPredictor $X_1$: Hours of sleep \nPredictor $X_2$: $Rain_{L1}$ or $Shine_{L2}$\n\tThe numerical values are meaningless, it's essentially a binary system which refers to non-numerical meaning aka 1 = rain, 2 = shine\n\n$b_1$ = -8.715\n$b_2$ = -1.455\na = 124.442\n\nIf $X_2$ is coded as '2' aka shine → 1.455 fewer grumpy units \n\n### Dummy Variables\nIf predictor $X_2$ has **3 potential values**: Rain, Clouds, or Shine, *we can no longer assign a slope coefficient ($b_2$)* → \"**Dummy variable**\"\n- Collapse variables Clouds + Shine together and take average, where Clouds or Shine = 0 and Rain = 1\n- (Or) Compare Clouds vs Rain for one slope and Clouds vs Shine for another\n\nCoding of categorical predictors changes interpretation of *b*\nDummy coding is most common + simplest, but many other methods exist depending on hypotheses\n","x":-12000,"y":4452,"width":711,"height":741,"color":"6"},
		{"id":"a69832afbe3e1df1","type":"text","text":"### Multiple regression\n**Symbols**\nr = normal distribution\n$r_s$ non-normal distribution, outliers = spearman's\n$r_pb$ = one continuous (interval, ratio) variable, one dichotomous (binary) variable \n$\\phi$ = two dichotomous variables (chi square!)\n\n$\\beta$ = standardized slope coefficient = $z_x$, $z_y$\n$b$ = unstandardized slope coefficient = $X_i, Y_i$\n$r^2$ = variability explained by predictor variable\n$R^2$ = variability explained by regression model  ","x":-12711,"y":4452,"width":711,"height":345,"color":"6"},
		{"id":"ad67697c0fb8c498","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.20.55 PM.png","x":-12280,"y":3836,"width":199,"height":107,"color":"6"},
		{"id":"638dd08d8ba2706e","type":"text","text":"**t-dist approximates z-dist as N increases**","x":-7760,"y":2178,"width":328,"height":61,"color":"#80ff00"},
		{"id":"01edde0fd3a6320d","type":"text","text":"#### Sampling procedures\n1. **Self-selection**: volunteer participants\n\t1. Efficient / Often unrepresentative\n2. **Quota sampling**: technique sets groups and their proportions to sample from \n\t1. Less efficient, limited by ignorance of the importance/quantity of each group, biased towards prototypical example of group\n3. **Random sampling**: every person has equal chance of participating\n\t1. Nearly impossible, very inefficient, but fully representative\n\n**Replacement**: can the same case be selected twice? \n- Sampling with vs without replacement: with, can sample case more than once","x":-12880,"y":5873,"width":711,"height":378,"color":"3"},
		{"id":"0259d813551d9290","type":"text","text":"### Probability\n**Dependent successive events**\nProbability of being dealt 2 kings (\"Cowboys\") = 4/52 * 3/51 = 12/2652 \n- 3/51 bc no replacement\nProbability of being dealt a matching pair = 12/2652(13) = .0588\n\nOr, 1 (probability of being dealt any card) x 3/51 (probability of being dealt a matching card) = .0588\n\nProbability of being dealt 2 hearts = .0588\n.235","x":-12001,"y":5739,"width":711,"height":354,"color":"#fe62bd"},
		{"id":"0db537fd85503624","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.14.53 PM.png","x":-13800,"y":3476,"width":380,"height":327,"color":"6"},
		{"id":"c9941312bdb8173f","type":"text","text":"### Standard Error","x":-13421,"y":4452,"width":710,"height":60,"color":"6"},
		{"id":"83a03b717cb29dfc","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-10 at 7.21.42 PM.png","x":-13421,"y":4512,"width":710,"height":239,"color":"6"},
		{"id":"3fa90c6626731b7b","type":"text","text":"### Homoscedasticity","x":-13421,"y":4751,"width":711,"height":60,"color":"6"},
		{"id":"676d63690cd7903a","type":"text","text":"How far off, on average, is our regression line from the actual data? \n68% of scores will be within 1 standard error \n\nHomoscedasticity = consistent error\n**Standard errors are only meaningful if the variability in Y is constant over values of X**\n\n**a) is homoscedastic, b) is not**\n","x":-13421,"y":4811,"width":711,"height":233,"color":"6"},
		{"id":"83a8ce68a28716cb","type":"file","file":"W2023T2/W2023T2 Files/Slides/annotated-PSYC218-L14.pdf","x":-13420,"y":5703,"width":391,"height":340,"color":"3"},
		{"id":"605d1b74a5fed095","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 2.32.05 PM.png","x":-13820,"y":1858,"width":401,"height":54,"color":"5"},
		{"id":"fe94b4ff4a88ac40","type":"text","text":"pp. 159-178","x":-13600,"y":3263,"width":180,"height":60,"color":"6"},
		{"id":"fd476ba534bd2f3b","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.10.29 PM.png","x":-13421,"y":5044,"width":710,"height":370,"color":"6"},
		{"id":"6d04e9c1e0377307","type":"text","text":"## Regression\n#### Least-squares regression line: \n- The prediction line that minimizes the total error of prediction, according to the least-squares criterion of \u0002$\\sum(Y – Y')^2$\n\n#### Standard error\n**Standard error of the estimate: difference between prediction and actual value**\n**We can assume the points are normally distributed about the regression line**(Figure 7.5). \nIf the assumption is valid and we were to construct two lines parallel to the regression line at distances of $±1~s_{Y|X}$, \u0006$±2~s_{Y|X}$, and $±3~s_{Y|X}$ → \n- 68% lie between $±1~s_{Y|X}$\n- 95% lie between $±2~s_{Y|X}$\n- 99% lie between $±3~s_{Y|X}$\n\n#### **Considerations for using Linear Regression:**\n- The relationship between X and Y must be linear\n- The basic computation group should be representative of the prediction group\n- Only used for the range of the variable on which it is based\n\n##### Pearson $r$ is t he slope of the least-squares regression line when the scores are plotted as $z$-scores\n\n## Multiple Regression\n### Calculating Criterion based on 2+ Predictors: $Y' = b_{1}X{1}+b_{2}X_{2}+a$\n### $R^2$, aka: \n Multiple coefficient of determination\n Proportion of Variance based on 2+ Predictors\n Squared multiple correlation\n","x":-13420,"y":3323,"width":1140,"height":1011,"color":"6"},
		{"id":"4b89d0f58239124a","type":"text","text":"$D_i$ = $\u0002R(X_i) - R(Y_i)$\n$R(X_i)$ = rank of the ith X score \n$R(Y_i)$ = rank of the ith Y score \nN \u0002 number of pairs of ranks","x":-13819,"y":1912,"width":400,"height":167,"color":"5"},
		{"id":"da888ec7ed556940","type":"text","text":"#### Spearman's $\\rho/r_s$","x":-13819,"y":1808,"width":400,"height":50,"color":"5"},
		{"id":"09cbb7f9e7eb6d7e","type":"text","text":"#### Meehl's 6th Law of Psychology\n**Stastical \"crud\":** everything correlates with everything else, but most aren't meaningful = adding more predictors always increases $R^2$ = we need to remove as many *irrelevant* predictors as possible\n\nWhen reporting $R^2$, **always** also report adjusted $R^2$","x":-11284,"y":4452,"width":711,"height":201,"color":"6"},
		{"id":"bef75428622d704e","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.46.44 AM.png","x":-11640,"y":4151,"width":711,"height":250,"color":"6"},
		{"id":"5a2dfa8a1f380ce6","type":"text","text":"#### $SS_X$ =  <span style=\"color:#c00000\">1.811</span>","x":-11640,"y":4402,"width":711,"height":50,"color":"6"},
		{"id":"5f1a734992b95565","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.45.33 AM.png","x":-11640,"y":3856,"width":710,"height":295,"color":"6"},
		{"id":"2e1aef17a71a1c9f","type":"text","text":"$n$ = # of people in *one condition*\nN = # of participants altogether\n$s_{(\\bar{X_1}-\\bar{X_2})}$ = SD of mean 1 minus mean 2 \n- Divide by number of people *per condition (n)* = no double counting ","x":-2360,"y":1040,"width":356,"height":184,"color":"#0070c0"},
		{"id":"1f37bff2dfabe54b","type":"text","text":"$s^2_w$ = average, corrected by degrees of freedom\nif groups $n_1 = n_2$, no weighting","x":-2360,"y":1270,"width":356,"height":94,"color":"#0070c0"},
		{"id":"94afdf5e35dde4ea","type":"text","text":"## Textbook notes","x":-2910,"y":-160,"width":250,"height":60,"color":"6"},
		{"id":"7e8520e4508a66d0","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-14 at 2.22.13 PM.png","x":-4443,"y":1116,"width":365,"height":275,"color":"4"},
		{"id":"4b5c81f4ac3ae61b","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-14 at 2.37.04 PM.png","x":-4443,"y":1391,"width":364,"height":104,"color":"4"},
		{"id":"c911151a1b063f09","type":"text","text":"Correlated (paired) t-test advantage: less reliant on population comparison - greater control","x":-4443,"y":1519,"width":369,"height":125,"color":"6"},
		{"id":"e5de23a96153c9bf","type":"text","text":"Single sample = raw scores\nPaired = difference scores","x":-4443,"y":1644,"width":369,"height":75,"color":"6"},
		{"id":"837db60f38831ace","type":"text","text":"Mean of population difference (D) scores $\\mu_D$ = 0, $\\sigma_D$ = unknown\nNull hypothesis= normal dist of D scores ","x":-4443,"y":1719,"width":369,"height":94,"color":"6"},
		{"id":"bfbcb44cd04b65b1","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-27 at 11.46.27 AM.png","x":-3150,"y":1328,"width":346,"height":136,"color":"#0070c0"},
		{"id":"c7642cd8b42d20c3","type":"text","text":"**Example: Survey on spice preference**\nN = 8\n$n_1$ = $n_2$ = 4\nIV = early or late 218 section\nDV = How much do you like spicy food *(1 = not at all, 3 = a lot)*\n\n$\\bar{X_1}$ = 1.75\n$\\bar{X_2}$ = 2.5\n\n$SS_1$ = 2.75\n$s^2$ = ${\\frac{SS}{N-1}}$\n$s_1^2$ = $\\frac{2.75}{4-1}$ = .9167\n\n$SS_2$ = 1\n$s_2^2$ = $\\frac{1}{4-1}$ = .3332\n\n$s^2_w$= $\\frac{s_1^2+s_2^2}{2}$ = $\\frac{.9167 + .3332}{2}$ = .62495\n*(because $n_1$ = $n_2$)*\n\n$t_{obt}$ = $\\frac{\\bar{X}_1-\\bar{X}_2}{\\sqrt{s_{w}^2(\\frac{1}{n_1}+\\frac{1}{n_2})}}$\n$\\frac{1.75-2.5}{\\sqrt{.62495(\\frac{1}{4}+\\frac{1}{4})}}$ = 0.5592383213\n\n> $t_{obt}$ = -1.341\n\n$\\alpha_{2-tail}$ = .05\n*df* = 3\n\n> $t_{crit}$ = ±2.447\n\n> ∴ **Retain $H_0$**\n\n**Cohen's $d_s$**\n$d_s$ = $\\frac{\\bar{X_1}-\\bar{X_2}}{\\sqrt{s^2_w}}$ \n$\\frac{-.75}{\\sqrt{.62495}}$ = -0.9487\n\n","x":-1880,"y":845,"width":538,"height":871,"color":"3"},
		{"id":"ae5430a938e0b7b0","type":"text","text":"\n| ID  | Section | Spice Att. |\n| --- | ------- | ---------- |\n| 1   | 2       | 3          |\n| 2   | 2       | 2          |\n| 3   | 1       | 2          |\n| 4   | 1       | 1          |\n| 5   | 1       | 3          |\n| 6   | 2       | 2          |\n| 7   | 1       | 1          |\n| 8   | 2       | 3          |\n","x":-1320,"y":845,"width":270,"height":360,"color":"3"},
		{"id":"d2a6a8530dbde581","type":"text","text":"Sampling distribution of difference scores: take sampling dist 1 and compare to sampling dist 2 = sampling of diff 1 - 2 ","x":-3150,"y":1503,"width":346,"height":125,"color":"6"},
		{"id":"955495005e8fc005","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 2.21.18 PM.png","x":-4751,"y":3004,"width":831,"height":437},
		{"id":"a4f3d4acaf2233de","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 2.22.29 PM.png","x":-3835,"y":3004,"width":913,"height":437},
		{"id":"c3567ca2d755c425","type":"text","text":"#### Effect size in ANOVA\n> Closer to regression effect size than *t*-test\n> $R^2, r^2$ ; $\\eta^2, \\omega^2$\n\n$\\eta$ is biased-upwards\n$\\omega$ is much better – but recent so most still use $\\eta^2$ :(\n\nScale (for $\\eta^2, \\omega^2$)\n.01-.05 = small \n.06-.13 = medium\n≥.14 = large\n\nMost common = $\\eta^2_p$","x":-5559,"y":3299,"width":471,"height":363},
		{"id":"e52ebe2372ed527d","type":"text","text":"Why not just multiple t-tests? Each comes with its own $\\alpha$ →  increases T2 error rate\n- ANOVA has \"family wise\" $\\alpha$: we can put in all data at once, do all comparisons at once → exactly .05 $\\alpha$, max. power","x":-5982,"y":3299,"width":423,"height":180},
		{"id":"4ca16b9612e2b409","type":"text","text":"**Assumptions**\n1. Sampled pop.s are normally dist. \n2. DV is interval or ratio\n3. Homogeneity of variance\n\t- Like t-test, we avg variance from diff conditions \n\nANOVA is fairly robust to violations of assumptions (altho less than *t*) assuming N is relatively large and *n*s are relatively equal \n","x":-5895,"y":2675,"width":423,"height":324},
		{"id":"4b0da278f8291b9e","type":"text","text":"##### $MS_{within}$\n> SS for scores *within* each group\n\n - Error\n - Residual, observations\n - How much DV varied as function of indiv. differences","x":-3432,"y":2640,"width":429,"height":246},
		{"id":"2e20bf38a193568b","type":"text","text":"##### $MS_{between}$\n> SS for difference between each *group* mean \nand *grand* mean \n- Effect variance\n- Systematic, groups\n- How much DV varied as function of IV","x":-3432,"y":2395,"width":429,"height":245},
		{"id":"54a4bdfcd3189c33","type":"text","text":"#### $df$\n- *numerator df*: corrects SS between-groups = k - 1 \n  (one $s^2$) \n- *denominator df*: corrects SS within-groups = N - k \n  (k number of $s^2$s)","x":-4526,"y":2658,"width":525,"height":189},
		{"id":"f117976d05efd07f","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 12.52.07 PM.png","x":-4998,"y":2486,"width":399,"height":316},
		{"id":"0fa0a469731d79a3","type":"text","text":"**F-distribution is *asymmetrical***","x":-4998,"y":2800,"width":296,"height":50},
		{"id":"5f3c4554d3a2cf28","type":"text","text":"**Formula for $CI$ (steps)**\n- Assume $\\bar{X}_{obt} = \\mu$\n- Add or subtract from best guess $\\bar{X}_{obt}$ to set upper/lower bounds\n- Multiply standard error by *t* value which sets upper/lower tail of dist. of sample means\n","x":-5580,"y":666,"width":414,"height":250,"color":"5"},
		{"id":"f529b511c68b88d9","type":"text","text":"If p val is **not** significant, CI should include 0 ","x":-6040,"y":1020,"width":414,"height":60,"color":"5"},
		{"id":"037a75a4d980765f","type":"text","text":"***Assuming a posteriori observation is true, here are plausible pop. param. given sampling variability***","x":-6038,"y":902,"width":412,"height":94,"color":"5"},
		{"id":"385ff23a0e75b4cc","type":"file","file":"W2023T2/W2023T2 Files/Screenshot 2024-04-22 at 1.54.12 PM.png","x":-2480,"y":156,"width":413,"height":207,"color":"6"},
		{"id":"a5a62855fd65a7f4","type":"text","text":"**Correlated vs independent groups** - analyzing *same data*\n> More likely to reject $H_0$ w/ **correlated** test: why? \n\n**SS** - correlated uses $SS_D$ & ind. uses $SS_1 + SS_2$ \n$SS_D$ < $SS_1 + SS_2$: decreased variability\n\n***df*** - ind. has 2x *df*\n\n\n","x":-2980,"y":156,"width":500,"height":207,"color":"6"},
		{"id":"d2a174ba23f08f65","type":"text","text":"Power:\nPlanned comparisons (*a priori*) > *post hoc*","x":-5100,"y":3727,"width":398,"height":93,"color":"6"},
		{"id":"291ffde1439cc6b8","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 2.39.42 PM.png","x":-5958,"y":3519,"width":399,"height":143},
		{"id":"86a5061a3a3c5513","type":"text","text":"##### *Limitation:* always non-directional","x":-5243,"y":3085,"width":388,"height":50},
		{"id":"f210c4105ad4a6d3","type":"text","text":"### $\\eta^2 = \\frac{SS_{between}}{SS_{total}}$\n","x":-5021,"y":3451,"width":250,"height":60,"color":"6"},
		{"id":"115c886cf839e34d","type":"text","text":"### $\\omega^2 = \\frac{\\sigma^2_{between}}{\\sigma^2_{between}+\\sigma^2_{within}}$\n","x":-5021,"y":3579,"width":250,"height":83,"color":"6"},
		{"id":"20dbc2bf81447c0d","type":"text","text":"Larger CI: \nMore *certainly* contain pop. parametres\nLess practically useful, difficult to falsify ","x":-6040,"y":1080,"width":414,"height":94,"color":"5"},
		{"id":"4c29015b8d332b11","type":"text","text":"**Example: Memory SPAN**\n\n**ANOVA hypothesis format:**\n$H_0: \\mu_{digits} = \\mu_{letters} = \\mu_{words}$\n$H_1$ = *At least one mean differs*","x":-6040,"y":3085,"width":538,"height":146,"color":"3"},
		{"id":"9fbffb9c7e13f52f","type":"text","text":"**ANOVA for 2 groups: $F = t^2$**","x":-5895,"y":2395,"width":295,"height":50},
		{"id":"2bb2423aa46862ef","type":"text","text":"95% CI can be for many different things, not just the population mean. For example, an effect size could have a corresponding 95% CI, which will instead represent the range that we are 95% confident contains the true effect size. Conceptually, the 95% CI is the same, just for different things.","x":-6038,"y":1188,"width":458,"height":189,"color":"3"},
		{"id":"f44957349f2cd767","type":"file","file":"W2023T2/W2023T2 Files/Screenshot 2024-04-22 at 2.03.19 PM.png","x":-6040,"y":1391,"width":460,"height":129,"color":"6"},
		{"id":"43691a19dc13e1db","type":"text","text":"**2-sample: $\\bar{X}_{obt}$ → $\\bar{X}_1 - \\bar{X}_2$, $s_{\\bar{X}}$ → $s_{\\bar{X}_1-\\bar{X}_2}$**","x":-6038,"y":1560,"width":458,"height":74,"color":"6"},
		{"id":"a66f796965807eee","type":"file","file":"W2023T2/W2023T2 Files/Screenshot 2024-04-22 at 12.55.24 PM.png","x":-3150,"y":1628,"width":346,"height":240,"color":"6"},
		{"id":"30f9f37697ccea63","type":"text","text":"#### Computational t-test formulas","x":-2730,"y":1270,"width":346,"height":87,"color":"#0070c0"},
		{"id":"4cc7f7cfd8b08427","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-27 at 11.26.18 AM.png","x":-2730,"y":1357,"width":346,"height":436,"color":"#0070c0"},
		{"id":"a609d610b8a79c56","type":"text","text":"**Assumption of homogeneity of variance (between group 1 and 2)** \n- If they are approx similar, we don't need to correct for it - if they are not, we do \n- We won't have to calculate non-homogeneous things–just matters to know that we make it here because we're getting a weighted average of variance","x":-2360,"y":1396,"width":356,"height":328,"color":"#0070c0"},
		{"id":"25642519a6eb03b5","type":"text","text":"**Confidence interval**\n> Characterize our confidence in **parameter** estimates\n> Describe variability of the statistical model (variability of $\\bar{X}$ around $\\mu_\\bar{X}$)\n> Set by researcher, like $\\alpha$","x":-6040,"y":666,"width":414,"height":210,"color":"5"},
		{"id":"64063707e9e7125e","type":"text","text":"\"One-way\" ANOVA = 1 IV\n\"Factorial\" ANOVA = multiple IVs\n","x":-5895,"y":2581,"width":343,"height":61},
		{"id":"2698d9eb9f21e46b","type":"text","text":"*t*-test tends to be more reliable & \nis easier to understand effect size","x":-5895,"y":2475,"width":343,"height":80},
		{"id":"e0c141bc4aade24e","type":"text","text":"### $\\hat{d} = \\frac{|\\bar{D}_{obt}|}{s_D}$\n\n","x":-4443,"y":1840,"width":177,"height":70,"color":"6"},
		{"id":"50e1d0679e263874","type":"text","text":"**Conditions for using independent t-test**\n> Compare 2 groups\nDV is interval/ratio\nDV is ~normal (both n > 30)\nAbsence of outliers\nHomogeneity of variance\n\nHowever, t-tests are relatively robust, so it's ok to violate some","x":-3150,"y":990,"width":365,"height":290,"color":"#0070c0"},
		{"id":"48e596e6b760ec5f","type":"text","text":"### Independent t-test","x":-3180,"y":870,"width":270,"height":60,"color":"#0070c0"},
		{"id":"8917f3519ae28b3e","type":"text","text":"### ANOVA Output","x":-4460,"y":2913,"width":250,"height":60},
		{"id":"2be899485d80c236","type":"text","text":"## Choosing the correct inferential test","x":-3967,"y":-69,"width":520,"height":55,"color":"#fec000"},
		{"id":"6cbc76b67808850a","type":"text","text":"## Comparing $\\mu$ and sample","x":-4426,"y":105,"width":416,"height":50},
		{"id":"c7673c5808e15b92","type":"text","text":"### Paired t-test","x":-4260,"y":666,"width":250,"height":60,"color":"#00b050"},
		{"id":"15cb10d152100cf1","type":"text","text":"**Conditions for using paired t-test**\n> Compare exactly 2 groups\n> DV is interval/ratio\n> DV is approximately normal, or N>30\n> Absence of outliers\n\nHowever, t-tests are relatively robust, so it's ok to violate some","x":-4443,"y":795,"width":365,"height":264,"color":"#00b050"},
		{"id":"5b2f0f2ddfaec66d","type":"text","text":"**Conduct a paired t-test**","x":-4010,"y":761,"width":365,"height":60,"color":"#00b050"},
		{"id":"e0ecb537b99cb430","type":"text","text":"##### Conceptual elements of ANOVA formula","x":-3932,"y":2378,"width":429,"height":50},
		{"id":"ca6a46604644e725","type":"text","text":"#### Conceptual t-test formulas","x":-2730,"y":820,"width":346,"height":50,"color":"#0070c0"},
		{"id":"34d2ed23fac7100f","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-27 at 11.23.10 AM.png","x":-2730,"y":870,"width":346,"height":400,"color":"#0070c0"},
		{"id":"6759f2318a3a1b0c","type":"text","text":"$\\bar{D}$ = mean difference score\n$s_{\\bar{D}}$ = standard error of the mean difference\n$s_w$ = weighted standard error btwn $\\bar{X_1}$ & $\\bar{X_2}$\n","x":-2360,"y":845,"width":356,"height":157,"color":"#0070c0"},
		{"id":"c25c1e3bbf8cb08f","type":"text","text":"### Single sample t-test","x":-4631,"y":324,"width":291,"height":60},
		{"id":"9e5491bff6795c9f","type":"text","text":"### z-test","x":-4040,"y":324,"width":145,"height":60},
		{"id":"a7ab7e2f2b4ebe98","type":"text","text":"**Example: Stroop task**\nN = 10\nDV = ms until correct response\n\n$\\bar{D}_{obt}$ = 268.5\n$SS_D = \\sum{D^2} - \\frac{(\\sum{D})^2}{N}$ = 750,810.5\n$s_{\\bar{D}}$ = $\\sqrt{\\frac{750,810.5}{10(10-1)}}$\n$\\frac{\\bar{D}_{obt}}{s_{\\bar{D}}}$= $\\frac{268.5}{91.3364050578}$ \n\n> $t_{obt}$ = 2.940\n\n$\\alpha_{2-tail}$ = .05\n*df* = 9\n\n> $t_{crit}$ = ±2.262\n\n> ∴ **Reject $H_0$**\n\n**Cohen's $d_z$**\n$s_{D} = \\sqrt{\\frac{SS_D}{N-1}}$ = 288.83\n$\\frac{\\bar{D}_{obt}}{s_D}$ = $\\frac{268.5}{288.83}$\n$d_z$ = .930 \n→ *large effect size*\n","x":-3914,"y":891,"width":538,"height":612,"color":"3"},
		{"id":"c1cf6eb53830e342","type":"text","text":"Median F value = 1","x":-5440,"y":2706,"width":250,"height":60,"color":"6"},
		{"id":"96505e803990ea5e","type":"text","text":"$s^2_{w} = MS_{within}$ when k = 2","x":-5440,"y":2766,"width":250,"height":60,"color":"6"},
		{"id":"59cd6f2bbaa60ea1","type":"file","file":"W2023T2/W2023T2 Files/Screenshot 2024-04-22 at 2.27.17 PM.png","x":-5440,"y":2825,"width":352,"height":115},
		{"id":"d47995cc5806446e","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 2.20.31 PM.png","x":-3932,"y":2622,"width":428,"height":160},
		{"id":"6d62878895d79aca","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 2.16.47 PM.png","x":-3932,"y":2465,"width":428,"height":105},
		{"id":"c8318dd89297859d","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 2.09.45 PM.png","x":-4523,"y":2083,"width":525,"height":240},
		{"id":"248bc6d3fdf1e504","type":"text","text":"**Example: Lightbulb burnout**\n$\\bar{X}_{obt}$ = 215\n*s* = 8 hrs\nN = 200\n$CI_{99}$\n\n$\\bar{X}_{obt}± (s_{\\bar{X}}*t_{.025})$\n\n$s_{\\bar{X}} = \\frac{s}{\\sqrt{N}} = \\frac{8}{\\sqrt{200}} = 0.5657$ \n\n$t_{.005}(199) = 2.617$\n\n$\\mu_{lower} = 215 - (0.5657 * 2.617) = 213.520$ \n$\\mu_{upper} = 215 + (0.5657 * 2.617) = 216.480$\n\n*M* = 215 hours (*s* = 8), $CI_{99}[213.52,216.48]$ \n","x":-5558,"y":1226,"width":538,"height":418,"color":"3"},
		{"id":"dccae38f422f0d1b","type":"text","text":"$.025*2 = .05$, for $CI_{95}$\n> 2.5% of time, actual parameter will be > interval, 2.5% of time, < interval\n","x":-5200,"y":981,"width":450,"height":130,"color":"5"},
		{"id":"bf61e01a1b3bfb2b","type":"text","text":"##### $CI$ = $\\bar{X}_{obt}± (s_{\\bar{X}}*t_{.025})$\n\n","x":-5496,"y":981,"width":246,"height":64,"color":"5"},
		{"id":"e3df2e6351692191","type":"text","text":"### Corrected Post-Hoc Tests","x":-3543,"y":2913,"width":330,"height":60},
		{"id":"9686d5f4c66fde75","type":"text","text":"## Comparing samples","x":-3406,"y":126,"width":326,"height":60},
		{"id":"231eda749775b6d2","type":"text","text":"### 356-436(421)","x":-5280,"y":-320,"width":250,"height":60},
		{"id":"e38e9266aa11285f","type":"text","text":"# t-table - 604","x":-4853,"y":-377,"width":250,"height":60,"color":"3"},
		{"id":"24debcf6c74409e8","type":"text","text":"# F table - 606\n","x":-4853,"y":-317,"width":250,"height":60,"color":"3"},
		{"id":"0eab514516727fda","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-04-17 at 12.42.20 PM.png","x":-4988,"y":1226,"width":366,"height":418},
		{"id":"70d31a97c3890bfc","type":"text","text":"**Conceptual steps to creating $CI_{x}$:**\n1. Determine total area under curve AUC that we're willling to be wrong\n2. Split total AUC between 2 tails (218 = 2tail always)\n3. Visualize $\\frac{AUC}{2}$ in upper/lower tail\n4. Determine val of *t* for $\\frac{AUC}{2}$, using appropriate *df*","x":-5100,"y":666,"width":494,"height":223,"color":"5"},
		{"id":"4532073b216a7be9","type":"text","text":"**If *t* = 1?**\n→$\\bar{X}_{obt}$ is 1 $s_{\\bar{X}}$ from $\\mu$\n∴ $CI_{68}$","x":-5540,"y":1066,"width":250,"height":122,"color":"5"},
		{"id":"783e46f267fa0fd9","type":"text","text":"# Ch. 14","x":-5280,"y":-497,"width":250,"height":60,"color":"1"},
		{"id":"01234691f719471e","type":"text","text":"# Ch. 15","x":-5280,"y":-437,"width":250,"height":60,"color":"1"},
		{"id":"2e86a7c408c10d2b","type":"text","text":"# binomial 595\n","x":-4853,"y":-437,"width":250,"height":60,"color":"3"},
		{"id":"32c33f2054069531","type":"text","text":"# z-table 591","x":-4853,"y":-497,"width":250,"height":60,"color":"3"},
		{"id":"cfd509902d8bed4d","type":"text","text":"# Review (E3)\n- Conditions/assumptions for testing\n- $\\bar{D}$ calculation\n- ","x":-5580,"y":-805,"width":507,"height":169,"color":"1"},
		{"id":"4db6807ae3778886","type":"text","text":"- Why is it called analysis of variance? \n- What are $MS_{between}, MS_{within}, F_{ratio}$\n- What is the shape of the F-distribution? \n- What are *df* for ANOVA?\n- What conclusion follows from ANOVA? What follow-ups? ","x":-5472,"y":2428,"width":423,"height":194},
		{"id":"89e922f7ff66a06b","type":"text","text":"ANOVA uses *variance* rather than *mean* as statistic for eval. $H_0$","x":-5440,"y":2645,"width":359,"height":61,"color":"6"},
		{"id":"9cafd3a5fed37295","type":"text","text":"- In this section or covered recently:\n    - Given measures of sample central tendency and variability (e.g., mean and standard deviation), conduct a single-sample _t_-test\n    - Given raw data or means/standard deviations, conduct a paired _t_-test\n    - Given sample means and standard deviation (or variance) for two groups, conduct an independent _t_-test\n    - Given raw data, calculate standard error of the mean\n    - Given sample means and standard deviations, calculate the appropriate Cohen's _d_\n    - Given a table of output from SPSS/Jamovi, interpret values from a _t_-test\n- Material not yet covered:  \n    - Given sample means and standard deviations, alpha, confidence level; construct a confidence interval\n    - Given all group _SS, SS_total, and group _n_'s, calculate the _F_-statistic (this would be similar to starting at \"Step 4\" from the example on pages 411-414 from the textbook)\n    - From _df_s and research design, determine _F_crit\n- Old but could make a return:\n    - Given standardized data (_z_-scores) for an _X_ and _Y_ variable, calculate Pearson's _r_\n    - Given sample means and standard deviations, conduct a Normal deviates test (z-test)\n\n- RECENT CALCULATIONS THAT ARE **NOT TESTABLE** ON CALCULATION COMPONENT OF EXAM:\n\t- Power _calculations_ (but know the conceptual relationships between design choices and power, along with alpha and beta)\n\t- Testing significance of Pearson's _r_ (know what statistical distribution we test _r_ against; this is the _t_-distribution)\n\t- Full on ANOVA (like practice problem 15.1 or steps 1-3 on pages 411-414 from the textbook)\n\t- Planned comparisons for ANOVA","x":-7500,"y":-2060,"width":540,"height":180},
		{"id":"34572e480d797e9b","type":"file","file":"W2023T2/PSYC 218 L31 Final Review.md","x":-7520,"y":-1464,"width":1060,"height":828,"color":"#7030a0"},
		{"id":"f7f3d4380fdbc643","type":"text","text":"- Chapters 1-7: 2 questions specific to these chapters\n- Chapters 8-9: 2 question related to these chapters\n- Chapter 10: 1 question related to this chapter\n- Chapter 11: No questions related to this chapter\n- Chapter 12: 4 questions related to this chapter\n- Chapter 13: 4 questions related to this chapter\n- Chapter 14: 4 questions related to this chapter\n- Chapter 15: 4 questions related to this chapter","x":-7500,"y":-1855,"width":540,"height":276},
		{"id":"8d79de16611dd640","type":"text","text":"#### Testable\nConduct t-test (single, paired, independent)\nCalculate SE of mean\nCalculate Cohen's d\nInterpret t-test values\n\nConstruct CI\nCalculate F statistic (411-414 TB) 4. onward\nDetermine F crit\n\nCalculate Pearson's *r*\nConduct z-test","x":-6920,"y":-2060,"width":440,"height":360},
		{"id":"ef948fec258b97bf","type":"text","text":"##### *Not testable*\nCalculating power\nPearson's *r* significance testing\nANOVA (steps 1-3 411-414)\nPlanned comparisons for ANOVA","x":-6920,"y":-1672,"width":440,"height":181},
		{"id":"d0f32f42a9440f25","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-21 at 4.55.29 PM.png","x":-7575,"y":138,"width":400,"height":84,"color":"6"},
		{"id":"b2f8b9fd6249fe77","type":"text","text":"**Setting Power**\nExp. 1: “We sought to collect 80 participants… Sensitivity analysis indicated with power set at .80, we could detect an effect size as small as $d_z$ = .317”\n- Sensitivity power b/c we don’t know the size of the effect \n- α = .05 \n- (1- β) = .80 \n- N = 80\n\nExp. 2: “Based on the observed effect size of $d_z$ = .430 in Experiment 1, we sampled from 48 participants to set power at .80” \n- Use a priori power after getting an estimate of effect size in Exp. 1 \n- α = .05 \n- (1- β) = .80 \n- Effect size = $d_z$ = .430","x":-7297,"y":4063,"width":398,"height":580,"color":"3"},
		{"id":"83481a863fa47b55","type":"text","text":"### z-testing\n1. Generate hypotheses: directional/non-directional\n2. Set alpha level\n3. Calculate *standard error of the mean*\n4. Calculate $z_{obt}$\n5. Compare $z_{obt}$ to $z_{crit}$\n6. Reject $H_0$ or fail to reject $H_0$\n","x":-7392,"y":1338,"width":484,"height":268,"color":"#3d24ff"},
		{"id":"2d02bc6c634f03d6","type":"text","text":"As *p* gets smaller, z(& t, F, $X^2$) gets larger","x":-7392,"y":1197,"width":380,"height":50,"color":"#3d24ff"},
		{"id":"0d3a3a1b9db629f6","type":"text","text":"Will they get an A?**\n**N = 50** \n- Textbook table only goes up to 20! **<span style=\"color:#0070c0\">Normal distribution</span>**\n\nNeed to get 40 right\n**P = .70** \n- Chance of getting question right >.20; student studied\nWhat z-score corresponds to 80% score? \n$z = \\frac{X - \\mu}{\\sigma}$\n- $\\mu = NP$, $\\sigma = \\sqrt{NPQ}$\nX = 40\n$\\mu$ = expected score = 35\n$\\sigma$ = possible variability in score\nz = 1.54 (6.8% chance)\n\n**Conceptual steps for Normal Approximation**\n1. Want to know p(≥80%)\n2. Binomial approximates z-dist\n3. z-score that corresponds to 80%\n4. What is area under curve for this z-score or higher?","x":-7880,"y":394,"width":380,"height":737,"color":"#cb6c9a"},
		{"id":"c4f42d9da92fed83","type":"text","text":"##### Normal approximation","x":-7880,"y":312,"width":380,"height":59,"color":"#cb6c9a"},
		{"id":"2cb10b01a5ef7c99","type":"text","text":"**As N increases, binomial distribution approximates normal, z-distribution**","x":-8280,"y":165,"width":380,"height":86,"color":"6"},
		{"id":"ba64e371fe3f280d","type":"text","text":"**Did they cheat?**\nHypothesis 1 ($H_1$): \n- Assuming they have not studied, student performs no better than they did on exam 1 (20%)\nHypothesis 2 ($H_2$): \n- Student cheated\n\nOr: is our assumption that they didn't study wrong? \n\nN = 20 questions\n$P_{expectation} = .20$\n$P_{observed} = .90$\n- Student got 18 correct, but 19 and 20 correct would be even stronger evidence against $H_1$\nCompute p(18) and add to p(19) & p(20): <span style=\"color:#0070c0\">tail of distribution</span>\np(18) + p(19) + p(20)","x":-8280,"y":312,"width":380,"height":583,"color":"6"},
		{"id":"d938fea4131a54ba","type":"text","text":"Power has been defined as the probability of rejecting the null hypothesis if the independent variable has a real effect","x":-6688,"y":3960,"width":535,"height":70,"color":"3"},
		{"id":"f97e0604421dc570","type":"text","text":"Pnull is the probability of getting a change with any subject in the sample of the experiment when the independent variable has no effect.\n- Pnull always = .50\n\nPreal is the probability of getting a change with any subject in the sample of the experiment when the independent variable has a real effect.\n- The further Preal is from .50 the greater the effect size","x":-6832,"y":3674,"width":679,"height":245,"color":"3"},
		{"id":"b1840111b7167afd","type":"text","text":"**3 types of power**\n1. a priori – Before data, find N given: \n\t1. α, (1 - β), expected effect size \n2. post hoc – After data, find power given: \n\t1. α, N, observed effect size \n3. Sensitivity – Before/after data, find detectable effect size given: \n\t1. α, (1 - β), N","x":-7738,"y":4103,"width":398,"height":288,"color":"3"},
		{"id":"70b3e6586cfcbff3","type":"text","text":"Binomial expansion","x":-7500,"y":51,"width":250,"height":60,"color":"6"},
		{"id":"f2772b24ddab47e4","type":"text","text":"## $z = \\frac{X - \\mu}{\\sigma}$$\\mu = NP$$\\sigma = \\sqrt{NPQ}$","x":-7480,"y":618,"width":250,"height":133,"color":"#fe95cb"},
		{"id":"fdb4e7cc8da5fc38","type":"text","text":"**When do we reject null?**\n- Reject when $z_{obt}$ gets large enough \n\t- Specifically when ±1.96 (for 2-tail tests): $z_{crit}$ is always ±1.96 or ±1.645 (for 1-tail) \n\t- Critical values change for other test statistics (e.g., t, F) \n","x":-8750,"y":3639,"width":560,"height":187,"color":"2"},
		{"id":"a76f5c11af128571","type":"text","text":"##### Increasing $z_{obt}$ & Increasing Power\n- Larger N increases $z_{obt}$ \n\t- (because it reduces $σ_{\\bar{X}}$) \n- Smaller $\\sigma$pop  increases $z_{obt}$ \n\t- (b/c it reduces $σ_{\\bar{X}}$) \n- Larger difference ($\\bar{X}$– μ) \n- Larger 𝛂, b/c it reduces $z_{crit}$ \n\t- ($z_{obt}$ does not need to be as extreme to reject $H_0$) \n- Directional $H_0$/$H_1$, b/c it reduces $z_{crit}$  \n\t- ($z_{obt}$ does not need to be as extreme to reject $H_0$)","x":-8750,"y":3264,"width":560,"height":349,"color":"2"},
		{"id":"f9802ea92f11147b","type":"text","text":"##### Standard error of the mean:\n### $\\sigma_\\bar{X} = \\frac{\\sigma}{\\sqrt N}$\n","x":-9145,"y":3437,"width":358,"height":144,"color":"4"},
		{"id":"5ea74516bf429a06","type":"text","text":"##### Estimated standard error of the mean:\n### $s_\\bar{X} = \\frac{s}{\\sqrt N}$\n","x":-9145,"y":3606,"width":358,"height":169,"color":"4"},
		{"id":"852d3abbe7819313","type":"text","text":"bi\nserial\nconstant P \nmutually exclusive\nindependent\n","x":-9660,"y":3264,"width":265,"height":173,"color":"1"},
		{"id":"941195706afeac8e","type":"text","text":"#### Type I and Type II Errors","x":-7467,"y":3583,"width":298,"height":53,"color":"3"},
		{"id":"9b2ad4ae012cd7a9","type":"text","text":"### t-distribution\n\n### $t_{obt} = \\frac{\\bar{X}-\\mu}{s_{\\bar{X}}}$\n### $s_{\\bar{X}} = \\frac{s}{\\sqrt N}$\n\n**We assume s = $\\sigma$, but s is biased to underestimate $\\sigma$, so we correct using degrees of freedom**","x":-7392,"y":2208,"width":523,"height":314,"color":"#80ff00"},
		{"id":"bad50227bfbd1c14","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-21 at 4.06.10 PM.png","x":-7592,"y":2649,"width":400,"height":227,"color":"#80ff00"},
		{"id":"5254d54ee10572fc","type":"text","text":"###### z test: \n- $\\mu$ is known\n- $\\sigma$ is known\n\n###### **t test:**\n- $\\mu$ is set hypothetically\n- **$\\sigma$ is estimated using s**","x":-7760,"y":2268,"width":328,"height":287,"color":"#80ff00"},
		{"id":"9f93c402dcdc7afb","type":"text","text":"Example problem: normal deviates test\n$\\mu$ = 14.00\n$\\sigma_{\\mu}$  = 5\n\nN = 24\n$\\bar{X}$ = 16.20\n$\\alpha$ = .05\n\n##### $\\sigma_\\bar{X} = \\frac{5}{\\sqrt 24} = 1.0206$\n##### $z_{obt} = \\frac{\\bar{X} - \\mu}{\\sigma_\\bar{X}} = \\frac{16.20 - 14.00}{1.0206} = 2.1556$ \n##### $z_{crit} = ±1.96$\n###### Thus, we reject $H_0$\n\n\n\n","x":-7392,"y":1628,"width":484,"height":504,"color":"#3d24ff"},
		{"id":"11b63e78b87fe294","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-21 at 3.13.51 PM.png","x":-8935,"y":1741,"width":549,"height":282,"color":"1"},
		{"id":"537908984dfefc60","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-03-21 at 3.13.51 PM.png","x":-9809,"y":1591,"width":549,"height":282,"color":"1"},
		{"id":"0e5297ffb36e608c","type":"text","text":"The sampling distribution of a statistic gives: \n1. all the values that the statistic can take  \n2. the probability of getting each value under the assumption that it resulted from chance alone.","x":-8913,"y":1558,"width":504,"height":153,"color":"4"},
		{"id":"59f8bbfafa61445b","type":"text","text":"Mean of the distribution of sample means = population mean \n### $\\mu_{\\bar{X}} = \\mu$","x":-9145,"y":3263,"width":358,"height":153,"color":"4"},
		{"id":"a141fde1a3d775bb","type":"text","text":"#### $\\hat{d} = \\frac{\\bar{X}_{obt}-\\mu}{s}$","x":-9370,"y":3264,"width":181,"height":75,"color":"3"},
		{"id":"b2d0f67df7bd1c1c","type":"text","text":"**3 types of power**\n1. a priori – Before data, find N given: \n\t1. α, (1 - β), expected effect size \n2. post hoc – After data, find power given: \n\t1. α, N, observed effect size \n3. Sensitivity – Before/after data, find detectable effect size given: \n\t1. α, (1 - β), N","x":-9660,"y":3487,"width":398,"height":288,"color":"3"},
		{"id":"6c9629a613e34ffa","type":"text","text":"#### Effect Size","x":-8659,"y":2438,"width":250,"height":60,"color":"3"},
		{"id":"887a46e08d9185db","type":"text","text":"#### Central Limit Theorem","x":-8650,"y":2227,"width":313,"height":57,"color":"4"},
		{"id":"b0836b060ba4e64f","type":"text","text":"N (Sample size) tells us how \"normal\" the distribution will look under $H_0$ and how close $\\bar{X}$ will be (on avg) to $\\mu$","x":-8320,"y":2303,"width":426,"height":94,"color":"4"}
	],
	"edges":[
		{"id":"4fd83e66eabcf6bb","fromNode":"9686d5f4c66fde75","fromSide":"bottom","toNode":"169480747e632bc2","toSide":"top","label":"2 correlated sample means"},
		{"id":"282450772547eff4","fromNode":"15cb10d152100cf1","fromSide":"right","toNode":"5b2f0f2ddfaec66d","toSide":"left"},
		{"id":"b050595ccdb3c01b","fromNode":"5b2f0f2ddfaec66d","fromSide":"bottom","toNode":"a7ab7e2f2b4ebe98","toSide":"top"},
		{"id":"53de8ba20de3e82d","fromNode":"9686d5f4c66fde75","fromSide":"bottom","toNode":"cce143c46bd5a5f0","toSide":"top","label":"2 independent sample means"},
		{"id":"80c7f85c284d3f19","fromNode":"6cbc76b67808850a","fromSide":"bottom","toNode":"9e5491bff6795c9f","toSide":"top","label":"Known SDpop"},
		{"id":"e784b9c02f8759dd","fromNode":"6cbc76b67808850a","fromSide":"bottom","toNode":"c25c1e3bbf8cb08f","toSide":"top","label":"Unknown SDpop"},
		{"id":"341e64c2d8dea768","fromNode":"2be899485d80c236","fromSide":"left","toNode":"6cbc76b67808850a","toSide":"top"},
		{"id":"9808cbd5d3cbf502","fromNode":"2be899485d80c236","fromSide":"right","toNode":"9686d5f4c66fde75","toSide":"top"},
		{"id":"ec445b61281ffc0c","fromNode":"5f3c4554d3a2cf28","fromSide":"bottom","toNode":"bf61e01a1b3bfb2b","toSide":"top"},
		{"id":"8a3bff266455fec7","fromNode":"bf61e01a1b3bfb2b","fromSide":"right","toNode":"dccae38f422f0d1b","toSide":"left"},
		{"id":"d7868c469773c760","fromNode":"70d31a97c3890bfc","fromSide":"left","toNode":"5f3c4554d3a2cf28","toSide":"right"},
		{"id":"5e82aa53ca2f57ba","fromNode":"e52ebe2372ed527d","fromSide":"top","toNode":"4c29015b8d332b11","toSide":"bottom"},
		{"id":"034d1edc2f39c94d","fromNode":"4c29015b8d332b11","fromSide":"right","toNode":"86a5061a3a3c5513","toSide":"left"},
		{"id":"54de1923e6bc8eb0","fromNode":"c8318dd89297859d","fromSide":"right","toNode":"6d62878895d79aca","toSide":"left"},
		{"id":"fcdf28e7f43b4ee6","fromNode":"6d62878895d79aca","fromSide":"right","toNode":"2e20bf38a193568b","toSide":"left"},
		{"id":"f785f55c460a16e4","fromNode":"6d62878895d79aca","fromSide":"right","toNode":"4b0da278f8291b9e","toSide":"left"},
		{"id":"758466a17b6671af","fromNode":"c8318dd89297859d","fromSide":"bottom","toNode":"54a4bdfcd3189c33","toSide":"top"},
		{"id":"bfaecc4572b58c4f","fromNode":"6d62878895d79aca","fromSide":"bottom","toNode":"d47995cc5806446e","toSide":"top"},
		{"id":"329028470d61e217","fromNode":"8917f3519ae28b3e","fromSide":"right","toNode":"e3df2e6351692191","toSide":"left"},
		{"id":"deac659c803e83e9","fromNode":"493afaa5225385c3","fromSide":"right","toNode":"e992a3eb13827ac1","toSide":"left","label":"Regression"},
		{"id":"f2d8232b4502af3f","fromNode":"1b771818fec26b8b","fromSide":"right","toNode":"6db61b3e97521a9b","toSide":"left","label":"Random Sampling"},
		{"id":"a62959208ae980e1","fromNode":"1ec7d5c8928ff3ed","fromSide":"bottom","toNode":"aefe48b61a4435d8","toSide":"top"},
		{"id":"8a52bacca09d9b7c","fromNode":"1ec7d5c8928ff3ed","fromSide":"bottom","toNode":"d0f69278a5f8efb4","toSide":"left"},
		{"id":"0adcbbdd687a5569","fromNode":"60d788c739fc5eda","fromSide":"right","toNode":"a458e543c10c76a4","toSide":"left","label":"Frequency Dist. & Central Tendency"},
		{"id":"f10fa89464b288b0","fromNode":"7ac582cb6a910de3","fromSide":"right","toNode":"95722743966f4b46","toSide":"left","label":"Standard Scores & Correlation"},
		{"id":"7687348460143a7a","fromNode":"1b23f97c812bf7a8","fromSide":"right","toNode":"9f93c402dcdc7afb","toSide":"left"},
		{"id":"a9971f436c5bb28e","fromNode":"c4df37f11b226ce7","fromSide":"bottom","toNode":"fe20d0e6fc5c6e18","toSide":"top"},
		{"id":"ac0c927e690afb03","fromNode":"cee62814e491f32b","fromSide":"left","toNode":"c4df37f11b226ce7","toSide":"right"},
		{"id":"ab94af705e090106","fromNode":"cee62814e491f32b","fromSide":"bottom","toNode":"8b65da1914ac804b","toSide":"top"},
		{"id":"36f0d9d6a601054d","fromNode":"09da228077bb0501","fromSide":"right","toNode":"638dd08d8ba2706e","toSide":"left"},
		{"id":"53aae16aa502608f","fromNode":"c7f50c5435865584","fromSide":"bottom","toNode":"b1840111b7167afd","toSide":"top"},
		{"id":"21173889fb2c8689","fromNode":"c7f50c5435865584","fromSide":"bottom","toNode":"b2f8b9fd6249fe77","toSide":"top"},
		{"id":"fbda8c0259512e94","fromNode":"c4f42d9da92fed83","fromSide":"right","toNode":"f2772b24ddab47e4","toSide":"top"},
		{"id":"5fafe9042442a349","fromNode":"c7f50c5435865584","fromSide":"top","toNode":"2d84e204045678ca","toSide":"bottom"},
		{"id":"72882337f83b9128","fromNode":"e0c141bc4aade24e","fromSide":"left","toNode":"4b5c81f4ac3ae61b","toSide":"left"},
		{"id":"05531985e31f14d5","fromNode":"c3567ca2d755c425","fromSide":"right","toNode":"115c886cf839e34d","toSide":"left"},
		{"id":"d24f4ed601a2c702","fromNode":"c3567ca2d755c425","fromSide":"right","toNode":"f210c4105ad4a6d3","toSide":"left"}
	]
}