{
	"nodes":[
		{"id":"8e177e8ba5b72d51","type":"text","text":"# Lectures 11-12","x":480,"y":1920,"width":320,"height":60,"color":"6"},
		{"id":"c7f798cf14ecf516","type":"text","text":"# Chapter 7","x":-1300,"y":1920,"width":240,"height":60,"color":"6"},
		{"id":"c36cc4e8fb85722f","type":"text","text":"# Chapter 8","x":-1300,"y":4240,"width":250,"height":60,"color":"3"},
		{"id":"41690ce06832ba20","type":"text","text":"Regression = prediction\nLine of best fit = regression line\n> Balances the magnitude of positive and negative errors\n> Minimizes $\\sum (X-Y')^2$\n\n**Formula for linear regression line**\n$Y' = b_Y X + a_Y$\n$Y'$ = Criterion variable\n$X$ = predictor variable\n$b_Y$ = slope of regression line\n$a_Y$ = intercept\n\n**Calculating $b_Y$**\nWhen you know r, $s_Y$, and $s_X$:\n$b_Y = r\\frac{s_Y}{s_X}$\n\n**When you only have raw data:** (screenshot)","x":480,"y":1980,"width":711,"height":461,"color":"6"},
		{"id":"5c72faf23aa21487","type":"text","text":"### **Example data**","x":480,"y":2441,"width":711,"height":72,"color":"6"},
		{"id":"3a4691c13e0d198c","type":"text","text":"## Regression\n#### Least-squares regression line: \n- The prediction line that minimizes the total error of prediction, according to the least-squares criterion of \u0002$\\sum(Y – Y')^2$\n\n#### Standard error\n**Standard error of the estimate: difference between prediction and actual value**\n**We can assume the points are normally distributed about the regression line**(Figure 7.5). \nIf the assumption is valid and we were to construct two lines parallel to the regression line at distances of $±1~s_{Y|X}$, \u0006$±2~s_{Y|X}$, and $±3~s_{Y|X}$ → \n- 68% lie between $±1~s_{Y|X}$\n- 95% lie between $±2~s_{Y|X}$\n- 99% lie between $±3~s_{Y|X}$\n\n#### **Considerations for using Linear Regression:**\n- The relationship between X and Y m ust be linear\n- The basic computation group should be representative of the prediction group\n- Only used for the range of the variable on which it is based\n\n##### Pearson $r$ is t he slope of the least-squares regression line when the scores are plotted as $z$-scores\n\n## Multiple Regression\n### Calculating Criterion based on 2+ Predictors: $Y' = b_{1}X{1}+b_{2}X_{2}+a$\n### $R^2$, aka: \n Multiple coefficient of determination\n Proportion of Variance based on 2+ Predictors\n Squared multiple correlation\n","x":-1300,"y":1980,"width":1140,"height":1011,"color":"6"},
		{"id":"9ccd443d7aafae02","type":"text","text":"pp. 159-178","x":-1480,"y":1920,"width":180,"height":60,"color":"6"},
		{"id":"3506b167e2bef98b","type":"text","text":"pp.187-220","x":-1480,"y":4240,"width":180,"height":60,"color":"3"},
		{"id":"9f44d434b2a57420","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.00.14 PM.png","x":-162,"y":1980,"width":400,"height":120,"color":"6"},
		{"id":"60e2718b1cb7064a","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 2.59.53 PM.png","x":-162,"y":2324,"width":303,"height":126,"color":"6"},
		{"id":"4c490e771288f299","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.45.13 AM.png","x":141,"y":2100,"width":339,"height":293,"color":"6"},
		{"id":"6a33ff16b2498a14","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.46.44 AM.png","x":480,"y":2808,"width":711,"height":250,"color":"6"},
		{"id":"1725bacfa2bc5f52","type":"text","text":"#### $SS_X$ =  <span style=\"color:#c00000\">1.811</span>","x":480,"y":3059,"width":711,"height":50,"color":"6"},
		{"id":"7c7ac9eaf8044859","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.45.33 AM.png","x":480,"y":2513,"width":710,"height":295,"color":"6"},
		{"id":"187940cda483e1fe","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.14.53 PM.png","x":-1680,"y":2133,"width":380,"height":327,"color":"6"},
		{"id":"69e4bba8eccb06e8","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.20.55 PM.png","x":-160,"y":2493,"width":199,"height":107,"color":"6"},
		{"id":"23dedfbe5ba42996","type":"text","text":"# Lectures 4-6","x":490,"y":-1412,"width":250,"height":60,"color":"4"},
		{"id":"7e40670337342e1b","type":"text","text":"##### **Video 1: Summation**\n$\\sum^{N}_{i=1} X_{i} = X_{1} + X_{2} + ... X_{N}$ \n##### Video 2: Summed differences\n$\\sum(X_{i}-\\bar{X}) = (X_{1}-7.8) + (X_{2}-7.8) ... + (X_{N}-7.8)$\n$\\bar{X}$ = sample mean (eg. 7.8)\n$\\bar{X} = \\frac{\\sum X_{i}}{N}$\n\n\n\n\n\n\n\n\n","x":490,"y":-1352,"width":711,"height":351,"color":"4"},
		{"id":"80dacc4f69cd715d","type":"text","text":"# Chapters 3 & 4\n","x":-1290,"y":-1399,"width":277,"height":50,"color":"4"},
		{"id":"8cf0d91de07b2a3e","type":"text","text":"**Frequency distribution**\n- Relative: proportion of total number of scores that occurs in each interval \n- Cumulative (cum *f*): number of scores that fall below the upper real limit of each interval \n- Cumulative percentage: percentage of scores that fall below the upper real limit of each interval (cum *f*/N * 100)\n\n#### Measures of central tendency\n**Mean**\nProperties of the mean\n- sensitive to exact value of all scores in dist. \n- the sum of the deviations about the mean = 0 (the deviations above the mean are equal to the deviations below)\n- sensitive to extreme scores\n- sum of squared deviations of all the scores about their mean is a *minimum* (the smallest possible sum of squared deviations about any value)\n- usually, the mean is the least subject to sampling variation compared ot other measures of central tendency\nOverall mean\n- If we know separate group means we can sum and divide by *n* to find the overall mean\n**Median**\n- Centermost score if n is odd, average between centermost scores if n is even\n- less sensitive than mean to extreme scores\n- more subject to sampling variability than mean but less than mode\n**Mode**\n- most frequent score in dist\n\n\n\n**Standard deviation**\n$\\bar{X}$ = mean of sample \nμ = mean of population\n\nDeviation score for sample data = $X - \\bar{X}$\nDeviation score for population data = $X~~–~~μ$\n\n","x":-1290,"y":-1349,"width":1140,"height":873,"color":"4"},
		{"id":"1b5752df92fc9eed","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-14 at 5.04.11 PM.png","x":-1490,"y":-1349,"width":200,"height":61,"color":"4"},
		{"id":"4facded319f3cb15","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-14 at 5.09.07 PM.png","x":-1690,"y":-1288,"width":400,"height":131,"color":"4"},
		{"id":"553de7e95a12c1b7","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-01-14 at 11.38.46 AM.png","x":-2089,"y":-1121,"width":399,"height":189,"color":"4"},
		{"id":"a45beece5853f544","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-01-14 at 11.40.38 AM.png","x":-1690,"y":-1160,"width":400,"height":266,"color":"4"},
		{"id":"db657f6149178cb8","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-01-14 at 11.45.00 AM.png","x":-2001,"y":-894,"width":711,"height":392,"color":"4"},
		{"id":"cb3df1175c693926","type":"text","text":"##### Video 3: Three models of variability\nRange \n- No symbol\n- max - min\nStandard Deviation \n- s, $\\sigma$, std, s.d.\nVariance\n\n**Distribution of observations**\n- Central tendency\n- Skew\n- Variability: how spread out scores are\n\n##### Video 4: Models of Variability and Sum of Squares\n**Summed Deviation**\n$\\sum (X_{i}-\\bar{X})$ = always 0\n\n**Sum of squares** (SS)\n$\\sum (X_{i}-\\bar{X})^2$\n","x":-1287,"y":-453,"width":711,"height":618,"color":"4"},
		{"id":"eac2406d1bf2dcd0","type":"text","text":"##### Video 5: Standard deviation and Variance\n**Variance**\nAverage of squared deviations (mean squared deviation)\n$\\frac{\\sum (X_{i}-\\bar{X})^2}{N}$\n\n**Standard deviation**\nSquare root of variance\nPopulation SD = $\\sqrt{\\frac{\\sum (X_{i}-\\mu)^2}{N}}$\nSample SD = $\\sqrt{\\frac{\\sum (X_{i}-\\bar{X})^2}{N-1}}$\n","x":-576,"y":-453,"width":711,"height":310,"color":"4"},
		{"id":"8a5f868b42341186","type":"text","text":"##### Video 7: Alternative Formulas for _SS, s,_ & _$s^2$_  \n**Sum of squares**\n$\\sum X^2 - \\frac{(\\sum X)^2}{N}$\n\n**Standard deviation**\n$\\sqrt \\frac{SS}{N-1}$\n\n**Variance**\n$\\frac{SS}{N-1}$\n\n##### Video 8: Calculating Sum of Squares, Standard Deviation, & Variance\n**Example: Sugar in cereal**\n\n##### Video 9: Conceptual Wrap-up for SS, s, & $s^2$\nSS: all squared deviation scores \nVariance: N-1 acts as correction because sample variance tends to underestimate variance\nStandard deviation is unsquared variance: average raw deviance from mean","x":135,"y":-453,"width":711,"height":589,"color":"4"},
		{"id":"82809a0553091118","type":"text","text":"# Lectures 7-10","x":481,"y":320,"width":250,"height":60,"color":"5"},
		{"id":"ed82e2cc7f74af28","type":"text","text":"##### 3b: The Normal Curve\n- Symmetric\n- Unimodal \n- Perfectly variable\n68.2% of scores will fall within 1 SD of $\\mu$\n13.6% of scores will be $\\mu-2\\sigma$, 13.6% $\\mu+2\\sigma$ (27.2% will be exactly 2SD away)\n2.3% of scores will be $\\mu-3\\sigma$, 2.3% $\\mu+3\\sigma$ (4.6% will be exactly 3SD away)\n\n##### 3c: z-scores (a.k.a., standard scores)\n**z score** \n$\\frac{X_{i}-\\mu}{\\sigma}$\n- how many $\\sigma$s an observation is from $\\mu$\nz-scores *center* data: \"standardizing\"\n\nCan only use normal curve to obtain percentile rank when data is virtually identitcal to normal distribution","x":481,"y":380,"width":711,"height":530,"color":"5"},
		{"id":"de3d080c9d859bea","type":"text","text":"# Chapters 5 & 6","x":-1299,"y":320,"width":300,"height":60,"color":"5"},
		{"id":"50d1fa0dec16d3ec","type":"text","text":"#### Scatterplots\n**Imperfect**: Data does not fall completely linearly (as is most typical)\n\nSpearman's $\\rho/r_s$ = when one or both of the variables are of ordinal scaling\nBiserial correlation coefficient $r_b$ = when one o f the variables is at least interval and the other is dichotomous\n$\\phi$ = when each of the variables is dichotomous\n\n**Effect of range on correlation**: restrict range = lower correlation\n","x":-1299,"y":380,"width":1140,"height":255,"color":"5"},
		{"id":"29fd61ce4e7f60f7","type":"text","text":"#### Equation for the normal curve","x":-99,"y":408,"width":400,"height":60,"color":"5"},
		{"id":"957203b40a997b56","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 1.42.56 PM.png","x":-99,"y":468,"width":400,"height":147,"color":"5"},
		{"id":"423409b61a68dd0b","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 2.08.26 PM.png","x":-99,"y":615,"width":511,"height":295,"color":"5"},
		{"id":"5cda9bcfd851ea3f","type":"text","text":"pp. 102-148","x":-1479,"y":320,"width":180,"height":60,"color":"5"},
		{"id":"5f94ec8a30a17eff","type":"text","text":"#### Spearman's $\\rho/r_s$","x":-1699,"y":465,"width":400,"height":50,"color":"5"},
		{"id":"61c772215078cf96","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 2.32.05 PM.png","x":-1700,"y":515,"width":401,"height":54,"color":"5"},
		{"id":"f8f5e13b933486f4","type":"text","text":"$D_i$ = $\u0002R(X_i) - R(Y_i)$\n$R(X_i)$ = rank of the ith X score \n$R(Y_i)$ = rank of the ith Y score \nN \u0002 number of pairs of ranks","x":-1699,"y":569,"width":400,"height":167,"color":"5"},
		{"id":"7a90ef01c7c873c5","type":"text","text":"### Calculating Pearson's $r$: \n- Check that relationship is linear (scatterplot)\n- Variables are normally distributed\n- Interval or ratio data\n- Absence of extreme outliers\n### Calculating $r$ rom z-scores:  $r = \\frac{\\sum z_{x}z_{y}}{N-1}$\n\nMost r values in psych research are 0.10-0.30\n\n#### Coefficient of determination: $r^2$\nr = relatedness, covariance\n$r^2$ = explained variance","x":-589,"y":910,"width":711,"height":527,"color":"5"},
		{"id":"2a8760f0e8f78399","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.35.26 AM.png","x":123,"y":912,"width":708,"height":176,"color":"5"},
		{"id":"d954397a2d4ec944","type":"text","text":"### Pearson's $r$ vs Spearman's $r_s / \\rho$ \n#### Pearson's r assumes\n- Equal intervals\n- Normal distribution of X & Y\n- Absence of multivariate outliers\n- Linear relationship X~Y\n- *If assumptions violated, model may be biased*\n\n#### Spearman $r_s$ assumes: \n- *Ranked data*\n- Normal distribution unnecessary\n- Multivariate outliers are OKish\n- Monotonic relationship X~Y, but not necessarily linear\n**Advantages:**\n- RObust to non-normal data (skewed) outliers\n- Model non-linear\n**Disadvantages**: \n- If X & Y have equal intervals, ranking loses information\n- If X & Y are normally distributed, Spearman $r_s$ is less powerful (may fail to find true relationship)","x":831,"y":912,"width":711,"height":697,"color":"5"},
		{"id":"fd5128467c950dfb","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.35.00 AM.png","x":123,"y":1088,"width":711,"height":273,"color":"5"},
		{"id":"fca7b70d9396006c","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-02 at 10.36.13 AM.png","x":123,"y":1362,"width":710,"height":361,"color":"5"},
		{"id":"ca8fd287771934fa","type":"text","text":"### Calculating Pearson's $r$ from example data\n\n**Determining if two variables are related: *r***\n**Determining how much of Y variance is explained by X variance: $r^2$**\n\n\"multivariate outlier\"\n","x":-589,"y":1435,"width":711,"height":174,"color":"5"},
		{"id":"7b4d44fdc2a50628","type":"text","text":"### SS (sum of squares) = $\\sum{X^2 - \\frac{(\\sum{X})^2}{N}}$\nformulas will be on MTs; recognize but don't memorize\n### population SD = $\\sigma_{pop}$ $\\sqrt \\frac{\\sum(X_{i}- μ)^2}{N}$ \nTaking the square root gives us mean deviation = **standard deviation**\n### s (sample SD) = $\\sigma_{est}$ = $\\sqrt{\\frac{\\sum(X_{i}-\\bar X)^2}{N-1}}$\n### s (standard deviation, alt.) = $\\sqrt{\\frac{SS}{N-1}}$\n#### z score = $\\frac{X_{i} - \\mu}{\\sigma}$\n> how many $\\sigma$ an observation is from $\\mu$","x":-1300,"y":910,"width":711,"height":548,"color":"5"},
		{"id":"8a40d2752237beaf","type":"text","text":"### Multiple regression\n**Symbols**\nr = normal distribution\n$r_s$ non-normal distribution, outliers = spearman's\n$r_pb$ = one continuous (interval, ratio) variable, one dichotomous (binary) variable \n$\\phi$ = two dichotomous variables (chi square!)\n\n$\\beta$ = standardized slope coefficient = $z_x$, $z_y$\n$b$ = unstandardized slope coefficient = $X_i, Y_i$\n$r^2$ = variability explained by predictor variable\n$R^2$ = variability explained by regression model  ","x":-591,"y":3109,"width":711,"height":345,"color":"6"},
		{"id":"02117505736f5db9","type":"text","text":"### Categorical Predictors in Regression\nArbitrary numbering can mess with regression \n**Multiple regression**\nCriterion Y: grumpiness\nPredictor $X_1$: Hours of sleep \nPredictor $X_2$: $Rain_{L1}$ or $Shine_{L2}$\n\tThe numerical values are meaningless, it's essentially a binary system which refers to non-numerical meaning aka 1 = rain, 2 = shine\n\n$b_1$ = -8.715\n$b_2$ = -1.455\na = 124.442\n\nIf $X_2$ is coded as '2' aka shine → 1.455 fewer grumpy units \n\n### Dummy Variables\nIf predictor $X_2$ has **3 potential values**: Rain, Clouds, or Shine, *we can no longer assign a slope coefficient ($b_2$)* → \"**Dummy variable**\"\n- Collapse variables Clouds + Shine together and take average, where Clouds or Shine = 0 and Rain = 1\n- (Or) Compare Clouds vs Rain for one slope and Clouds vs Shine for another\n\nCoding of categorical predictors changes interpretation of *b*\nDummy coding is most common + simplest, but many other methods exist depending on hypotheses\n","x":120,"y":3109,"width":711,"height":741,"color":"6"},
		{"id":"ba6af968236ec7b2","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-10 at 7.52.46 PM.png","x":-591,"y":3454,"width":710,"height":483,"color":"6"},
		{"id":"1ca31a4ff5aa029d","type":"text","text":"### Standard Error","x":-1301,"y":3109,"width":710,"height":60,"color":"6"},
		{"id":"0d6e39bb9d4192c4","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-10 at 7.21.42 PM.png","x":-1301,"y":3169,"width":710,"height":239,"color":"6"},
		{"id":"68789a6e83b3f609","type":"text","text":"### Homoscedasticity","x":-1301,"y":3408,"width":711,"height":60,"color":"6"},
		{"id":"161c16e60c9b783e","type":"text","text":"How far off, on average, is our regression line from the actual data? \n68% of scores will be within 1 standard error \n\nHomoscedasticity = consistent error\n**Standard errors are only meaningful if the variability in Y is constant over values of X**\n\n**a) is homoscedastic, b) is not**\n","x":-1301,"y":3468,"width":711,"height":233,"color":"6"},
		{"id":"3f78a544c6e1cee0","type":"file","file":"W2023T2/W2023T2 Files/Screenshots/Screenshot 2024-02-11 at 3.10.29 PM.png","x":-1301,"y":3701,"width":710,"height":370,"color":"6"},
		{"id":"561c1f1d720bb230","type":"text","text":"#### Meehl's 6th Law of Psychology\n**Stastical \"crud\":** everything correlates with everything else, but most aren't meaningful = adding more predictors always increases $R^2$ = we need to remove as many *irrelevant* predictors as possible\n\nWhen reporting $R^2$, **always** also report adjusted $R^2$","x":836,"y":3109,"width":711,"height":201,"color":"6"},
		{"id":"62a06f1055250d3d","type":"file","file":"W2023T2/W2023T2 Files/Slides/annotated-PSYC218-L14.pdf","x":-1300,"y":4360,"width":391,"height":340,"color":"3"},
		{"id":"81b730a1a82b205e","type":"text","text":"# Lectures 13-15","x":-760,"y":4240,"width":320,"height":60,"color":"3"},
		{"id":"711c9bed8a692a29","type":"text","text":"##### Why sample? \n- Save time and expense\n- Ensure accuracy of measurement\n- Ensure quality/durability \nSample must be **representative** for valid generalizations to be made\n\n#### Sampling procedures\n1. **Self-selection**: volunteer participants\n\t1. Efficient / Often unrepresentative\n2. **Quota sampling**: technique sets groups and their proportions to sample from \n\t1. Less efficient, limited by ignorance of the importance/quantity of each group, biased towards prototypical example of group\n3. **Random sampling**: every person has equal chance of participating\n\t1. Nearly impossible, very inefficient, but fully representative\n\n**Replacement**: can the same case be selected twice? \n- Sampling with vs without replacement: with, can sample case more than once\n\n### Probability\n**Dependent successive events**\nProbability of being dealt 2 kings (\"Cowboys\") = 4/52 * 3/51 = 12/2652 \n- 3/51 bc no replacement\nProbability of being dealt a matching pair = 12/2652(13) = .0588\n\nOr, 1 (probability of being dealt any card) x 3/51 (probability of being dealt a matching card) = .0588\n\nProbability of being dealt 2 hearts = .0588\n.235\n\n\n","x":-760,"y":4300,"width":711,"height":914,"color":"3"},
		{"id":"92b9a4c4d5b27e0b","type":"file","file":"W2023T2/W2023T2 Files/Textbooks/Formulae.pdf","x":1320,"y":-1349,"width":840,"height":755},
		{"id":"d0601600180237c4","type":"file","file":"W2023T2/W2023T2 Files/Textbooks/ztable.pdf","x":1320,"y":-515,"width":840,"height":596}
	],
	"edges":[
		{"id":"a40c74fc3be95758","fromNode":"de3d080c9d859bea","fromSide":"right","toNode":"82809a0553091118","toSide":"left","label":"Standard Scores & Correlation"},
		{"id":"e848ed08892065d3","fromNode":"80dacc4f69cd715d","fromSide":"right","toNode":"23dedfbe5ba42996","toSide":"left","label":"Frequency Dist. & Central Tendency"},
		{"id":"f9e48634211d6cd8","fromNode":"c7f798cf14ecf516","fromSide":"right","toNode":"8e177e8ba5b72d51","toSide":"left","label":"Regression"},
		{"id":"4348b4adf38456fc","fromNode":"c36cc4e8fb85722f","fromSide":"right","toNode":"81b730a1a82b205e","toSide":"left","label":"Random Sampling"},
		{"id":"a723f22e5408d7bb","fromNode":"9f44d434b2a57420","fromSide":"bottom","toNode":"60e2718b1cb7064a","toSide":"top"},
		{"id":"c2759cd70e50efae","fromNode":"9f44d434b2a57420","fromSide":"bottom","toNode":"4c490e771288f299","toSide":"left"}
	]
}