{"path":"W2023T2/W2023T2 Files/Slides/annotated-COGS300-L9-connectionism.pdf","text":"COGS300 Introducing connectionism 1 Instructor: Márton Sóskuthy marton.soskuthy@ubc.ca TAs: Daichi Furukawa · Victoria Lim · Amy Wang cogs.300@ubc.ca What is the role of neural network models in understanding and modelling the expression of feelings and emotions in cognitive processes? 2 Back to the XOR problem Σ ? ? θ = 1 0 0 0 0 1 1 1 0 1 1 1 0 XOR p ⊕ q = (p ∨ q) ∧ ¬(p ∧ q) 3 Σ θ = 1 0 0 0 0 1 1 1 0 1 1 1 0 XOR p ⊕ q = (p ∨ q) ∧ ¬(p ∧ q) Σ θ = –1 Σ 1.1 –0.6 0.6 0.6 1.1 –0.6 θ = 1 Back to the XOR problem 4 Σ θ = 1 0 0 0 0 1 1 1 0 1 1 1 1 OR p ⊕ q = (p ∨ q) ∧ ¬(p ∧ q) Σ θ = –1 Σ 1.1 1.1 –0.6 –0.6 0.6 0.6 θ = 1 Back to the XOR problem 5 Σ θ = 1 0 0 1 0 1 1 1 0 1 1 1 0 NOT AND p ⊕ q = (p ∨ q) ∧ ¬(p ∧ q) Σ θ = –1 Σ 1.1 –0.6 0.6 0.6 1.1 –0.6 θ = 1 Back to the XOR problem 6 Σ θ = 1 0 0 0 0 1 0 1 0 0 1 1 1 AND p ⊕ q = (p ∨ q) ∧ ¬(p ∧ q) Σ θ = –1 Σ θ = 1 1.1 –0.6 0.6 0.6 1.1 –0.6 Back to the XOR problem 7 Σ θ = 1 0 0 0 0 1 1 1 0 1 1 1 0 XOR p ⊕ q = (p ∨ q) ∧ ¬(p ∧ q) Σ θ = –1 Σ 1.1 –0.6 0.6 0.6 1.1 –0.6 θ = 1 Back to the XOR problem 8 Σ θ = 1 0 0 0 0 1 1 1 0 1 1 1 0 XOR p ⊕ q = (p ∨ q) ∧ ¬(p ∧ q) Σ θ = –1 Σ 1.1 –0.6 0.6 0.6 1.1 –0.6 θ = 1 hidden layer Back to the XOR problem 9 But how do you train such a network? A more complex learning algorithm is needed – the perceptron learning rule just doesn’t cut it! The actual algorithm is pretty complex, so we’ll only look at the main intuition behind it. 10 But how do you train such a network? Changing the activation function −10 −5 0 5 100.00.20.40.60.81.0 linear scaleprobabilities summed inputoutput activation 11 But how do you train such a network? −10 −5 0 5 100.00.20.40.60.81.0 linear scaleprobabilities summed inputoutput activation Changing the activation function 12 But how do you train such a network? The loss function – the error to be minimised during learning 0.98 instead of 0 13 But how do you train such a network? 0.98 instead of 0 E = target – actual? The loss function – the error to be minimised during learning 14 But how do you train such a network? 0.98 instead of 0 E = (target – actual)2 The loss function – the error to be minimised during learning 15 But how do you train such a network? 0.98 instead of 0 E = (target – actual)2 w1 w1 E 16 But how do you train such a network? w1 E 17 But how do you train such a network? w1 E the gradient 18 But how do you train such a network? w1 E the gradient 19 But how do you train such a network? w1 E gradient = 0 at minimum 20 But how do you train such a network? w1 E gradient descent 21 But how do you train such a network? E = (target – actual)2 w1 & w2 w1 w2 E 22 But how do you train such a network? 23 But how do you train such a network? Back propagation is an algorithm that implements gradient descent for neural networks with hidden layers. 24 https://www.youtube.com/watch?v=Ilg3gGewQ5U Let’s have a play around with the following website: https://playground.tensorﬂow.org 25 Examples of connectionist networks NETtalk 26 Examples of connectionist networks NETtalk • distributed, superpositional & subsymbolic representations ➡ graceful degradation ➡ generalisation 27 Examples of connectionist networks Recurrent networks 28","libVersion":"0.3.2","langs":""}