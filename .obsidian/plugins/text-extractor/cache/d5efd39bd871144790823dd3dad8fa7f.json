{"path":"W2023T1/W2023T1 Files/23W1 Lecture 9 Nov 3.pdf","text":"Instructions for Presentations Quick Recap from Last Week Statistics and Replication COGS 303 Gary Neels UBC Nov 3, 2023 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication 1 Instructions for Presentations 2 Quick Recap from Last Week 3 Statistics and Replication Estimating parameters Signiﬁcance testing Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Presentations Presentations begin in two weeks (Nov 17, Nov 24, and Dec 1): If you haven’t signed up for a presentation date yet, you should do so soon 8-10 minutes, with 2-4 minutes for Q & A The standard is “work in progress” Component Criteria Points -Clear Content -Well-organized 7 -Informative -Correct length (8-10 minutes) Speech -Polished 4 -Flows well -Relevant Slides -Interesting 4 -Aids the presentation Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Presentations Presentations begin in two weeks (Nov 17, Nov 24, and Dec 1): If you haven’t signed up for a presentation date yet, you should do so soon 8-10 minutes, with 2-4 minutes for Q & A The standard is “work in progress” Component Criteria Points -Clear Content -Well-organized 7 -Informative -Correct length (8-10 minutes) Speech -Polished 4 -Flows well -Relevant Slides -Interesting 4 -Aids the presentation Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Presentations Presentations begin in two weeks (Nov 17, Nov 24, and Dec 1): If you haven’t signed up for a presentation date yet, you should do so soon 8-10 minutes, with 2-4 minutes for Q & A The standard is “work in progress” Component Criteria Points -Clear Content -Well-organized 7 -Informative -Correct length (8-10 minutes) Speech -Polished 4 -Flows well -Relevant Slides -Interesting 4 -Aids the presentation Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Presentations Presentations begin in two weeks (Nov 17, Nov 24, and Dec 1): If you haven’t signed up for a presentation date yet, you should do so soon 8-10 minutes, with 2-4 minutes for Q & A The standard is “work in progress” Component Criteria Points -Clear Content -Well-organized 7 -Informative -Correct length (8-10 minutes) Speech -Polished 4 -Flows well -Relevant Slides -Interesting 4 -Aids the presentation Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Quick recap from last week We talked about the “replication crisis” How are we doing with respect to replicating scientiﬁc results? Not great! Does this mean the unreplicated results are false? Not necessarily, but it should undermine our conﬁdence So, what needs to change in order to improve the situation? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Quick recap from last week We talked about the “replication crisis” How are we doing with respect to replicating scientiﬁc results? Not great! Does this mean the unreplicated results are false? Not necessarily, but it should undermine our conﬁdence So, what needs to change in order to improve the situation? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Quick recap from last week We talked about the “replication crisis” How are we doing with respect to replicating scientiﬁc results? Not great! Does this mean the unreplicated results are false? Not necessarily, but it should undermine our conﬁdence So, what needs to change in order to improve the situation? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Quick recap from last week We talked about the “replication crisis” How are we doing with respect to replicating scientiﬁc results? Not great! Does this mean the unreplicated results are false? Not necessarily, but it should undermine our conﬁdence So, what needs to change in order to improve the situation? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Quick recap from last week We talked about the “replication crisis” How are we doing with respect to replicating scientiﬁc results? Not great! Does this mean the unreplicated results are false? Not necessarily, but it should undermine our conﬁdence So, what needs to change in order to improve the situation? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Sources of the problem We identiﬁed two broad categories of sources of this problem: Systemic problems Publication bias Over-reliance on statistical signiﬁcance tests The incentive structure to publish innovative results Individual problems (Questionable Research Practices) P-hacking HARKing Outright fraud Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Systemic Problems Publication bias Journals refusing to publish replication studies What’s the incentive to perform a replication study if you can’t get it published? Bias toward innovation rather than rigorously testing current ideas This suggests overconﬁdence in our current theories Innovation is exciting, but empirical testing is how we tell if our theories are likely Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Systemic Problems Publication bias Journals refusing to publish replication studies What’s the incentive to perform a replication study if you can’t get it published? Bias toward innovation rather than rigorously testing current ideas This suggests overconﬁdence in our current theories Innovation is exciting, but empirical testing is how we tell if our theories are likely Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Systemic Problems Publication bias Journals refusing to publish replication studies What’s the incentive to perform a replication study if you can’t get it published? Bias toward innovation rather than rigorously testing current ideas This suggests overconﬁdence in our current theories Innovation is exciting, but empirical testing is how we tell if our theories are likely Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Systemic Problems Publication bias Journals refusing to publish replication studies What’s the incentive to perform a replication study if you can’t get it published? Bias toward innovation rather than rigorously testing current ideas This suggests overconﬁdence in our current theories Innovation is exciting, but empirical testing is how we tell if our theories are likely Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Systemic Problems Publication bias Journals refusing to publish replication studies What’s the incentive to perform a replication study if you can’t get it published? Bias toward innovation rather than rigorously testing current ideas This suggests overconﬁdence in our current theories Innovation is exciting, but empirical testing is how we tell if our theories are likely Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Systemic Problems Publication bias Journals refusing to publish replication studies What’s the incentive to perform a replication study if you can’t get it published? Bias toward innovation rather than rigorously testing current ideas This suggests overconﬁdence in our current theories Innovation is exciting, but empirical testing is how we tell if our theories are likely Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Individual Problems HARKing–Hypothesizing After Results are Known Kerr told us HARKing was bad because prediction is more valuable than accommodation Is he right? Isn’t all hypothesizing done after (at least some) results are known? The issue with HARKing is when the HARKed hypothesis is presented as if it has been tested and conﬁrmed rather than merely suggested by the result of the test Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Individual Problems HARKing–Hypothesizing After Results are Known Kerr told us HARKing was bad because prediction is more valuable than accommodation Is he right? Isn’t all hypothesizing done after (at least some) results are known? The issue with HARKing is when the HARKed hypothesis is presented as if it has been tested and conﬁrmed rather than merely suggested by the result of the test Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Individual Problems HARKing–Hypothesizing After Results are Known Kerr told us HARKing was bad because prediction is more valuable than accommodation Is he right? Isn’t all hypothesizing done after (at least some) results are known? The issue with HARKing is when the HARKed hypothesis is presented as if it has been tested and conﬁrmed rather than merely suggested by the result of the test Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Individual Problems HARKing–Hypothesizing After Results are Known Kerr told us HARKing was bad because prediction is more valuable than accommodation Is he right? Isn’t all hypothesizing done after (at least some) results are known? The issue with HARKing is when the HARKed hypothesis is presented as if it has been tested and conﬁrmed rather than merely suggested by the result of the test Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Individual Problems HARKing–Hypothesizing After Results are Known Kerr told us HARKing was bad because prediction is more valuable than accommodation Is he right? Isn’t all hypothesizing done after (at least some) results are known? The issue with HARKing is when the HARKed hypothesis is presented as if it has been tested and conﬁrmed rather than merely suggested by the result of the test Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Recap of Individual Problems HARKing–Hypothesizing After Results are Known Kerr told us HARKing was bad because prediction is more valuable than accommodation Is he right? Isn’t all hypothesizing done after (at least some) results are known? The issue with HARKing is when the HARKed hypothesis is presented as if it has been tested and conﬁrmed rather than merely suggested by the result of the test Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication More on HARKing So, how do we prevent the bad sort of HARKing? Note how HARKing can lead to poor replication–if the HARKed hypothesis is just a quirk of the sample, that connection won’t be seen in subsequent tests One innovation that has been put in place is “pre-registration” This practice ﬁxes a few of the problems we’ve identiﬁed: The study is accepted for publication prior to the test being done–it’s going to be published regardless of the result This removes the incentive/need to have some ground-breaking innovation to get published It also means you can’t revision the purpose of the test (as happens in HARKing) If your test has a surprising result that suggests a new hypothesis, you can present that as a candidate for future testing in your conclusion (but not as having been conﬁrmed by the test) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication More on HARKing So, how do we prevent the bad sort of HARKing? Note how HARKing can lead to poor replication–if the HARKed hypothesis is just a quirk of the sample, that connection won’t be seen in subsequent tests One innovation that has been put in place is “pre-registration” This practice ﬁxes a few of the problems we’ve identiﬁed: The study is accepted for publication prior to the test being done–it’s going to be published regardless of the result This removes the incentive/need to have some ground-breaking innovation to get published It also means you can’t revision the purpose of the test (as happens in HARKing) If your test has a surprising result that suggests a new hypothesis, you can present that as a candidate for future testing in your conclusion (but not as having been conﬁrmed by the test) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication More on HARKing So, how do we prevent the bad sort of HARKing? Note how HARKing can lead to poor replication–if the HARKed hypothesis is just a quirk of the sample, that connection won’t be seen in subsequent tests One innovation that has been put in place is “pre-registration” This practice ﬁxes a few of the problems we’ve identiﬁed: The study is accepted for publication prior to the test being done–it’s going to be published regardless of the result This removes the incentive/need to have some ground-breaking innovation to get published It also means you can’t revision the purpose of the test (as happens in HARKing) If your test has a surprising result that suggests a new hypothesis, you can present that as a candidate for future testing in your conclusion (but not as having been conﬁrmed by the test) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication More on HARKing So, how do we prevent the bad sort of HARKing? Note how HARKing can lead to poor replication–if the HARKed hypothesis is just a quirk of the sample, that connection won’t be seen in subsequent tests One innovation that has been put in place is “pre-registration” This practice ﬁxes a few of the problems we’ve identiﬁed: The study is accepted for publication prior to the test being done–it’s going to be published regardless of the result This removes the incentive/need to have some ground-breaking innovation to get published It also means you can’t revision the purpose of the test (as happens in HARKing) If your test has a surprising result that suggests a new hypothesis, you can present that as a candidate for future testing in your conclusion (but not as having been conﬁrmed by the test) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication More on HARKing So, how do we prevent the bad sort of HARKing? Note how HARKing can lead to poor replication–if the HARKed hypothesis is just a quirk of the sample, that connection won’t be seen in subsequent tests One innovation that has been put in place is “pre-registration” This practice ﬁxes a few of the problems we’ve identiﬁed: The study is accepted for publication prior to the test being done–it’s going to be published regardless of the result This removes the incentive/need to have some ground-breaking innovation to get published It also means you can’t revision the purpose of the test (as happens in HARKing) If your test has a surprising result that suggests a new hypothesis, you can present that as a candidate for future testing in your conclusion (but not as having been conﬁrmed by the test) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication More on HARKing So, how do we prevent the bad sort of HARKing? Note how HARKing can lead to poor replication–if the HARKed hypothesis is just a quirk of the sample, that connection won’t be seen in subsequent tests One innovation that has been put in place is “pre-registration” This practice ﬁxes a few of the problems we’ve identiﬁed: The study is accepted for publication prior to the test being done–it’s going to be published regardless of the result This removes the incentive/need to have some ground-breaking innovation to get published It also means you can’t revision the purpose of the test (as happens in HARKing) If your test has a surprising result that suggests a new hypothesis, you can present that as a candidate for future testing in your conclusion (but not as having been conﬁrmed by the test) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication More on HARKing So, how do we prevent the bad sort of HARKing? Note how HARKing can lead to poor replication–if the HARKed hypothesis is just a quirk of the sample, that connection won’t be seen in subsequent tests One innovation that has been put in place is “pre-registration” This practice ﬁxes a few of the problems we’ve identiﬁed: The study is accepted for publication prior to the test being done–it’s going to be published regardless of the result This removes the incentive/need to have some ground-breaking innovation to get published It also means you can’t revision the purpose of the test (as happens in HARKing) If your test has a surprising result that suggests a new hypothesis, you can present that as a candidate for future testing in your conclusion (but not as having been conﬁrmed by the test) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing There are a number of connections between Statistical Analysis and the Replication Crisis. Some suggest that there is an over-reliance on statistical signiﬁcance tests Some suggest that the convention of treating p < 0.05 as the threshold for signiﬁcance is too lax, and that p < 0.01 is more appropriate P-hacking Some suggest that frequentist/classical statistical methods should be replaced by Bayesian methods Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing There are a number of connections between Statistical Analysis and the Replication Crisis. Some suggest that there is an over-reliance on statistical signiﬁcance tests Some suggest that the convention of treating p < 0.05 as the threshold for signiﬁcance is too lax, and that p < 0.01 is more appropriate P-hacking Some suggest that frequentist/classical statistical methods should be replaced by Bayesian methods Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing There are a number of connections between Statistical Analysis and the Replication Crisis. Some suggest that there is an over-reliance on statistical signiﬁcance tests Some suggest that the convention of treating p < 0.05 as the threshold for signiﬁcance is too lax, and that p < 0.01 is more appropriate P-hacking Some suggest that frequentist/classical statistical methods should be replaced by Bayesian methods Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing There are a number of connections between Statistical Analysis and the Replication Crisis. Some suggest that there is an over-reliance on statistical signiﬁcance tests Some suggest that the convention of treating p < 0.05 as the threshold for signiﬁcance is too lax, and that p < 0.01 is more appropriate P-hacking Some suggest that frequentist/classical statistical methods should be replaced by Bayesian methods Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian vs Frequentist Recall from Week 4 the distinction we made between objective and subjective interpretations of probability These diﬀerent understandings of the meaning of probability lead to diﬀerent methodologies with using statistical information in scientiﬁc reasoning Frequentists favour the objective interpretation Bayesians favour the subjective interpretation Let’s illustrate with an example Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian vs Frequentist Recall from Week 4 the distinction we made between objective and subjective interpretations of probability These diﬀerent understandings of the meaning of probability lead to diﬀerent methodologies with using statistical information in scientiﬁc reasoning Frequentists favour the objective interpretation Bayesians favour the subjective interpretation Let’s illustrate with an example Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian vs Frequentist Recall from Week 4 the distinction we made between objective and subjective interpretations of probability These diﬀerent understandings of the meaning of probability lead to diﬀerent methodologies with using statistical information in scientiﬁc reasoning Frequentists favour the objective interpretation Bayesians favour the subjective interpretation Let’s illustrate with an example Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian vs Frequentist Recall from Week 4 the distinction we made between objective and subjective interpretations of probability These diﬀerent understandings of the meaning of probability lead to diﬀerent methodologies with using statistical information in scientiﬁc reasoning Frequentists favour the objective interpretation Bayesians favour the subjective interpretation Let’s illustrate with an example Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose I’m about to ﬂip a coin What’s the probability of the toss resulting in heads? Bayesian–0.5 Frequentist–0.5 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Now suppose I have already ﬂipped the coin, but haven’t revealed the result to you...what’s the probability of heads? Bayesian–0.5 Frequentist–??? This question doesn’t make sense to the frequentist It is what it is. The result is ﬁxed Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Now suppose I have already ﬂipped the coin, but haven’t revealed the result to you...what’s the probability of heads? Bayesian–0.5 Frequentist–??? This question doesn’t make sense to the frequentist It is what it is. The result is ﬁxed Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Now suppose I have already ﬂipped the coin, but haven’t revealed the result to you...what’s the probability of heads? Bayesian–0.5 Frequentist–??? This question doesn’t make sense to the frequentist It is what it is. The result is ﬁxed Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Now suppose I have already ﬂipped the coin, but haven’t revealed the result to you...what’s the probability of heads? Bayesian–0.5 Frequentist–??? This question doesn’t make sense to the frequentist It is what it is. The result is ﬁxed Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing So, what do Bayesians and Frequentists mean when they say the probability is 0.5? Frequentist In the long run, repeating this type of exercise results in heads roughly 50% of the time What is “this type of exercise”? General coin ﬂips (recall the discussion a few weeks ago about reference classes) Bayesian Here the probability is a reasonable expectation heads possibilities assuming the possibilities are equally likely Where the Frequentist focusses on the result of repeating a type of exercise, the Bayesian focusses on the uncertainty involved in the exercise Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing These philosophical diﬀerences in understanding probabilities lead to diﬀerences in how statistical methods are used in research Bayesians and frequentists go about estimating parameters diﬀerently For example, estimating the mean height of a population Bayesians and frequentists go about comparing hypotheses diﬀerently Frequentists use signiﬁcance tests Bayesians consider likelihood ratios Let’s consider these in turn Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing These philosophical diﬀerences in understanding probabilities lead to diﬀerences in how statistical methods are used in research Bayesians and frequentists go about estimating parameters diﬀerently For example, estimating the mean height of a population Bayesians and frequentists go about comparing hypotheses diﬀerently Frequentists use signiﬁcance tests Bayesians consider likelihood ratios Let’s consider these in turn Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing These philosophical diﬀerences in understanding probabilities lead to diﬀerences in how statistical methods are used in research Bayesians and frequentists go about estimating parameters diﬀerently For example, estimating the mean height of a population Bayesians and frequentists go about comparing hypotheses diﬀerently Frequentists use signiﬁcance tests Bayesians consider likelihood ratios Let’s consider these in turn Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing These philosophical diﬀerences in understanding probabilities lead to diﬀerences in how statistical methods are used in research Bayesians and frequentists go about estimating parameters diﬀerently For example, estimating the mean height of a population Bayesians and frequentists go about comparing hypotheses diﬀerently Frequentists use signiﬁcance tests Bayesians consider likelihood ratios Let’s consider these in turn Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing These philosophical diﬀerences in understanding probabilities lead to diﬀerences in how statistical methods are used in research Bayesians and frequentists go about estimating parameters diﬀerently For example, estimating the mean height of a population Bayesians and frequentists go about comparing hypotheses diﬀerently Frequentists use signiﬁcance tests Bayesians consider likelihood ratios Let’s consider these in turn Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing These philosophical diﬀerences in understanding probabilities lead to diﬀerences in how statistical methods are used in research Bayesians and frequentists go about estimating parameters diﬀerently For example, estimating the mean height of a population Bayesians and frequentists go about comparing hypotheses diﬀerently Frequentists use signiﬁcance tests Bayesians consider likelihood ratios Let’s consider these in turn Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Example–mean height of a population Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose we are interested in ﬁguring out the mean height of a population. As Howson and Urbach mention, Bayesians and Frequentists go about this diﬀerently: Frequentists treat this unknown value as a ﬁxed (ie non-random) quantity (call it µ) Fixed–it is what it is, like the coin that’s been ﬂipped To estimate this value, we take a random sample of the population The mean height of the sample group is a random variable We then take it that µ = m There is, of course, uncertainty here...how conﬁdent should we be that our estimate is correct? To answer this, frequentists point to the “conﬁdence interval” Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose we are interested in ﬁguring out the mean height of a population. As Howson and Urbach mention, Bayesians and Frequentists go about this diﬀerently: Frequentists treat this unknown value as a ﬁxed (ie non-random) quantity (call it µ) Fixed–it is what it is, like the coin that’s been ﬂipped To estimate this value, we take a random sample of the population The mean height of the sample group is a random variable We then take it that µ = m There is, of course, uncertainty here...how conﬁdent should we be that our estimate is correct? To answer this, frequentists point to the “conﬁdence interval” Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose we are interested in ﬁguring out the mean height of a population. As Howson and Urbach mention, Bayesians and Frequentists go about this diﬀerently: Frequentists treat this unknown value as a ﬁxed (ie non-random) quantity (call it µ) Fixed–it is what it is, like the coin that’s been ﬂipped To estimate this value, we take a random sample of the population The mean height of the sample group is a random variable We then take it that µ = m There is, of course, uncertainty here...how conﬁdent should we be that our estimate is correct? To answer this, frequentists point to the “conﬁdence interval” Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose we are interested in ﬁguring out the mean height of a population. As Howson and Urbach mention, Bayesians and Frequentists go about this diﬀerently: Frequentists treat this unknown value as a ﬁxed (ie non-random) quantity (call it µ) Fixed–it is what it is, like the coin that’s been ﬂipped To estimate this value, we take a random sample of the population The mean height of the sample group is a random variable We then take it that µ = m There is, of course, uncertainty here...how conﬁdent should we be that our estimate is correct? To answer this, frequentists point to the “conﬁdence interval” Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose we are interested in ﬁguring out the mean height of a population. As Howson and Urbach mention, Bayesians and Frequentists go about this diﬀerently: Frequentists treat this unknown value as a ﬁxed (ie non-random) quantity (call it µ) Fixed–it is what it is, like the coin that’s been ﬂipped To estimate this value, we take a random sample of the population The mean height of the sample group is a random variable We then take it that µ = m There is, of course, uncertainty here...how conﬁdent should we be that our estimate is correct? To answer this, frequentists point to the “conﬁdence interval” Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose we are interested in ﬁguring out the mean height of a population. As Howson and Urbach mention, Bayesians and Frequentists go about this diﬀerently: Frequentists treat this unknown value as a ﬁxed (ie non-random) quantity (call it µ) Fixed–it is what it is, like the coin that’s been ﬂipped To estimate this value, we take a random sample of the population The mean height of the sample group is a random variable We then take it that µ = m There is, of course, uncertainty here...how conﬁdent should we be that our estimate is correct? To answer this, frequentists point to the “conﬁdence interval” Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Suppose we are interested in ﬁguring out the mean height of a population. As Howson and Urbach mention, Bayesians and Frequentists go about this diﬀerently: Frequentists treat this unknown value as a ﬁxed (ie non-random) quantity (call it µ) Fixed–it is what it is, like the coin that’s been ﬂipped To estimate this value, we take a random sample of the population The mean height of the sample group is a random variable We then take it that µ = m There is, of course, uncertainty here...how conﬁdent should we be that our estimate is correct? To answer this, frequentists point to the “conﬁdence interval” Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Conﬁdence interval The conﬁdence interval is a purely mathematical (objective) value Suppose we target a 95% level Let σ denote the standard deviation of heights in the population We can use that value to calculate the standard deviation of the sample s = σ√n (where n is the sample size) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Conﬁdence interval The conﬁdence interval is a purely mathematical (objective) value Suppose we target a 95% level Let σ denote the standard deviation of heights in the population We can use that value to calculate the standard deviation of the sample s = σ√n (where n is the sample size) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Conﬁdence interval The conﬁdence interval is a purely mathematical (objective) value Suppose we target a 95% level Let σ denote the standard deviation of heights in the population We can use that value to calculate the standard deviation of the sample s = σ√n (where n is the sample size) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Conﬁdence interval The conﬁdence interval is a purely mathematical (objective) value Suppose we target a 95% level Let σ denote the standard deviation of heights in the population We can use that value to calculate the standard deviation of the sample s = σ√n (where n is the sample size) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Conﬁdence interval The conﬁdence interval is a purely mathematical (objective) value Suppose we target a 95% level Let σ denote the standard deviation of heights in the population We can use that value to calculate the standard deviation of the sample s = σ√n (where n is the sample size) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Is this conﬁdence interval really objective? Howson & Urbach argue that it is not First, 95% conﬁdence interval does not mean that the probability of µ being in the range is 0.95 (because µ is not a random variable and therefore has no probability) It may be tempting to think this way, but that is to shift from an objective to a subjective understanding of probability Second, the validity of this calculation depends on the sample size n having the value that it has. But, why does it have that value? Howson & Urbach note that this depends on the (often private) intentions of the experimenter So, there’s some obstacles to understanding conﬁdence intervals as purely objective Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Is this conﬁdence interval really objective? Howson & Urbach argue that it is not First, 95% conﬁdence interval does not mean that the probability of µ being in the range is 0.95 (because µ is not a random variable and therefore has no probability) It may be tempting to think this way, but that is to shift from an objective to a subjective understanding of probability Second, the validity of this calculation depends on the sample size n having the value that it has. But, why does it have that value? Howson & Urbach note that this depends on the (often private) intentions of the experimenter So, there’s some obstacles to understanding conﬁdence intervals as purely objective Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Is this conﬁdence interval really objective? Howson & Urbach argue that it is not First, 95% conﬁdence interval does not mean that the probability of µ being in the range is 0.95 (because µ is not a random variable and therefore has no probability) It may be tempting to think this way, but that is to shift from an objective to a subjective understanding of probability Second, the validity of this calculation depends on the sample size n having the value that it has. But, why does it have that value? Howson & Urbach note that this depends on the (often private) intentions of the experimenter So, there’s some obstacles to understanding conﬁdence intervals as purely objective Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Is this conﬁdence interval really objective? Howson & Urbach argue that it is not First, 95% conﬁdence interval does not mean that the probability of µ being in the range is 0.95 (because µ is not a random variable and therefore has no probability) It may be tempting to think this way, but that is to shift from an objective to a subjective understanding of probability Second, the validity of this calculation depends on the sample size n having the value that it has. But, why does it have that value? Howson & Urbach note that this depends on the (often private) intentions of the experimenter So, there’s some obstacles to understanding conﬁdence intervals as purely objective Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Is this conﬁdence interval really objective? Howson & Urbach argue that it is not First, 95% conﬁdence interval does not mean that the probability of µ being in the range is 0.95 (because µ is not a random variable and therefore has no probability) It may be tempting to think this way, but that is to shift from an objective to a subjective understanding of probability Second, the validity of this calculation depends on the sample size n having the value that it has. But, why does it have that value? Howson & Urbach note that this depends on the (often private) intentions of the experimenter So, there’s some obstacles to understanding conﬁdence intervals as purely objective Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Is this conﬁdence interval really objective? Howson & Urbach argue that it is not First, 95% conﬁdence interval does not mean that the probability of µ being in the range is 0.95 (because µ is not a random variable and therefore has no probability) It may be tempting to think this way, but that is to shift from an objective to a subjective understanding of probability Second, the validity of this calculation depends on the sample size n having the value that it has. But, why does it have that value? Howson & Urbach note that this depends on the (often private) intentions of the experimenter So, there’s some obstacles to understanding conﬁdence intervals as purely objective Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian estimation So, how might the Bayesian statistician estimate µ? Unlike the frequentist, Bayesians treat µ as a random variable They begin with an initial guess about the distribution of µ As data comes in, they update that distribution according to Bayes’ rule As the sample size increases, the eﬀect of the prior distribution is minimized That means that as evidence accumulates, two researchers with very diﬀerent prior distributions will come to the same posterior distribution Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian estimation So, how might the Bayesian statistician estimate µ? Unlike the frequentist, Bayesians treat µ as a random variable They begin with an initial guess about the distribution of µ As data comes in, they update that distribution according to Bayes’ rule As the sample size increases, the eﬀect of the prior distribution is minimized That means that as evidence accumulates, two researchers with very diﬀerent prior distributions will come to the same posterior distribution Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian estimation So, how might the Bayesian statistician estimate µ? Unlike the frequentist, Bayesians treat µ as a random variable They begin with an initial guess about the distribution of µ As data comes in, they update that distribution according to Bayes’ rule As the sample size increases, the eﬀect of the prior distribution is minimized That means that as evidence accumulates, two researchers with very diﬀerent prior distributions will come to the same posterior distribution Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian estimation So, how might the Bayesian statistician estimate µ? Unlike the frequentist, Bayesians treat µ as a random variable They begin with an initial guess about the distribution of µ As data comes in, they update that distribution according to Bayes’ rule As the sample size increases, the eﬀect of the prior distribution is minimized That means that as evidence accumulates, two researchers with very diﬀerent prior distributions will come to the same posterior distribution Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Bayesian estimation So, how might the Bayesian statistician estimate µ? Unlike the frequentist, Bayesians treat µ as a random variable They begin with an initial guess about the distribution of µ As data comes in, they update that distribution according to Bayes’ rule As the sample size increases, the eﬀect of the prior distribution is minimized That means that as evidence accumulates, two researchers with very diﬀerent prior distributions will come to the same posterior distribution Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is signiﬁcance testing? Signiﬁcance tests are a frequentist method for testing a hypothesis with a statistical result They are used to assess how well a particular sample of statistical evidence supports some hypothesis about the population being studied The hypothesis is about some feature of the population, such as the proportion that have some trait or the mean (average) value of some trait in the population The result of the test is a probability of the result given the “null hypothesis” (H0) The null hypothesis is the hypothesis that the study will not demonstrate an eﬀect It contrasts with the “alternative hypothesis” (Ha) which is the hypothesis we are actually interested in Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is signiﬁcance testing? Signiﬁcance tests are a frequentist method for testing a hypothesis with a statistical result They are used to assess how well a particular sample of statistical evidence supports some hypothesis about the population being studied The hypothesis is about some feature of the population, such as the proportion that have some trait or the mean (average) value of some trait in the population The result of the test is a probability of the result given the “null hypothesis” (H0) The null hypothesis is the hypothesis that the study will not demonstrate an eﬀect It contrasts with the “alternative hypothesis” (Ha) which is the hypothesis we are actually interested in Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is signiﬁcance testing? Signiﬁcance tests are a frequentist method for testing a hypothesis with a statistical result They are used to assess how well a particular sample of statistical evidence supports some hypothesis about the population being studied The hypothesis is about some feature of the population, such as the proportion that have some trait or the mean (average) value of some trait in the population The result of the test is a probability of the result given the “null hypothesis” (H0) The null hypothesis is the hypothesis that the study will not demonstrate an eﬀect It contrasts with the “alternative hypothesis” (Ha) which is the hypothesis we are actually interested in Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is signiﬁcance testing? Signiﬁcance tests are a frequentist method for testing a hypothesis with a statistical result They are used to assess how well a particular sample of statistical evidence supports some hypothesis about the population being studied The hypothesis is about some feature of the population, such as the proportion that have some trait or the mean (average) value of some trait in the population The result of the test is a probability of the result given the “null hypothesis” (H0) The null hypothesis is the hypothesis that the study will not demonstrate an eﬀect It contrasts with the “alternative hypothesis” (Ha) which is the hypothesis we are actually interested in Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is signiﬁcance testing? Signiﬁcance tests are a frequentist method for testing a hypothesis with a statistical result They are used to assess how well a particular sample of statistical evidence supports some hypothesis about the population being studied The hypothesis is about some feature of the population, such as the proportion that have some trait or the mean (average) value of some trait in the population The result of the test is a probability of the result given the “null hypothesis” (H0) The null hypothesis is the hypothesis that the study will not demonstrate an eﬀect It contrasts with the “alternative hypothesis” (Ha) which is the hypothesis we are actually interested in Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is signiﬁcance testing? Signiﬁcance tests are a frequentist method for testing a hypothesis with a statistical result They are used to assess how well a particular sample of statistical evidence supports some hypothesis about the population being studied The hypothesis is about some feature of the population, such as the proportion that have some trait or the mean (average) value of some trait in the population The result of the test is a probability of the result given the “null hypothesis” (H0) The null hypothesis is the hypothesis that the study will not demonstrate an eﬀect It contrasts with the “alternative hypothesis” (Ha) which is the hypothesis we are actually interested in Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Fisher Source: Probability Course (see syllabus for link) The x-axis represents all the possible results of the test The curve represents the probability density function for test results given H0 The tail ends of the curve represent test values that are signiﬁcant (ie. mathematically unlikely if H0 were true) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Fisher Source: Probability Course (see syllabus for link) The x-axis represents all the possible results of the test The curve represents the probability density function for test results given H0 The tail ends of the curve represent test values that are signiﬁcant (ie. mathematically unlikely if H0 were true) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Fisher Source: Probability Course (see syllabus for link) The x-axis represents all the possible results of the test The curve represents the probability density function for test results given H0 The tail ends of the curve represent test values that are signiﬁcant (ie. mathematically unlikely if H0 were true) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing What is the logic of signiﬁcance testing? Why not just test Ha directly? To answer this, we need to consider the logic behind signiﬁcance testing While it uses probabilities, the logic is deductive Recall Popper’s schema: 1 Theory T implies Observation O 2 O is not observed 3 Therefore, T is false Signiﬁcance testing is an instance of this schema: 1 H0 predicts a non-signiﬁcant result 2 A signiﬁcant result is observed 3 Therefore, H0 is rejected If H0 is rejected, then this corroborates Ha But it does not conﬁrm Ha! (Nothing does, because this framework is Popperian) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Fisher vs Neyman-Pearson Source: Cyril Pernet Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing But what actually is “signiﬁcance”? Signiﬁcance is a measure of how (im)probable the result would be if H0 were true A signiﬁcant result is one that is mathematically unlikely if H0 were true In terms of probability, a signiﬁcant result is one in which P(R|H0) is very low Of course, we know that this is not the same as P(H0|R) Note, Travers makes this mistake! When describing what p = 0.26 means: “In other words...there is a 26 percent probability that the null hypothesis is true given these results” (p.214) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing But what actually is “signiﬁcance”? Signiﬁcance is a measure of how (im)probable the result would be if H0 were true A signiﬁcant result is one that is mathematically unlikely if H0 were true In terms of probability, a signiﬁcant result is one in which P(R|H0) is very low Of course, we know that this is not the same as P(H0|R) Note, Travers makes this mistake! When describing what p = 0.26 means: “In other words...there is a 26 percent probability that the null hypothesis is true given these results” (p.214) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing But what actually is “signiﬁcance”? Signiﬁcance is a measure of how (im)probable the result would be if H0 were true A signiﬁcant result is one that is mathematically unlikely if H0 were true In terms of probability, a signiﬁcant result is one in which P(R|H0) is very low Of course, we know that this is not the same as P(H0|R) Note, Travers makes this mistake! When describing what p = 0.26 means: “In other words...there is a 26 percent probability that the null hypothesis is true given these results” (p.214) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing But what actually is “signiﬁcance”? Signiﬁcance is a measure of how (im)probable the result would be if H0 were true A signiﬁcant result is one that is mathematically unlikely if H0 were true In terms of probability, a signiﬁcant result is one in which P(R|H0) is very low Of course, we know that this is not the same as P(H0|R) Note, Travers makes this mistake! When describing what p = 0.26 means: “In other words...there is a 26 percent probability that the null hypothesis is true given these results” (p.214) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing But what actually is “signiﬁcance”? Signiﬁcance is a measure of how (im)probable the result would be if H0 were true A signiﬁcant result is one that is mathematically unlikely if H0 were true In terms of probability, a signiﬁcant result is one in which P(R|H0) is very low Of course, we know that this is not the same as P(H0|R) Note, Travers makes this mistake! When describing what p = 0.26 means: “In other words...there is a 26 percent probability that the null hypothesis is true given these results” (p.214) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Rejecting H0 Signiﬁcance is measured in p-values The lower the p−value, the higher the signiﬁcance of the result of the test We can set a critical value α to determine the threshold for signiﬁcance (ie. where p < 0.05) Often, you can look these critical values up in pre-calculated tables, based on the structure of the test Again, what does signiﬁcance tell us? What does it mean if our result passes the threshold for signiﬁcance? That the result would be unlikely if H0 were true Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Rejecting H0 Signiﬁcance is measured in p-values The lower the p−value, the higher the signiﬁcance of the result of the test We can set a critical value α to determine the threshold for signiﬁcance (ie. where p < 0.05) Often, you can look these critical values up in pre-calculated tables, based on the structure of the test Again, what does signiﬁcance tell us? What does it mean if our result passes the threshold for signiﬁcance? That the result would be unlikely if H0 were true Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Rejecting H0 Signiﬁcance is measured in p-values The lower the p−value, the higher the signiﬁcance of the result of the test We can set a critical value α to determine the threshold for signiﬁcance (ie. where p < 0.05) Often, you can look these critical values up in pre-calculated tables, based on the structure of the test Again, what does signiﬁcance tell us? What does it mean if our result passes the threshold for signiﬁcance? That the result would be unlikely if H0 were true Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Rejecting H0 Signiﬁcance is measured in p-values The lower the p−value, the higher the signiﬁcance of the result of the test We can set a critical value α to determine the threshold for signiﬁcance (ie. where p < 0.05) Often, you can look these critical values up in pre-calculated tables, based on the structure of the test Again, what does signiﬁcance tell us? What does it mean if our result passes the threshold for signiﬁcance? That the result would be unlikely if H0 were true Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Rejecting H0 Signiﬁcance is measured in p-values The lower the p−value, the higher the signiﬁcance of the result of the test We can set a critical value α to determine the threshold for signiﬁcance (ie. where p < 0.05) Often, you can look these critical values up in pre-calculated tables, based on the structure of the test Again, what does signiﬁcance tell us? What does it mean if our result passes the threshold for signiﬁcance? That the result would be unlikely if H0 were true Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Rejecting H0 Signiﬁcance is measured in p-values The lower the p−value, the higher the signiﬁcance of the result of the test We can set a critical value α to determine the threshold for signiﬁcance (ie. where p < 0.05) Often, you can look these critical values up in pre-calculated tables, based on the structure of the test Again, what does signiﬁcance tell us? What does it mean if our result passes the threshold for signiﬁcance? That the result would be unlikely if H0 were true Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Source: xkcd.com Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing P-hacking This refers to a collection of less-than-honest practices (fudging): Stop collecting data once a result that is statistically signiﬁcant has been obtained Checking statistical signiﬁcance in order to decide whether to collect more data Not reporting on the eﬀect of removing outliers from the data set Deciding to remove outliers only after checking the eﬀect this has on statistical signiﬁcance These practices raise the likelihood of making a Type 1 error Why? What does that mean? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing P-hacking This refers to a collection of less-than-honest practices (fudging): Stop collecting data once a result that is statistically signiﬁcant has been obtained Checking statistical signiﬁcance in order to decide whether to collect more data Not reporting on the eﬀect of removing outliers from the data set Deciding to remove outliers only after checking the eﬀect this has on statistical signiﬁcance These practices raise the likelihood of making a Type 1 error Why? What does that mean? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing P-hacking This refers to a collection of less-than-honest practices (fudging): Stop collecting data once a result that is statistically signiﬁcant has been obtained Checking statistical signiﬁcance in order to decide whether to collect more data Not reporting on the eﬀect of removing outliers from the data set Deciding to remove outliers only after checking the eﬀect this has on statistical signiﬁcance These practices raise the likelihood of making a Type 1 error Why? What does that mean? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing P-hacking This refers to a collection of less-than-honest practices (fudging): Stop collecting data once a result that is statistically signiﬁcant has been obtained Checking statistical signiﬁcance in order to decide whether to collect more data Not reporting on the eﬀect of removing outliers from the data set Deciding to remove outliers only after checking the eﬀect this has on statistical signiﬁcance These practices raise the likelihood of making a Type 1 error Why? What does that mean? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing P-hacking This refers to a collection of less-than-honest practices (fudging): Stop collecting data once a result that is statistically signiﬁcant has been obtained Checking statistical signiﬁcance in order to decide whether to collect more data Not reporting on the eﬀect of removing outliers from the data set Deciding to remove outliers only after checking the eﬀect this has on statistical signiﬁcance These practices raise the likelihood of making a Type 1 error Why? What does that mean? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing P-hacking This refers to a collection of less-than-honest practices (fudging): Stop collecting data once a result that is statistically signiﬁcant has been obtained Checking statistical signiﬁcance in order to decide whether to collect more data Not reporting on the eﬀect of removing outliers from the data set Deciding to remove outliers only after checking the eﬀect this has on statistical signiﬁcance These practices raise the likelihood of making a Type 1 error Why? What does that mean? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing P-hacking Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Should signiﬁcance be the “gold standard”? Signiﬁcance is not the only relevant statistical measure Statistical power–the probability of correctly rejecting H0 (eg. Consider a sample size of 50, and an eﬀect that occurs in 1% of cases. This is a case of low statistical power.) Eﬀect size–how great a diﬀerence the intervention makes (eg. Both alcohol and tobacco have been shown to have a statistically signiﬁcant increase in cancer risk. But alcohol makes it 2-3 times more likely to get cancer, while smoking makes it 15-30 times more likely.) Statistically insigniﬁcant ﬁndings can also be of interest. For example, a famous study out of Denmark in 2003 found no signiﬁcant correlation between thimerasol exposure (in vaccines) and autism diagnoses. (In fact, they were slightly negatively correlated–as thimerasol use went down, the rate of autism diagnoses went up.) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Should signiﬁcance be the “gold standard”? Signiﬁcance is not the only relevant statistical measure Statistical power–the probability of correctly rejecting H0 (eg. Consider a sample size of 50, and an eﬀect that occurs in 1% of cases. This is a case of low statistical power.) Eﬀect size–how great a diﬀerence the intervention makes (eg. Both alcohol and tobacco have been shown to have a statistically signiﬁcant increase in cancer risk. But alcohol makes it 2-3 times more likely to get cancer, while smoking makes it 15-30 times more likely.) Statistically insigniﬁcant ﬁndings can also be of interest. For example, a famous study out of Denmark in 2003 found no signiﬁcant correlation between thimerasol exposure (in vaccines) and autism diagnoses. (In fact, they were slightly negatively correlated–as thimerasol use went down, the rate of autism diagnoses went up.) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Should signiﬁcance be the “gold standard”? Signiﬁcance is not the only relevant statistical measure Statistical power–the probability of correctly rejecting H0 (eg. Consider a sample size of 50, and an eﬀect that occurs in 1% of cases. This is a case of low statistical power.) Eﬀect size–how great a diﬀerence the intervention makes (eg. Both alcohol and tobacco have been shown to have a statistically signiﬁcant increase in cancer risk. But alcohol makes it 2-3 times more likely to get cancer, while smoking makes it 15-30 times more likely.) Statistically insigniﬁcant ﬁndings can also be of interest. For example, a famous study out of Denmark in 2003 found no signiﬁcant correlation between thimerasol exposure (in vaccines) and autism diagnoses. (In fact, they were slightly negatively correlated–as thimerasol use went down, the rate of autism diagnoses went up.) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Should signiﬁcance be the “gold standard”? Signiﬁcance is not the only relevant statistical measure Statistical power–the probability of correctly rejecting H0 (eg. Consider a sample size of 50, and an eﬀect that occurs in 1% of cases. This is a case of low statistical power.) Eﬀect size–how great a diﬀerence the intervention makes (eg. Both alcohol and tobacco have been shown to have a statistically signiﬁcant increase in cancer risk. But alcohol makes it 2-3 times more likely to get cancer, while smoking makes it 15-30 times more likely.) Statistically insigniﬁcant ﬁndings can also be of interest. For example, a famous study out of Denmark in 2003 found no signiﬁcant correlation between thimerasol exposure (in vaccines) and autism diagnoses. (In fact, they were slightly negatively correlated–as thimerasol use went down, the rate of autism diagnoses went up.) Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Strengths and Weaknesses of NHST Strengths It gives a standardized framework for testing hypotheses Anyone who has been trained in statistics can verify the conclusion just using math It is a useful tool to evaluate claims about about whether an intervention will have the eﬀect we are interested in (eg. oral reading ﬂuency intervention) Weaknesses It is easy to misunderstand Choosing p < 0.05 is a bit arbitrary and inﬂexible. What about the consequences of rejecting/failing to reject the H0? It is tempting to see signiﬁcance as a pass/fail measure of whether the test was successful. But that’s overly simplistic Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Some suggest that p < 0.05 is too relaxed of a threshold, and we should change the benchmark p < 0.01 to improve replication This could actually make the problem worse! Take the set of statistically signiﬁcant ﬁndings: this is a mixture of honest ﬁndings and fraudulent ones What happens to that ratio if we reduce the amount of honest ﬁndings? Should there be a “one-size ﬁts all” approach? p < 0.05 is considered signiﬁcant by convention...but it’s a bit arbitrary Should there even be a single standard value? What would Heather Douglas say? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Others suggest we need to pay more attention to other statistical factors Eﬀect size Statistical power “File-drawer eﬀect”–publication bias in favour of signiﬁcant results (at the expense of insigniﬁcant results) We mentioned pre-registration earlier. Here is another way that pre-registration is useful (as mentioned in the Romero & Sprenger article) What do you think? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Others suggest we need to pay more attention to other statistical factors Eﬀect size Statistical power “File-drawer eﬀect”–publication bias in favour of signiﬁcant results (at the expense of insigniﬁcant results) We mentioned pre-registration earlier. Here is another way that pre-registration is useful (as mentioned in the Romero & Sprenger article) What do you think? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Others suggest we need to pay more attention to other statistical factors Eﬀect size Statistical power “File-drawer eﬀect”–publication bias in favour of signiﬁcant results (at the expense of insigniﬁcant results) We mentioned pre-registration earlier. Here is another way that pre-registration is useful (as mentioned in the Romero & Sprenger article) What do you think? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Others suggest we need to pay more attention to other statistical factors Eﬀect size Statistical power “File-drawer eﬀect”–publication bias in favour of signiﬁcant results (at the expense of insigniﬁcant results) We mentioned pre-registration earlier. Here is another way that pre-registration is useful (as mentioned in the Romero & Sprenger article) What do you think? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Others suggest we need to pay more attention to other statistical factors Eﬀect size Statistical power “File-drawer eﬀect”–publication bias in favour of signiﬁcant results (at the expense of insigniﬁcant results) We mentioned pre-registration earlier. Here is another way that pre-registration is useful (as mentioned in the Romero & Sprenger article) What do you think? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing How does NHST relate to replication? Others suggest we need to pay more attention to other statistical factors Eﬀect size Statistical power “File-drawer eﬀect”–publication bias in favour of signiﬁcant results (at the expense of insigniﬁcant results) We mentioned pre-registration earlier. Here is another way that pre-registration is useful (as mentioned in the Romero & Sprenger article) What do you think? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? One proposal has been that we should switch from frequentism (NHST) to Bayesian analysis One note: Bayesian analyses often make use of the “Bayes Factor” (Recall from Week 5) The Bayes Factor is a likelihood ratio P(R|Ha) P(R|H0) The Bayes Factor is a contrastive measure of how strongly the evidence favours Ha over H0 If BF = 1, then the evidence is neutral (supports each hypothesis equally) If BF > 1, then the evidence supports Ha If BF < 1, then the evidence supports H0 By convention: “Inconclusive” is when 1 3 < BF < 3 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? One proposal has been that we should switch from frequentism (NHST) to Bayesian analysis One note: Bayesian analyses often make use of the “Bayes Factor” (Recall from Week 5) The Bayes Factor is a likelihood ratio P(R|Ha) P(R|H0) The Bayes Factor is a contrastive measure of how strongly the evidence favours Ha over H0 If BF = 1, then the evidence is neutral (supports each hypothesis equally) If BF > 1, then the evidence supports Ha If BF < 1, then the evidence supports H0 By convention: “Inconclusive” is when 1 3 < BF < 3 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? One proposal has been that we should switch from frequentism (NHST) to Bayesian analysis One note: Bayesian analyses often make use of the “Bayes Factor” (Recall from Week 5) The Bayes Factor is a likelihood ratio P(R|Ha) P(R|H0) The Bayes Factor is a contrastive measure of how strongly the evidence favours Ha over H0 If BF = 1, then the evidence is neutral (supports each hypothesis equally) If BF > 1, then the evidence supports Ha If BF < 1, then the evidence supports H0 By convention: “Inconclusive” is when 1 3 < BF < 3 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? One proposal has been that we should switch from frequentism (NHST) to Bayesian analysis One note: Bayesian analyses often make use of the “Bayes Factor” (Recall from Week 5) The Bayes Factor is a likelihood ratio P(R|Ha) P(R|H0) The Bayes Factor is a contrastive measure of how strongly the evidence favours Ha over H0 If BF = 1, then the evidence is neutral (supports each hypothesis equally) If BF > 1, then the evidence supports Ha If BF < 1, then the evidence supports H0 By convention: “Inconclusive” is when 1 3 < BF < 3 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? One proposal has been that we should switch from frequentism (NHST) to Bayesian analysis One note: Bayesian analyses often make use of the “Bayes Factor” (Recall from Week 5) The Bayes Factor is a likelihood ratio P(R|Ha) P(R|H0) The Bayes Factor is a contrastive measure of how strongly the evidence favours Ha over H0 If BF = 1, then the evidence is neutral (supports each hypothesis equally) If BF > 1, then the evidence supports Ha If BF < 1, then the evidence supports H0 By convention: “Inconclusive” is when 1 3 < BF < 3 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? One proposal has been that we should switch from frequentism (NHST) to Bayesian analysis One note: Bayesian analyses often make use of the “Bayes Factor” (Recall from Week 5) The Bayes Factor is a likelihood ratio P(R|Ha) P(R|H0) The Bayes Factor is a contrastive measure of how strongly the evidence favours Ha over H0 If BF = 1, then the evidence is neutral (supports each hypothesis equally) If BF > 1, then the evidence supports Ha If BF < 1, then the evidence supports H0 By convention: “Inconclusive” is when 1 3 < BF < 3 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? One proposal has been that we should switch from frequentism (NHST) to Bayesian analysis One note: Bayesian analyses often make use of the “Bayes Factor” (Recall from Week 5) The Bayes Factor is a likelihood ratio P(R|Ha) P(R|H0) The Bayes Factor is a contrastive measure of how strongly the evidence favours Ha over H0 If BF = 1, then the evidence is neutral (supports each hypothesis equally) If BF > 1, then the evidence supports Ha If BF < 1, then the evidence supports H0 By convention: “Inconclusive” is when 1 3 < BF < 3 Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Inconclusive Evidence Where we would see 1 3 < BF < 3 on either of these graphs? Source: Cyril Pernet Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Can Bayesian analysis help replication? Romero & Sprenger: There is less of a “ﬁle-drawer” eﬀect with Bayesian analysis Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing This lends some support to the claim that Bayesian analysis can improve replication Where resources are limited, and where inconclusive evidence is suppressed, Bayesian analysis can be more accurate But, Bayesian analysis is imperfect as well (eg. it tends to underestimate eﬀect size) Summing up: NHST is not the cause of the replication crisis Misapplication of it (through suppression of inconclusive results) does contribute A switch to Bayesian analysis might help improve that problem But other changes like publishing data sets (facilitating meta-analysis) and pre-accepting studies for publication (to avoid suppressing valuable data) should also be encouraged Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Summing up There isn’t a single cause for the replication crisis We shouldn’t expect a single cure A varied approach is needed Funding for replications Pre-registration/pre-acceptance for publication Better use of statistical methods Publication of all data (to facilitate better meta-analysis) Other? Gary Neels COGS 303 Instructions for Presentations Quick Recap from Last Week Statistics and Replication Estimating parameters Signiﬁcance testing Readings for next week Readings for next week Dentith and Keeley “The Applied Epistemology of Conspiracy Theories: an Overview” Grimes “On the Viability of Conspiratorial Beliefs” O’Connor and Weatherall “The Social Network” Gary Neels COGS 303","libVersion":"0.3.2","langs":""}