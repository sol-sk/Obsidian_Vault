{"path":"W2023T2/W2023T2 Files/Slides/annotated-COGS300-L12-LLMs.pdf","text":"COGS300 Large Language Models Instructor: Márton Sóskuthy marton.soskuthy@ubc.ca TAs: Daichi Furukawa · Victoria Lim · Amy Wang cogs.300@ubc.ca 2 AI hype our goal: to cut through the noise! my own take: LLMs are not intelligent, but they are also not stupid 5 Language ModelsLanguage Models next word prediction: The internet is also called the world wide ____ 6 Language ModelsLanguage Models next word prediction up until recently: N-gram models unigram: probability of target word The internet is also called the world wide the bigram: probability given previous word The internet is also called the world wide open trigram: probability given previous two words The internet is also called the world wide web … 7 Language ModelsLanguage Models Better performance via deep learning RNNs / LSTMs The internet is also called the ______________ 8 Language ModelsLanguage Models Better performance via deep learning RNNs / LSTMs The internet is also called the world wide web 9 Language ModelsLanguage Models Better performance via deep learning – but not perfect! RNNs / LSTMs The internet – whose popularity started to rise in the early 90s – is also called the Big Apple 10 Language ModelsLanguage Models Transformer architectures Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30.11 Language ModelsLanguage Models Transformer architectures A few key innovations, including ATTENTION The internet – whose popularity started to rise in the early 90s – is also called the world wide web 12 Language ModelsLanguage Models Transformer architectures Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., ... & Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30. Encoder Decoder 13 Language ModelsLanguage Models Transformer architectures Encoder Decoder e.g. BERT e.g. GPT Bidirectional Encoder Representations from Transformers Generative Pre-trained Transformer Large Language Models main differences from earlier models: 1. training 2. size Large Language Models Training: 1. unsupervised pre-training: huge amounts of text to learn statistical regularities governing language use 2. supervised ﬁne-tuning: much smaller data sets used to ﬁne-tune for speciﬁc tasks Large Language Models Size (number of parameters) https://huggingface.co/blog/large-language-models Large Language Models Size (pre-training data): GPT-1: BookCorpus: 4.5 GB of text GPT-2: WebText: 40 GB of text GPT-3: 570 GB of text (most of the internet) https://huggingface.co/blog/large-language-models Large Language Models innovation in ChatGPT: Reinforcement Learning from Human Feedback (RLHF) https://arxiv.org/pdf/2203.02155.pdf Large Language Models Mahowald et al. (2023): Large Language Models are… • good at formal language competence • bad at functional language competence Large Language Models Mahowald et al. (2023): examples of functional language competence: 1. formal reasoning 2. world knowledge and commonsense reasoning 3. situation modeling 4. social reasoning (pragmatics and intent)","libVersion":"0.3.2","langs":""}