{"path":"W2023T2/W2023T2 Files/Slides/PSYC218-L12-Regression_part2.pdf","text":"Learning Objectives • Describe linear regression and construct (fit) linear models to data • Build equations for simple linear regression and multiple regression • Calculate and interpret standard error of the estimate • Contrast r, r2, R, and R2 1 Calculate 𝑠!|# from raw data 𝑠!|# = $$!%[∑ $!%(∑ $)(∑ !) ( ]* ++$ &%' \t 2 Night Sleep (X) Grump (Y) X2 Y2 XY 9 7.40 60 54.76 3600 444 24 7.86 56 61.78 3136 440.16 28 6.93 66 48.025 4356 457.38 60 6.22 67 38.688 4489 416.74 99 6.45 55 41.602 3025 354.75 N = 5 Σ𝑋 = 34.86 Σ𝑌 = 304 Σ𝑋2 = 244.855 Σ𝑌2 = 18606 Σ𝑋𝑌 = 2113.03 55.0 57.5 60.0 62.5 65.0 67.5 6.5 7.0 7.5 Sleep_hrsGrump Calculate Standard Error of the Est. 3 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 (Σ𝑌)2= 92416 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 sY|X = 122.8 − [2113.03\t − 34.86 304 5 ]! 1.811 \t 5\t − 2 Calculate 𝑠$|& 4 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 Σ𝑋 2= 1215.22 (Σ𝑌)2= 92416 Σ𝑋2= 244.855 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 sY|X = 122.8 − [2113.03\t − 𝟐𝟏𝟏𝟗. 𝟒𝟖𝟖]! 1.811 \t 𝟑 Calculate 𝑠$|& 5 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 Σ𝑋 2= 1215.22 (Σ𝑌)2= 92416 Σ𝑋2= 244.855 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 sY|X = 122.8 − [−𝟔. 𝟒𝟓𝟖]! 1.811 \t 3 Calculate 𝑠$|& 6 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 Σ𝑋 2= 1215.22 (Σ𝑌)2= 92416 Σ𝑋2= 244.855 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 sY|X = 122.8 − 𝟒𝟏. 𝟕𝟎𝟔 1.811 \t 3 Calculate 𝑠$|& 7 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 Σ𝑋 2= 1215.22 (Σ𝑌)2= 92416 Σ𝑋2= 244.855 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 sY|X = 122.8 − 𝟐𝟑. 𝟎𝟑\t 3 Calculate 𝑠$|& 8 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 Σ𝑋 2= 1215.22 (Σ𝑌)2= 92416 Σ𝑋2= 244.855 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 sY|X = 𝟗𝟗. 𝟕𝟕 3 Calculate 𝑠$|& 9 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 Σ𝑋 2= 1215.22 (Σ𝑌)2= 92416 Σ𝑋2= 244.855 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 sY|X = 𝟑𝟑. 𝟐𝟓𝟕 Calculate 𝑠$|& 10 𝑠$|& = ''!([∑ $!% (∑ $)(∑ !) ( ] * ++$ )(* \t SSY = Σ𝑌2 - (,$)* ) N = 5 Σ𝑋= 34.86 Σ𝑌= 304 Σ𝑋 2= 1215.22 (Σ𝑌)2= 92416 Σ𝑋2= 244.855 Σ𝑌2= 18606 Σ𝑋𝑌= 2113.03 SSX = 1.811 SSY = 122.8 55.0 57.5 60.0 62.5 65.0 67.5 6.5 7.0 7.5 Sleep_hrsGrumpsY|X = 𝟓. 𝟕𝟔𝟕 Homoscedasticity = consistent error (or same scatter) 11 Homoscedasticity = consistent error (or same scatter) 12 Multiple Regression • Regression model contains 2 or more predictors – Still have only 1 criterion • Quantitatively, our prediction will always improve…how would we know? – Standard error of the estimate (sy|x) decreases, or… – Multiple coefficient of determination (R2) increases 16 Y’ = bYX + aY Y’ = b1X1 + b2X2 + aY Example: Predicting Happiness • Two predictors: Income & Optimism 17 Y’happy = b$X$ + bopt.Xopt. + aY r2 = .31 r2 = .23 Trick Question: What is R2 ? R2 in Multiple Regression • We cannot simply add up the r2 values • Goal of Prediction: shade as much of the blue circle as possible – Challenge: Predictors might be explaining the same variability 18 R2 in Multiple Regression • Worst case: Fully redundant predictors • Goal: shade as much of the blue circle as possible – Challenge: Predictors explaining exact same variability 19 Optimism Income Happiness R2 in Multiple Regression • Best case: Orthogonal predictors • Goal: shade as much of the blue circle as possible – Predictors explain unique variability 20 Optimism Income Happiness Symbology Old & New 21 r rs or ρ rpb ϕ β = standardized slope coefficient zx, zy 𝑏 = unstandardized slope coefficient Xi, Yi r2 = variability explained by predictor variable R2 = variability explained by regression model R2 Formula: Multiple Regression 22 𝑅2 = 𝑟$&, * + 𝑟$&* * − 2(𝑟$&,×𝑟$&*×𝑟&,&*) 1\t − 𝑟&,&* * Coefficient of Determination: Y ~X1 r2 = Y ~X2 Residual variability of X1 ~ X2 Get rid of parts we double- counted Meehl’s 6th Law of Psychology “Damnit; everything correlates with everything else!” 23 R2adj. • If everything correlates with everything else… – Then, every predictor correlates w/criterion to some extent – Some of these correlations will be spurious L • “Crud” is a meaningless correlation • Results from capitalization on chance • Adding predictors always increases R2, even crud predictors – R2 adj. is a penalized R2 • Each new predictor could be crud, so let’s be cautious 24 “Everything correlates with everything else” 25 Another approach to Meehl’s law: **Try** to trash all crud predictors Categorical Predictors in Regression: Sleep, Weather, and Grumpiness 27 Sleep, Weather & Grumpiness 𝑌.= b1X1 + b2X2 + a • Criterion Y: How grumpy is Dr. Dan? • Predictor X1: Hours of sleep • Predictor X2: RainL1 or ShineL2 – b1 = -8.715 – b2 = -1.455 – a = 124.442 • If rain level of X2 is coded as ‘1’ and shine as ‘2’, then good weather predicts 1.455 fewer grumpy units 2829 40 60 80 100 -2 -1 0 12 dan.sleepdan.grumpRain_or_Shine 1 2 Figure. Regression lines predicting grumpiness by continuous predictor sleep (standardized) and by categorical predictor (rain vs. shine). Level 1: Rain Level 2: Shine Coding Categorical Predictors Y’ = -8.715*X1 + -1.455*X2 + 124.442 • Criterion Y: How grumpy is Dr. Dan? • Predictor X1: Hours of sleep • Predictor X2: RainL1 or ShineL2 – b1 = -8.715 – b2 = -1.455 – a = 124.442 • What if Dan sleeps 5 hours and it’s raining? 30 Coding Categorical Predictors Y’ = b1X1 + b2X2 + a • Criterion Y: How grumpy is Dr. Dan? • Predictor X1: Hours of sleep • Predictor X2: RainL1, CloudsL2, or ShineL3 – b1 = -8.968 – b2 = ??? • Estimate 1 and Estimate 2??? – a = 126.178 31 Coding Categorical Predictors • Criterion Y: How grumpy is Dr. Dan? • Predictor X1: Hours of sleep • Predictor X2: RainL1, CloudsL0, or ShineL0 – Dummy variable (for Rain) – b1 = -8.968 – b2 = 0.350 – a = 125.908 • What is the effect of rain on grumpiness? 32 Coding Categorical Predictors • Coding of categorical predictors changes interpretation of b • Dummy coding is most common and simplest method • Many other coding methods exist depending on hypotheses 33","libVersion":"0.3.2","langs":""}