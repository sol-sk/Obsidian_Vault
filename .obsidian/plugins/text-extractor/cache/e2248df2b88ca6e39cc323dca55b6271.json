{"path":"W2023T1/W2023T1 Files/Travers NHST (1).pdf","text":"Learning Disabilities Research & Practice, 32(4), 208–215 C⃝ 2017 The Division for Learning Disabilities of the Council for Exceptional Children DOI: 10.1111/ldrp.12147 Null Hypothesis Signiﬁcance Testing and p Values Jason C. Travers University of Kansas Bryan G. Cook and Lysandra Cook University of Hawaii p values are commonly reported in quantitative research, but are often misunderstood and misinterpreted by research consumers. Our aim in this article is to provide special educators with guidance for appropriately interpreting p values, with the broader goal of improving research consumers’ understanding and interpretation of research ﬁndings. Speciﬁcally, we discuss null hypothesis signiﬁcance testing, describe what p values mean and how they are reported, describe some common misconceptions of p values, and provide two examples from the research literature to illustrate how p values are used in the ﬁeld. Our take-home message is that p values indicate how likely study results are to occur if the null hypothesis is true, and that p values should be cautiously interpreted. Ms. Freiheit is a literacy specialist who receives two journals each month as a membership beneﬁt in a professional asso- ciation. One journal primarily publishes research, which she ﬁnds interesting but sometimes has difﬁculty understanding. She read a study of an oral reading ﬂuency intervention that repeatedly referred to results as statistically signiﬁcant when p < .05. To better understand the meaning of the results, Ms. Freiheit turned to a group of special educators on an online forum. She asked a question about the meaning of statisti- cal signiﬁcance and p values. Answers were abundant but contradictory, which confused Ms. Freiheit and resulted in a debate on the online forum. Some remarked that smaller p values meant that the study was higher quality and proved that the intervention worked. Others suggested that she pay attention to effect sizes, and that p values didn’t matter very much. A back-and-forth conversation between two teachers became adversarial and personal. Exasperated and unsure what to believe, Ms. Freiheit turned off notiﬁcations to her post and later deleted the question. An evidence-based approach to special education depends largely on the research knowledge of professionals who teach and serve students with disabilities. Savvy professionals turn to the research literature for practical guidance, but critically evaluating a study and interpreting its ﬁndings is a daunt- ing task. One metric commonly relied on when interpreting research ﬁndings is the p statistic or p value. When con- sumers of research see a p value reported (e.g., p < .05), they may know it indicates something important about the signiﬁcance of study ﬁndings, but often aren’t sure exactly what it means. Educators often infer that a low p value in- dicates the study was successful (e.g., the intervention ex- amined was effective), and/or the ﬁndings are important. Requests for reprints should be sent to Jason C. Travers, University of Kansas. Electronic inquiries should be sent to jason.travers@ku.edu. But these conclusions are not technically accurate. Indeed, despite ubiquitous reporting of p values in quantitative anal- ysis, both researchers and research consumers commonly misunderstand the p value (Goodman, 2008; Hubbard & Lyndsay, 2008; Wasserstein & Lazar, 2016). Misunderstand- ings about the p value are commonplace and can mislead professionals to wrongly believe that statistically signiﬁcant ﬁndings prove that an intervention was effective. The p value, hypothesis testing, and statistical modeling are technical concepts and, in some cases, can be exception- ally confusing. In this article, we seek to clarify what p values do and do not mean, and provide guidelines for appropriately interpreting p values in educational research. Our aim is to provide guidance to special educators seeking to evaluate research results. We provide some preliminary and general discussions of p values, which are admittedly incomplete, with the goal of clarifying this abstract and sometimes con- tentious statistic to special educators who are not experts in statistical analysis. Throughout this article, we discuss p values and related concepts by referencing group research. Although some single-case design researchers have begun to incorporate sta- tistical analyses into the methodology and occasionally use p values to inform some conclusions about their data, consen- sus about this approach to data analysis has yet to be reached. Thus, although research consumers may observe statistical data and p values in published reports of single-case exper- iments, we do not address this topic because it is currently an unresolved and emerging area. Instead, our focus is on the p value as it relates to traditional group research, espe- cially group experimental research—something that special educators often consider when examining the effectiveness of instructional practices (Cook & Cook, 2016). Our primary message is that the p value indicates how likely study results are to occur if the null hypothesis (e.g., LEARNING DISABILITIES RESEARCH 209 Null hypotheses Definition: A proposition that study findings will not demonstrate an effect, but is subjected to a scientific experiment. Example: There will be no differences between intervention and control groups in oral reading fluency at the end of the study. Relevance: A null hypothesis is tested and either is rejected or accepted based on several study features and outcomes, including the p value. Alternative hypothesis Definition: A proposition that study findings will demonstrate an effect, which is not subjected to a scientific experiment. Example: Students with LD receiving the oral reading fluency intervention will read significantly more words per minute than those who did not. Relevance: Can be considered a viable explanation for results when the null hypothesis is rejected, but is not proven by the experiment. p value Definition: The probability the results from a study occurred when the null hypothesis is actually true. Example: The intervention group's oral reading fluency was significantly higher than the control groups at p =.01. Relevance: In an experiment, indicates the mathematical probability the data were obtained when no difference between groups actually exists. Type I error Definition: Occurs when when the null hypothesis is rejected despite being true; a false positive result. Example: A researchers concludes the reading fluency intervention produced improvements, but no differences between groups actually exists. Relevance: The p value is an estimate of the likelihood of a Type I error. Any study can lead to a false positive result, regardless of the p value. Type II error Definition: Occurs when the null hypothesis is accepted despite being false; a false negative result. Example: A researcher concludes the reading fluency intervention produced no improvements, but differences between groups actually exist. Relevance: 1-p is an estimate of the likelihood of a Type II error. Any study can result a false negative. Statistical significance Definition: A result that is mathematically unlikely to occur if the null hypothesis is true. Example: The difference in oral reading fluency between control and intervention group was statistically significant. Relevance: A significant difference between groups may be one of several reasons to reject the null hypothesis. Effect size Definition: A statistic indicating the size or magnitude of the effect, which is not affected by sample size. In group experiments, an effect size quantifies the difference between groups to show how effective an intervention was. Example: The intervention group read 30% more words per minute than the control group. Relevance: Should be considered along with p values when interpreting study results and the effects or benefits of an intervention. H0 Ha p I II Sig. ES FIGURE 1 Deﬁnitions, examples, and relevance of key concepts. [Color ﬁgure can be viewed at wileyonlinelibrary.com] an intervention will not be effective for a population of learn- ers) is true, and that p values should be cautiously interpreted. To make sense of what p values mean, one must understand several terms and concepts associated with group research (see Figure 1) as well as the role of hypotheses, and null hypotheses in particular, in scientiﬁc research. After a dis- cussion of null hypothesis signiﬁcance testing (NHST), we provide an overview of what p values mean, as well as some prominent misconceptions about this common statistic. We also review the use of p values in two recent intervention studies in special education to illustrate how special edu- cators can appropriately interpret p values when they read experimental research to further their professional develop- ment and practice. NULL HYPOTHESIS SIGNIFICANCE TESTING Hypotheses are propositions or conjectures made on the basis of incomplete information; in other words, an educated guess. By hypothesizing about what they will ﬁnd before they con- duct the study, researchers provide testable predictions that are a critical part of the scientiﬁc method. In research reports, authors often state one or more hypotheses at the end of the introduction section. As an example, assume a researcher believes, based on theory and previous research that an intervention is likely to result in improved oral reading ﬂu- ency for students with learning disabilities (LD). Therefore, before conducting an experiment to examine the efﬁcacy of the intervention, the researcher hypothesizes that the inter- vention will cause improved oral reading ﬂuency for students with LD. It is often assumed that science establishes knowledge by researchers conducting a study (or multiple researchers con- ducting multiple studies) that proves a research hypothesis (e.g., that a particular intervention improves oral reading ﬂu- ency for students with LD). However, the way hypotheses function in modern scientiﬁc research is a bit more com- plicated. Fisher (1925) and Popper (1959) argued that one cannot deﬁnitively prove a theory by positively afﬁrming it through research, but one can clearly falsify a theory by refut- ing it. Using Popper’s classic example to illustrate this point, no matter how many times a hypothesis that all swans are white is conﬁrmed through empirical research (e.g., observ- ing swans), one cannot deﬁnitively prove that all swans are white because researchers cannot observe all possible swans. 210 TRAVERS ET AL.: NULL HYPOTHESIS SIGNIFICANCE In other words, a black swan that researchers have not yet discovered may exist. In contrast, just one instance of falsi- ﬁcation (e.g., sighting a black swan) refutes the proposition. Similarly, no matter how many times an oral reading ﬂuency intervention is found to be effective for students with LD, we cannot conclude that all students with LD will beneﬁt from the intervention. This is because researchers are unable to examine the effects of the intervention for all students with LD. Because one can refute a theory by falsifying a hypoth- esis, but can never deﬁnitively prove a theory no matter how much empirical support is gathered, most quantitative sci- entiﬁc research is designed to test, and potentially falsify, a hypothesis that is contrary to the researcher’s theory. Thus, the null hypothesis is the basis of NHST and the referent of p values. In our example of a researcher who theorizes that an inter- vention will improve oral reading ﬂuency for students with LD, the null hypothesis (i.e., the hypothesis that will be nul- liﬁed if study ﬁndings support the researcher’s theory) could be stated as the intervention results in no difference in the oral reading ﬂuency for students with LD. In other words, the null hypothesis, which is often represented as H0, states what will occur if the researcher’s theory is not supported by the re- search. In intervention research, the null hypothesis typically predicts that an instructional practice will have no effect, or that there will be no differences in performance between the learners who do and do not receive the intervention. The al- ternative hypothesis rivals the null hypothesis and predicts what results will occur if the researcher’s theory is valid. In our example, the alternative hypothesis could be stated as the intervention improves oral reading ﬂuency for students with LD. Though the alternative hypothesis, typically represented as H1 or Ha, corresponds with the researcher’s theory-based prediction, it is not actually being tested during an exper- iment. Rather, researchers test the degree to which study results are consistent with the null hypothesis. If the null hy- pothesis is rejected, then the alternative hypothesis may be indirectly supported. Each time a similar study refutes the null hypothesis, support for the alternative hypothesis grows incrementally stronger, even though the alternative hypothe- sis is never completely proven. NHST is a conservative approach to accumulating scien- tiﬁc knowledge. The assumption is that the null hypothesis is true until it is sufﬁciently refuted. This approach stands in contrast to pseudoscience, in which a theory or practice is assumed valid until proven otherwise (Travers, 2017). This approach is analogous to the U.S. criminal justice system, in which defendants are assumed innocent until proven guilty beyond a reasonable doubt. In a sense, the null hypothesis in criminal cases is that the defendant is not guilty. Just as criminal courts are designed to err on the side of not falsely convicting innocent people, NHST errs on the side of not identifying practices as effective until convincing evidence has refuted the null hypothesis. Identifying the null and alternative hypotheses in research articles often is challenging. Although researchers sometimes state the null and alternative hypotheses, they more typically just state the alternative hypothesis, or only list one or more research questions without specifying hypotheses. Although it would be helpful for researchers to formally state their null and alternative hypotheses, hypotheses can typically be inferred without much difﬁculty. For example, the author of our example study might have just stated the following research question: Does the intervention result in improved oral reading ﬂuency for students with LD? Although unstated, one can infer the null hypothesis is that the intervention will not improve oral reading ﬂuency, and the alternative hypothesis is that it will. After research questions are posed and hypotheses have been formulated, the researcher designs a study to test the null hypothesis. In group experimental studies, groups of participants are selected and assigned to control and inter- vention conditions. Assessments typically are conducted to determine prestudy performance, and to ensure the groups are not different in ways that might affect the results. The intervention and control phases are instituted and, after the intervention is completed, researchers again administer as- sessments to both groups of learners. At this point, the exper- iment is completed, but many steps of the study remain. The researchers must now analyze their data to determine whether sufﬁcient evidence allows rejection of the null hypothesis. One piece of evidence that helps to determine whether to reject the null hypothesis is the p statistic. WHAT THE p VALUE MEANS Because no study is perfect and all studies involve error, researchers may design experiments that detect no true ef- fects, but wrongly conclude the intervention was effective (i.e., a false positive ﬁnding; Type I error) on the basis of study ﬁndings. Conversely, researchers may develop an ef- fective intervention, but then conduct a study that fails to detect the effects and wrongly conclude the intervention was not effective (i.e., a false negative ﬁnding; Type II error). The p value can help researchers draw reasonable conclu- sions about a study’s results by indicating how likely their ﬁndings occurred when the intervention truly had no effect. In other words, the p value (e.g., p = .03) indicates the probability of making a Type I error by rejecting the null hypothesis. p values are reported as decimals that often, but not neces- sarily, are rounded to two decimal places and are interpreted as probability percentages. For example, p = .03 means that there is a 3 percent probability that the study results occurred as they did if the null hypothesis is true. Larger p values (e.g., p = .60) indicate a strong probability that results oc- curred as they did with a true null hypothesis (i.e., ﬁndings are relatively consistent with the null hypothesis), and lower p values (e.g., p = .01) indicate a small probability that re- sults occurred as they did with a true null hypothesis (i.e., ﬁndings are relatively inconsistent with the null hypothesis). p values range between 0 and 1. Because there is never a 0 percent probability that results occurred as they did with the null hypothesis being true, a p value is never zero. Similarly, because results never absolutely prove the null (or any other) hypothesis, a p value is never 1. Recall our scenario of an experimental study being con- ducted to examine the effect of an intervention on oral read- ing ﬂuency. Assume the oral reading ﬂuency of participants LEARNING DISABILITIES RESEARCH 211 in the experimental group (who received the intervention) improved quite a bit more over the course of the study than for participants in the control group (who did not receive the intervention). Even though these ﬁndings appear inconsistent with the null hypothesis, there is still some possibility that differences between the groups occurred in the study even though the null hypothesis is true. For example, it is possible that more “ready-to-learn” emergent readers, who were just starting to make large gains in reading performance when the study started, were randomly assigned to the experimen- tal group. In this scenario, although the intervention group demonstrated a large improvement in oral reading ﬂuency, their gain had little or nothing to do with the intervention. Therefore, studies showing the experimental group improved more than the control group may reﬂect sampling problems and not the effects of the intervention. In this case, the null hypothesis (i.e., the intervention results in no difference in oral reading ﬂuency for students with LD) is true for the broader population, even though study ﬁndings showed the experimental group outperformed the control group (i.e., a Type I, false positive error). Because some error is present in every study, and especially in studies conducted in applied settings like schools and classrooms, there is always some possibility the null hypothesis is true, no matter what the study ﬁndings are. Three factors impact the p value: effect size (i.e., magni- tude of effect), variability, and sample size. Using our exam- ple of a reading intervention study, the p value is inﬂuenced by (a) the magnitude of the difference between groups on the outcome measure (oral reading ﬂuency), (b) the variability in performance on the outcome measure within each group, and (c) the number of participants in the study. All else be- ing equal, stronger evidence against the null hypothesis (and alower p value) is associated with the experimental group outperforming the control group by a lot rather than a lit- tle. Lower variability within groups also is associated with lower p values. For example, a difference between groups of, say, seven words per minute is inconsistent with the null hy- pothesis if all students in the experimental group improved between 12 and 14 words per minute (M = 13) whereas all participants in the experimental group improved between 5 and 7 words per minute (M = 6). However, those same mean values are less inconsistent with the null hypothesis if student performance within groups is highly variable, with some students in the experimental group making no progress at all and some students in the control group making huge gains. Finally, all else being equal, studies with large num- bers of participants have lower p values. The likelihood that differences between groups are due to sampling error (and not due to the null hypothesis being false) decreases with large samples. For example, the performance of a few par- ticipants with idiosyncratic learning responses (i.e., outliers) in a treatment condition will have little impact on the mean performance of a group in a study with 200 participants per group. Thus, for group intervention studies, small p values are most likely to occur in studies with large differences between groups, small variability within groups, and large samples. How small should a p value be to conclude that the null hypothesis should be rejected? Fisher (1925) noted that p = .05 (i.e., a 5 percent likelihood that results occurred as they did if the null hypothesis is true) is a convenient criterion for denoting interesting, or signiﬁcant, ﬁndings that warrant rejection of the null hypothesis—a ﬁgure that remains the common standard. Although p = .05 means the null hypoth- esis will be falsely rejected (i.e., Type I error) in one of every twenty analyses (5 percent), using a lower p value as the criterion for statistical signiﬁcance would increase the likelihood of accepting the null hypothesis when it should be rejected (i.e., Type II error). Many researchers and re- search consumers have become accustomed to p < .05 sig- nifying that results are statistically signiﬁcant, but there is nothing inherently important about that p value; it is an ar- bitrary criterion that has become ingrained over many years. Reﬂecting the commonly accepted approach of using a p level of .05 as the threshold for statistical signiﬁcance, some researchers simply report whether p values are <.05 (and statistically signiﬁcant) or >.05 (and not statistically signif- icant), rather than reporting the speciﬁc p value (e.g., p = .034). One might also see researchers reporting p <.01 or p <.001 to indicate very low p values. We agree with Rosnow and Rosenthal’s (1989) sentiment about p values that “surely, God loves the .06 nearly as much as the .05” (p. 1277). Indeed, p values should be thought of as a contin- uum of probabilities, not as a strict dichotomy of statistical signiﬁcance. Additional internet resources that special ed- ucation professionals may ﬁnd helpful for understanding p values and NHST can be found in Figure 2. WHAT THE p VALUE CANNOT TELL YOU Unfortunately, inaccurate beliefs about what the p value means have proliferated for generations, and many re- searchers and research consumers subscribe to one or more erroneous beliefs about this ubiquitous statistic. In this sec- tion, we present several common misconceptions related to p values. This is not intended to be an exhaustive dis- cussion of the issues and problems related to p values. We refer interested readers to several detailed overviews for additional information (e.g., Goodman, 1993, 2008; Greenland et al., 2016; Hubbard & Lyndsay, 2008; Sullivan & Fein, 2012). ALow p Value Indicates Practical Signiﬁcance This is arguably the most prominent misconception, and illus- trates well the difﬁculty associated with interpreting p values. Importantly, when applied to group experiments the p value can inform research consumers about the degree to which differences between groups are statistically interesting, but it cannot inform conclusions about the practical signiﬁcance of a difference. Practical signiﬁcance, sometimes referred to as the “clinical signiﬁcance” or “social validity” of results, refers to whether a difference between groups has any prac- tical or real-world value. This is important because sample size directly affects p values, and large samples can cause relatively small (i.e., practically useless) differences between groups to be statistically signiﬁcant. Thus, a low p value 212 TRAVERS ET AL.: NULL HYPOTHESIS SIGNIFICANCE Web pages and articles Five guidelines for using p values: http://blog.minitab.com/blog/adventures-in-statistics-2/five- guidelines-for-using-p-values How to correctly interpret p values: http://blog.minitab.com/blog/adventures-in-statistics-2/how-to- correctly-interpret-p-values Not even scientists can easily explain p values: http://fivethirtyeight.com/features/not-even-scientists- can-easily-explain-p-values/ What a p value tells you about statistical data: http://www.dummies.com/education/math/statistics/what - a-p-value-tells-you-about-statistical-data/ Videos Hypothesis testing and p values by Khan Academy: https://youtu.be/-FtlH4svqx4 Is most published research wrong? https://www.youtube.com/watch?v=42QuXLucH3Q&t=26s Understanding p values – Statistics help: https://www.youtube.com/watch?v=eyknGvncKLw What is a null hypothesis (and alternate hypothesis): https://www.youtube.com/watch?v=tDmCFVQvv2A What makes science true?: https://www.youtube.com/watch?v=NGFO0kdbZmk Podcasts One humble test that makes or breaks companies and careers: https://www.statnews.com/2017/03/31/pvalue-statistics-podcast-science p values: https://itunes.apple.com/us/podcast/mini-p-values/id890348705?i=1000314963010&mt=2 FIGURE 2 Videos, podcasts, and webpages for learning more about p values and null hypothesis statistical testing. [Color ﬁgure can be viewed at wileyonlinelibrary.com] cannot inform conclusions about the real-world importance of the difference between groups. For example, our oral reading ﬂuency study might have 500 participants in the in- tervention and control conditions—1,000 total participants. Statistical analysis of study results might reveal a difference between groups that is statistically signiﬁcant (e.g., p = .001). However, on average the intervention group only read three words per minute (103 words per minute) more than the con- trol group (100 words per minute)—an effect size of about 3 percent. Although the difference is statistically signiﬁcant, most educators would consider this outcome to have little practical signiﬁcance because the small improvement does not justify investment in the intervention. Professionals should be cautious about drawing prac- tical conclusions based solely on a low p value, espe- cially if the study included a large sample size. Although a large sample size is important for drawing inferences about the generalizability of the ﬁndings (Cook & Cook, 2017), the p value cannot inform conclusions about the practical value of the ﬁndings. Professionals should rely on effect sizes to determine the practical importance of study ﬁndings. The effect size informs decisions about “the magnitude of the difference between groups . . . (and) is the main ﬁnd- ing of a quantitative study” (Sullivan & Fein, 2012, p. 279). The effect size can be reported in several different ways de- pending on the statistical method of analysis, but often is reported along with interpretations as to whether the effect is small, moderate, or large. In most cases, professionals will be interested in the size of the effect (i.e., the amount of change associated with an intervention) rather than the mere presence of a statistically signiﬁcant effect (i.e., the p value). ALow p Value Means it is Correct to Reject the Null Hypothesis Goodman (1993) explained why the p statistic alone is in- sufﬁcient reason to reject the null hypothesis. Researchers and research consumers should examine other features of the study—including participant selection, assignment of par- ticipants to treatment and control conditions, adherence to intervention procedures, and attrition of participants from the study—to justify rejection of the null hypothesis. Careful examination of the data and statistical methods used also is necessary to ensure analyses were conducted in ways that do not produce erroneous results (Nuzzo, 2015). Finally, other explanations that might reasonably account for observed dif- ferences between the groups should be examined. If the study was conducted appropriately and no competing explanations are found, then the collective evidence from the study, includ- ing a low p value, may justify rejection of the null hypothesis. In essence, results from poorly designed or conducted stud- ies should not be used to justify rejecting the null hypothesis regardless of the p value. ALow p Value Indicates the Likelihood the Alternative Hypothesis is True Evidence that the null hypothesis is incorrect does not constitute direct evidence that the alternative hypothesis is true. Many individuals, including researchers, may be un- der the impression that obtaining a low p value and reject- ing the null hypothesis means “the intervention worked.” This assumption is not accurate. Evidence that the null LEARNING DISABILITIES RESEARCH 213 hypothesis is incorrect does not qualify the alternative hy- pothesis as true because only the null hypothesis is actually tested via experimentation. Although deductive logic may lead to conclusions that an intervention is the most plausible explanation for differences between groups, this claim (i.e., the alternative hypothesis) is not put to a direct, scientiﬁc test. Analogously, concluding insufﬁcient evidence exists to convict someone of a crime is not the same as concluding the person is innocent. The former claim is the product of insuf- ﬁcient evidence (of guilt), and the latter requires presenting evidence to establish innocence (a different claim). Similarly, rejecting the null hypothesis from our oral reading ﬂuency experiment does not mean that our intervention caused the treatment group to perform better than the control group. Many alternative explanations could be the cause for the ob- served differences between groups (e.g., sampling error). We must look to the p value and other factors (e.g., whether the intervention procedures were applied as designed, or with ﬁdelity) to decide whether the alternative hypothesis can be considered a viable explanation for our results. Moreover, results from a single study, or from a few studies, should not be considered reason to believe an intervention will have the intended effect for the broader population regardless of the p value. Instead, initial rejection of the null hypothesis sig- nals the need for additional research. If a sufﬁcient number of high-quality replication studies generally ﬁnd the same results, then conﬁdence about the positive effects of the in- tervention is warranted (Travers, Cook, Therrien, & Coyne, 2016). ALow p Value Indicates the Likelihood Results Occurred by Chance As explained previously, the p value represents the likelihood results would have occurred if the null hypothesis was true. A common but mistaken belief is that the p value informs researchers how likely it is that their results were caused by chance alone. In other words, a p valueof.05 maybe wrongly perceived as evidence that results from an experi- ment have only a 5 percent probability of being the product of coincidence (and are 95 percent likely to be caused by the in- tervention). But the p value does not show the researcher why the results were statistically signiﬁcant and, consequently, the p value cannot be used to conclude whether the results oc- curred due to coincidence or any other reason. Thus, the p value can tell us the degree to which results are inconsistent with the null hypothesis and worthy of further consideration, but cannot tell us why the results were obtained, or how likely it is that results occurred by coincidence. Inaccurate interpre- tations like these can lead to overconﬁdence about the results of a single study or body of research. ALow p Value Indicates the Likelihood Results are Generalizable Individuals may presume that a low p value indicates the re- sults are generalizable, or are more likely to apply to other individuals in the population (e.g., all students with LD), and that studies with smaller p values are more generalizable than studies with larger p values. These presumptions are not accurate. Generalizability of results is dependent on several factors, with sample representativeness being the primary concern (Cook & Cook, 2017; Shadish, Cook, & Camp- bell, 2002). Recall the p value tells us how likely the results obtained in a study would have occurred if the null hypoth- esis is true. This statistic has no relation to the procedures for choosing participants, assigning participants to a condi- tion, or whether important characteristics of the participants are consistent with the broader population. Thus, the p value cannot be relied on to inform decisions about whether study ﬁndings (e.g., an intervention causing improved oral reading ﬂuency) can be expected to produce similar results at the population or individual levels. EXAMPLES FROM THE LITERATURE To illustrate how p statistics are used and what they mean, we brieﬂy review two of the many recent studies in the ﬁeld of special education that reported p values. Training Preservice Teachers Sayeski et al. (2015) examined the effects of multimedia training modules on preservice teacher (i.e., college student) knowledge and skills related to phonology and phonics. At the end of the introduction section of their report, the authors posed multiple research questions, including “To what ex- tent can a series of interactive, multimedia modules support participants’ development of basic language knowledge and related literacy skills?” (p. 242). Although the authors did not state a null or alternative hypothesis, the research question can be used to derive hypotheses. For example, we can infer that the null hypothesis would be The interactive multimedia modules will have no effect on basic language knowledge and related literacy skills at the end of the study. Similarly, the alternative hypothesis would be The interactive multimedia modules will have an effect on basic language knowledge and related literacy skills at the end of the study. The 76 study participants were randomly assigned to treatment and control conditions. The authors examined the groups before the study to ensure they were not different in ways that might explain differences at post-test (i.e., both groups had participants with similar literacy courses, back- ground experiences, levels of interest in the course, and levels of motivation). The authors concluded “given the similarities across groups in terms of both participant characteristics and pre-intervention questionnaire items, it was not anticipated that differences in groups would account for differences in study outcomes” (Sayeski et al., 2015, p. 243). To evaluate the effects of the intervention, the researchers administered the Survey of Basic Language Constructs (Binks-Cantrell, Joshi, & Washburn, 2012), which evaluated phonological and de- coding knowledge of the participants prior to and following the study. The researchers found differences between the interven- tion and control groups at the end of the study. Speciﬁcally, 214 TRAVERS ET AL.: NULL HYPOTHESIS SIGNIFICANCE they found the intervention group (n = 38) had an average score of 24.47 after the study, compared to the control group’s (n = 38) average score of 19.74. This difference was statisti- cally signiﬁcant (p < .001). In other words, there is less than a 0.1 percent probability these results occurred with the null hypothesis being true. Consequently, the authors concluded the results were statistically signiﬁcant. Because statistical signiﬁcance does not indicate practical signiﬁcance, the au- thors also calculated and reported an effect size of d = 0.91, which they described as a large effect. Because of the low p value and the ruling out of possible explanations for the differences between groups (e.g., groups were similar, no- body withdrew from the study), the study provides initial support for the alternative hypothesis. The authors appropri- ately remarked that multimedia modules have the potential for improving literacy knowledge and skills of preservice professionals, and emphasized the importance of conducting additional research to clarify how multimedia modules can be useful. A Multicomponent Reading Comprehension Intervention Solis, Vaughn, and Scammacca (2015) conducted a ran- domized controlled trial to investigate the effects of a multicomponent intervention—involving vocabulary in- struction, text-based instruction, grammar print structures, complex language structures, inference reading drills, and curriculum-based measurement—on the reading comprehen- sion of at-risk ninth graders. Like Sayeski et al. (2015), So- lis et al. posed research questions but did not specify null or alternative hypotheses. The ﬁrst research question posed was, “When differences in verbal ability are controlled for, to what extent does a multicomponent reading intervention with adolescent students who are low in reading comprehen- sion impact reading comprehension outcomes?” (p. 105). We can infer that the null hypothesis would be The multicompo- nent reading intervention does not impact reading compre- hension outcomes for adolescent students with low reading comprehension, whereas the alternative hypothesis would be The multicomponent reading intervention positively impacts reading comprehension outcomes for adolescent students low in reading comprehension. The study involved 44 ninth-grade students who were previously enrolled in a reading intervention class due to reading comprehension difﬁculties. Students were randomly assigned to either a treatment or business-as-usual compari- son group. The intervention was delivered across the school year as an elective class that met for 90 minutes, two to three times a week. The researchers assessed student performance on three different tests of reading comprehension before and after delivering the intervention. For the sake of brevity, we focus on results related to the Woodcock-Johnson-III Passage Comprehension Subtest (WJIII-PC; Woodcock, McGrew, & Mather, 2007; results were relatively consistent across all three tests). After controlling for verbal ability, as measured by a standardized test, scores on the WJIII-PC rose from 77.4 at pretest to 81.8 at post-test for the treatment group, but fell very slightly (from 77.6 to 76.7) for the comparison group. Although the authors interpreted the effect size (η2 = 0.03) as medium, the difference was not statistically signif- icant (p = .26). In other words, even though the treatment group outperformed the comparison group, there is a 26 per- cent probability that the null hypothesis is true given these results. Accordingly, the researchers did not reject the null hy- pothesis, and the alternative hypothesis was not supported. The authors discussed potential reasons why the intervention did not yield a statistically signiﬁcant difference between the groups, such as the possibility that remediation of reading comprehension for secondary students with low verbal abil- ity may require a more intensive intervention over longer periods of time (e.g., multiple years). Given the medium ef- fect size and the relatively small number of participants, the authors noted that future studies involving larger samples of participants might detect statistically signiﬁcant effects for the intervention. TAKE-HOME MESSAGE AND CONCLUSION Professionals who intend to remain current with professional practice and deliver evidence-based interventions often turn to peer-reviewed research in reputable journals for guid- ance. The p statistic associated with NHST is a ubiquitous though commonly misunderstood aspect of quantitative re- search that can perplex and mislead education professionals and researchers alike when reading and interpreting research studies. Our take-home message is that the p value only tells us how likely it is that the results from a study occurred if the null hypothesis is true. Given its narrow utility and the widespread misunderstandings about p values, profes- sionals should carefully consider many aspects of the study (e.g., effect size, sample size, representativeness of sample, plausibility of alternative explanations for ﬁndings) in con- junction with the p value when interpreting and applying study ﬁndings. Importantly, the p value signals whether a result can be considered interesting and worthy of further investigation, but cannot—in isolation—tell us whether the null hypothesis should be rejected, ﬁndings are practically valuable, the alternative hypothesis is true, or results are generalizable. We have provided a brief overview of the p statistic by de- scribing its meaning in educational research, and have also ar- ticulated some common misconceptions to which researchers and professionals alike may subscribe. The tendency to misat- tribute meaning to the p value suggests that special educators should be very cautious about ascribing value to research results based solely on the p value. Determining the valid- ity and practical value of a study depends on understanding not just whether a low p value was obtained, but whether the effect size was practically signiﬁcant, the participants represent a population of concern, other studies have ob- tained similar results, and the intervention likely caused the observed differences. If professionals cautiously approach research ﬁndings, understand the experimental method used, and critically evaluate the results—including but not limited to the p value—they may better interpret and apply research LEARNING DISABILITIES RESEARCH 215 ﬁndings in order to improve the learning outcomes of excep- tional students. REFERENCES Binks-Cantrell, E., Joshi, R. M., & Washburn, E. K. (2012). Valida- tion of an instrument for assessing teacher knowledge of basic language constructs of literacy. Annals of Dyslexia, 62, 153–171. https://doi.org/10.1007/s11881-012-0070-8. Cook, B. G., & Cook, L. (2016). Research designs and spe- cial education research: Different designs address different ques- tions. Learning Disabilities Research & Practice, 31, 190–198. https://doi.org/10.1111/ldrp.12110. Cook, B. G., & Cook, L. (2017). Sampling and special education re- search: Examining whether and how study results apply to you. Learning Disabilities Research & Practice, 32, 78–84. https://doi.org/ 10.1111/ldrp.12132. Fisher, R. A. (1925). Theory of statistical estimation. Proceedings of the Cambridge Philosophical Society, 22, 699–725. Goodman, S. N. (1993). P values, hypothesis tests, and likelihood: Impli- cations for epidemiology of a neglected historical debate. American Journal of Epidemiology, 137, 485–496. Goodman, S. (2008, July). A dirty dozen: Twelve p-value miscon- ceptions. Seminars in Hematology, 45, 135–140. https://doi.org/ 10.1053/j.seminhematol.2008.04.003. Greenland, S., Senn, S. J., Rothman, K. J., Carlin, J. B., Poole, C., Goodman, S. N., et al. (2016). Statistical tests, P. European Journal of Epidemi- ology, 31, 337–350. https://doi.org/10.1007/s10654-016-0149-3. Hubbard, R., & Lindsay, R. M. (2008). Why P values are not a useful measure of evidence in statistical signiﬁcance testing. Theory & Psychology, 18, 69–88. https://doi.org/10.1177/0959354307086923. Nuzz, R. L. (2015). The inverse fallacy and interpreting p-values. PMR: The Journal of Injury, Function, and Rehabilitation, 7, 311–314. Popper, K. (1959). The logic of scientiﬁc discovery. London: Hutchins and Company. Rosnow, R. L., & Rosenthal, R. (1989). Statistical procedures and the justiﬁ- cation of knowledge in psychological science. American Psychologist, 44, 1276–1284. https://doi.org/10.1037/0003-066X.44.10.1276. Sayeski, K. L., Kennedy, M. J., de Irala, S., Clinton, E., Hamel, M., & Thomas, K. (2015). The efﬁcacy of multimedia modules for teaching basic literacy-related concepts. Exceptionality, 23, 237–257. https://doi.org/10.1080/09362835.2015.1064414. Shadish, W.R., Cook, T. D., & Campbell, D. T. (2002). Experimental and quasi-experimental designs for generalized causal inference. Boston: Houghton Mifﬂin. Solis, M., Vaughn, S., & Scammacca, N. (2015). The effects of an in- tensive reading intervention for ninth graders with very low read- ing comprehension. Learning Disabilities Research & Practice, 30, 104–113. Sullivan, G. M., & Fein, R. (2012). Using effect size - or why the P value is not enough. Journal of Graduate Medical Education, 4, 279–282. Travers, J. C. (2017). Evaluating claims to avoid pseudoscientiﬁc and un- proven practices in special education. Intervention in School and Clinic, 52, 195–203. https://doi.org/10.1177/1053451216659466. Travers, J. C., Cook, B., Therrien, W. J., & Coyne, M. (2016). Replication research and special education. Remedial and Special Education, 37, 195–204. https://doi.org/10.1177/0741932516648462. Wasserstein, R.L., & Lazar, N.A. (2016). The ASA’s statement on p-values: Context, process, and purpose. The American Statistician, 70, 129–133. https://doi.org/10.1080/00031305.2016.1154108. Woodcock, R. W., McGrew, K., & Mather, N. (2007). Woodcock-Johnson III Tests of Achievement. Itasca, IL: Riverside. About the Authors Jason C. Travers is an associate professor and behavior analyst in the Department of Special Education at the University of Kansas where he coordinates the autism programs. He earned his doctorate at the University of Nevada Las Vegas and is a former public school special educator for learners with autism. He is interested in evidence-based practices, sexuality education, and technology-based interventions and supports for learners with autism. Bryan G. Cook Professor of Special Education at the University of Hawaii, earned his PhD from the University of California at Santa Barbara. He is Past President of CEC’s Division for Research, chairs the Research Committee for CEC’s Division for Learning Disabilities, and co-edits Behavioral Disorders (the research journal of CEC’s Council for Children with Behavioral Disorders). He is interested in evidence-based practices, bridging the research-to-practice gap, and examining the special education research base. Lysandra Cook Associate Professor of Special Education at the University of Hawaii, earned her PhD from Kent State University. She is the coordinator for Project Laulima, a federal grant supporting the formation of a fully merged, co-taught teacher preparation program in elementary general and special education. Her scholarly interests include teacher preparation and evidence-based practices.","libVersion":"0.3.2","langs":""}