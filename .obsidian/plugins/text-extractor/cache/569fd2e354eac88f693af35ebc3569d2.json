{"path":"W2023T1/W2023T1 Files/oconnor and weatherall the social network.pdf","text":"Yale University Press Chapter Title: The Social Network Book Title: The Misinformation Age Book Subtitle: How False Beliefs Spread Book Author(s): Cailin O’Connor and James Owen Weatherall Published by: Yale University Press. (2019) Stable URL: https://www.jstor.org/stable/j.ctv8jp0hk.8 JSTOR is a not-for-profit service that helps scholars, researchers, and students discover, use, and build upon a wide range of content in a trusted digital archive. We use information technology and tools to increase productivity and facilitate new forms of scholarship. For more information about JSTOR, please contact support@jstor.org. Your use of the JSTOR archive indicates your acceptance of the Terms & Conditions of Use, available at https://about.jstor.org/terms Yale University Press is collaborating with JSTOR to digitize, preserve and extend access to The Misinformation Age This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms 147 On Sunday, December 4, 2016, a twenty-eight-year-old man named Edgar Maddison Welch told his wife and two daughters that he had some business to attend to and left the house. 1 He got into his car and drove about six hours from his home in Salisbury, North Car- olina, to a pizzeria in Washington, D.C. He carried with him an AR-15 semiautomatic rifle, a handgun, and a shotgun, all loaded. When he arrived at the restaurant at about 3 p.m., he entered car- rying the AR-15 and opened fire, unloading multiple rounds into a locked door inside the establishment. Welch thought he was a hero. He believed that the pizzeria, known as Comet Ping Pong, was the staging ground for an inter- national child prostitution ring headed by none other than Hillary Clinton, the former Democratic nominee for president. Welch was there to investigate the pizza parlor—and if possible, save the children. A little over a month earlier, shortly before the 2016 election, FBI director James Comey had announced that he was reopening FOUR The Social Network This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 148 an investigation into Hillary Clinton’s use of a private email server while secretary of state. (Recall that it was the fact that Comey had previously closed this investigation, without recommending prose- cution, that, according to ETF News, had prompted Pope Francis to endorse Donald Trump for president.) The reason for Comey’s announcement was the presence of an unknown number of pos- sibly new emails on a computer that had been confiscated from the  home of a top Clinton aide. The circumstances were sordid: the computer belonged to disgraced former congressman Anthony Weiner, who had recently been accused of sending nude pictures of himself to a fifteen-year-old girl. (Weiner eventually pleaded guilty to transferring obscene material to a minor.) Weiner’s wife, Huma Abedin, was a top Clinton aide. Two days later, a post on the social media site Twitter alleged that the emails in fact revealed something far worse than the origi- nal allegation that classified material had passed through Clinton’s server. Citing “NYPD [New York Police Department] sources,” the tweet claimed that the emails implicated Hillary Clinton as the kingpin of “an international child enslavement and sex ring.” This tweet was shared more than six thousand times during the next week.2 The following day, an article appeared on the website YourNewsWire.com claiming that an “FBI insider” had confirmed allegations of a pedophile sex ring linked to many people in the US government, including several sitting members of Congress and, of course, Hillary Clinton. The story, quickly shared on other blogs and news aggregators, became the topic of multiple new fake news stories. Some of these were verbatim reproductions of the YourNews Wire.com article, while others included new text, new allegations, and new claims of inside sources. One website, called Subject: Pol- itics, claimed that the NYPD had “raided” a Clinton property and found further damaging material. (No such raid occurred.) On the heels of these articles, online sleuths began investigating This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 149 other publicly available materials—including a slew of emails that were apparently stolen from Clinton campaign chair (and former Bill Clinton chief of staff) John Podesta and released on the website of the organization WikiLeaks. 3 These amateur investigators soon came to believe that they had detected a code hidden in Podesta’s lunch orders. Starting from “cheese pizza,” which has the same ini- tials as “child prostitution,” they created a translation manual: “hot- dog” meant “boy,” “pasta” meant “little boy,” “sauce” meant “orgy,” and so on.4 A discussion board soon appeared on the website Reddit with the title “Pizzagate,” where these allegations were discussed and new “evidence” was brought to bear; other discussions contin- ued on websites popular with right-wing youths, such as 4chan. In a particularly baffling inference, this community somehow reasoned that since pizza-related terms were involved in their “code,” the prostitution ring must be run out of a pizzeria. (Yes, a real pizzeria—that one was not code!) Soon they identified a location: D.C.’s Comet Ping Pong, which was owned by a man with links to the Clintons (he had previously dated the CEO of pro-Clinton media watchdog Media Matters for America) and which Podesta was known to frequent. (Their investigation also determined that Podesta practiced witchcraft and drank the blood of his human victims.) These bizarre and evidence-free allegations soon spread beyond the dark underbelly of the internet to relatively mainstream right- wing media such as the Drudge Report and Infowars. Infowars is far from a reliable source of information, but it has an enormous following. Alex Jones, its creator and principal voice, has more than 2 million follows on YouTube and 730,000 followers on Twitter; by spreading the rumors, Jones vastly increased their reach. (He later apologized for his role in the whole affair.) 5 Soon the pizzeria was flooded with phone calls threatening violence, and hundreds of ob- scene restaurant “reviews” appeared online, echoing the allegations. This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 150 By the end of November, the story had gotten so much attention that mainstream news sources, including the New York Times and the Washington Post, ran articles debunking the claims—which, pre- dictably, only led to more attention for the conspiracy theory, along with articles and videos attempting to refute the debunkers. 6 Welch became aware of the theories around this time. He had first been told the stories by friends in North Carolina; but in the days before he drove to D.C., he had had an internet service in- stalled in his house—at which point he was able to read about the Pizzagate allegations for himself. What he found was deeply con- cerning to him. He got the “impression something nefarious was happening”—and that no one else was doing anything about it.7 He decided to take matters into his own hands. Our focus in the previous three chapters has been on science. As we explain in the Introduction, the reason for this is that it is relatively clear that in scientific communities some or all of the actors in- volved are trying to learn about the world in what they take to be the most reliable and effective ways possible. They want to discover the truth, in the sense discussed in Chapter 1. They want to dis- cover what events actually happened and their physical causes. The members of these communities can certainly be wrong, and often are. But there is no doubt that they are in the business of gathering and evaluating evidence. This makes it particularly clear how our models apply—and all the more significant that there are so many ways for these communities to end up mistaken. But as we also argued in the Introduction, science can be thought of as an extreme case of something we are all trying to do in our daily lives. Most of us are not trained as scientists, and even fewer have jobs in which they are paid to do research. But we are often trying to figure stuff out about the world—and to do this, we use the This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 151 same basic kinds of reasoning that scientists do. We learn from our experience—and, crucially, we learn from the experiences of others. Inasmuch as all of us are learning from the world and from one another, our models of communities of scientists apply just as well to groups of ordinary people. The mechanisms we have identified for the spread of specific beliefs operate in the population at large in similar ways. And understanding these mechanisms and how they can be subverted to other people’s ends tells us a great deal about the political situation today in the United States, the United King- dom, and much of Western Europe. The American public, for example, is deeply divided on many is- sues. They include some of the topics discussed in this book, such as global climate change. But Americans are also polarized on whether the Affordable Care Act—aka Obamacare—should be the basis for future healthcare policy or repealed and replaced by a completely different policy; whether the multilateral treaty through which Iran has agreed to give up its nuclear weapons program should be aban- doned; whether free-trade agreements ultimately improve the country’s economic conditions; how tightly the government should (or legally may) regulate guns; and whether lowering corporate tax rates and tax rates on high earners will stimulate middle-class wage growth. Virtually all of these cases feature disagreements about basic matters of fact that contribute to the disagreements about policy. These disagreements themselves arise because people tend to trust different information sources: some rely on MSNBC, the New York Times, or the Washington Post, whereas others look to Fox News, the Wall Street Journal, and the Washington Times. Some point to stud- ies produced by the Heritage Foundation, the Cato Institute, or the Heartland Institute, whereas others look to the Southern Poverty Law Center, the Brookings Institution, or the Center for American Progress. Some point to less dependable sources such as Breitbart This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 152 News, Infowars, AlterNet, and the Palmer Report—or even worse, RT or Sputnik. When friends, family members, colleagues, and especially strang- ers disagree with our views, it is easy to attribute this disagreement to the failures of those people: they are ignorant of the facts, too emotional to properly evaluate the situation, or too stupid. But what if that is not what’s going on? Or at least, what if ignorance and emotion are only part of the story—and perhaps not the most im- portant part? Emotion plays no role in our models. Neither does intelligence nor political ideology. We have only very simple, highly idealized agents trying to learn about their worlds using (mostly) rational methods. And they often fail. Moreover, they can be readily ma- nipulated to fail, simply by an agent taking advantage of the same social mechanisms that, in other contexts, help them to succeed. What if these sorts of social factors lie behind the spread of “fake news” and even the bleeding of conspiracy theories into mainstream sources such as the Washington Post and Fox News? “Fake news” has a long history, particularly in the United States. In the decades immediately before and after the American Revolution, for instance, partisans on all sides attacked their opponents through vicious pamphlets that were often filled with highly questionable ac- cusations and downright lies. Likewise, fake news arguably launched the Spanish American War.8 After the USS Maine—a US warship sent to Havana in 1898 to protect American interests while Cuba revolted against Spain—mysteriously exploded in Havana Harbor, several US newspapers, most notably the New York World and New York Journal, began to run sensational articles blaming Spain for the explosion and demanding a war of revenge. 9 (The actual cause of the explosion was and remains controversial, but concrete evi- This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 153 dence has never been produced that Spain was involved.) 10 Ulti- mately, spurred in part by pressure from the news media, the US government gave Spain an ultimatum that it surrender Cuba or face war—to which the Spanish responded by declaring war on the United States. 11 (Spain sued for peace fewer than three months later.) In 1835, the New York Sun, a politically conservative but gener- ally reputable newspaper, published a series of six articles asserting that the English astronomer John Herschel had discovered life on the moon. 12 The articles claimed to have been reprinted from an Edinburgh newspaper and contained a number of alleged quotes from Herschel. They even included illustrations of winged homi- nids Herschel was said to have seen. Needless to say, there is no life on the moon—and Herschel never claimed to have found it. The articles were never retracted. (Compare these claims to ones made by a guest on Alex Jones’s Infowars radio show in June 2017 to the effect that NASA is running a child slave colony on Mars.) 13 Nine years later, Edgar Allan Poe published a story in the Sun in which he described (as factual) a trans-Atlantic hot-air balloon jour- ney by a famous balloonist named Monck Mason.14 This, too, never occurred. (The article was retracted two days later.) So fake news has been with us for a long time. And yet something has changed—gradually over the past decade, and then suddenly during the lead-up to the 2016 UK Brexit vote and US election. In 1898, when the New York World and New York Journal began agitating for war, they had large circulations. The New York Journal claimed 1.25 million readers per day, allegedly the largest in the world. 15 (New York City then had a population of about 3.4 million; the Journal figures are surely inflated, but perhaps not by much: the aggregate circulation for New York dailies in 1900, according to the U.S. Census that year, was more than 2.7 million.) But their audiences consisted almost exclusively of New Yorkers—and not even all New Yorkers, as the better-respected Times, Herald Tribune, This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 154 and Sun also had wide readerships. Regional newspapers outside New York generally did not pick up the World and Journal articles calling for war with Spain. Although the stories surely influenced public opinion and likely contributed to the march toward war, their impact was limited by Gilded Age media technology. In the past decade, these limitations have vanished. In February 2016, Facebook reported that the 1.59 billion people active on its website are, on average, connected to one another by 3.59 degrees of separation. Moreover, this number is shrinking: in 2011, it was 3.74. 16 And the distribution is skewed, so that more people are more closely connected than the average value suggests. According to the Pew Research Center, 68 percent of American adults use Facebook (out of 79 percent of American adults who use the internet at all).17 Twitter, meanwhile, has about 70 million users each month in the United States—a bit under 30 percent of American adults—and about 330 million users worldwide.18 Information posted and widely shared on Facebook and Twitter has the capacity to reach huge proportions of the voting public in the United States and other Western democracies. Even if fake news is not new, it can now spread as never before. This makes it far more dangerous. But does anyone actually believe the outrageous stories that get posted, shared, and liked on social media? These stories’ persistence could have other explanations. Perhaps some people find them funny or unbelievable, or share them ironi- cally. Others may share them because, even though they know the content is false, the stories reflect their feelings about a topic. Many internet “memes”—digital artifacts that are widely shared on the internet—have the character of elaborate jokes, which often signal some sort of social status or engagement. 19 Is fake news the same? Perhaps. But some people do believe fake news. Clearly Edgar Welch, for instance, believed that the Comet Ping Pong pizzeria This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 155 harbored trafficked children. And he is not alone. In a survey con- ducted by the polling firm Ipsos Public Affairs for BuzzFeed News, 3,015 US adults were shown six headlines, three of which were real news and three of which were fake. 20 The headlines were drawn from a list of the most-shared news items in the months before the election, and in total, they had been shared a similar number of times. Respondents were asked whether they recalled the headlines and, for those they did recall, whether they were accurate. One-third of the survey respondents recalled seeing at least one of the fake news headlines. Those who remembered the headline judged the fake news to be “very” or “somewhat” accurate 75 percent of the time. By contrast, respondents who recalled seeing the real news headlines judged them to be accurate 83 percent of the time. Other surveys and experiments have found results broadly con- sistent with this picture.21 A Pew survey of 1,002 adults found that 23 percent admitted to having shared fake news—of whom 73 per- cent admitted that they did so unwittingly, discovering only later that the news was fake.22 (The others claimed to have known that it was fake at the time but shared it anyway.) Of course, these results do not include participants who unwittingly shared fake news and never learned that it was fake, nor do they include those who would not admit to having been duped. There is a famous aphorism in journalism, often attributed to a nineteenth-century New York Sun editor, either John B. Bogart or Charles A. Dana: “If a dog bites a man it is not news, but if a man bites a dog it is.” The industry takes these as words to live by: we rarely read about the planes that do not crash, the chemicals that do not harm us, the shareholder meetings that are uneventful, or the scientific studies that confirm widely held assumptions. For many issues, focusing on novel or unexpected events is un- This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 156 problematic. Novelty makes things salient, and salience sells papers and attracts clicks. It is what we care about. But for some subjects, including science as well as politics and economics, a novelty bias can be deeply problematic. We saw in Chapter 3 that a key feature of the Tobacco Strategy was to amplify and promote research produced by legitimate, unbi- ased scientists that tended to support the tobacco industry’s agenda. This was extremely effective, and in the models we have consid- ered, simply amplifying a subset of the evidence by sharing it more broadly can lead a community of policy makers, or the public, to become confident in a false belief, even as scientists themselves converge to the true belief. The basic takeaway here is that when trying to solve a problem in which the evidence is probabilistic or statistical, it is essential to have a complete and unbiased sample. Focusing on only part of the available evidence is a good way to reach the wrong belief. This sort of biasing can happen even if no one is actively trying to bias the evidence shared with the public. All it takes is a mecha- nism by which the evidence is selectively disseminated. This is precisely what happens when journalists focus on novel, surprising, or contrarian stories—the sorts that are most likely to gain attention, arouse controversy, and be widely read or shared. When journalists share what they take to be most interesting—or of greatest interest to their readers—they can bias what the public sees in ways that ultimately mislead, even if they report only on real events. To better understand how this sort of thing can happen, we looked at a variation of the propagandist models described in Chap- ter 3. Now, instead of having a group of policy makers connected to scientists and also connected to a propagandist, we imagine that we have a collection of policy makers who receive all of their informa- This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 157 tion from a single third party: a journalist, who scours the scientific results and shares the evidence that she judges to be most novel. We suppose that the journalist is updating her beliefs in light of all of the scientific evidence, so that as more evidence is gathered, she tends to converge to whatever the scientific consensus is. But she only shares what is most surprising. There are a few ways to implement this basic idea. For instance, one way is to have the jour- nalist look at each round of scientists’ results and then share what- ever result or results from that round are the most unlikely. An- other way is to have the journalist share all of the results that pass some kind of threshold, given her current beliefs: that is, the ones that the journalist thinks are sufficiently strange to be of interest. 23 In both of these cases, we find that the public sometimes con- verges to the false belief, even when the journalist and the scientific community converge to the true one. 24 In such a model, the journalist will generally share some results that are unlikely because they point so strongly in the direction of the true belief, and also some that are unlikely because they point strongly toward the false belief. She is not biasing the evidence in the explicit way that a propagandist would. Sometimes the journal- ist will actually strengthen our confidence in the true belief, relative to what the science supports. But that does not mean that her ac- tions are neutral. As we saw in the propagandist examples, it is the total distribution of evidence that tends to lead us to the true belief. And intervening to change this distribution will change where con- sumers of that evidence end up. There is another way in which journalists can unwittingly spread false beliefs. Journalism has legal and ethical frameworks that seek to promote “fairness” by representing all sides of a debate. From 1949 until 1987 the US Federal Communications Commission even maintained a policy called the Fairness Doctrine that required media This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 158 with broadcast licenses to offer contrasting views on controversial topics of public interest. The rule no longer applies—and even if it did, few people get their news from broadcast media any longer. But since few journalists relish being accused of bias, pressures re- main for journalists to present both sides of disagreements (or at least appear to). Fairness sounds great in principle, but it is extremely disruptive to the public communication of complex issues. Consider again the model of a journalist selectively communicating results. Now, instead of the journalist sharing only surprising results, suppose that every time she chooses to share a result supporting one view, she also shares one supporting the other view—by searching through the history of recent results to find one that reflects the “other per- spective.” In figure 16 we show a possible sample of outcomes for scientists performing trials (with ten data points each). The journalist shares the two bolded results, one randomly selected from those that sup- port B and one randomly selected from those that support A. The policy makers will see two studies where B was successful six times and three times, whereas the true distribution of results—nine, six, six, and three—would lead to a much more favorable picture of B’s general success. What happens?25 In this case, the policy makers do tend to con- verge to the true belief when the scientists do. But this convergence is substantially slower for policy makers than for the scientists— and indeed, it is substantially slower than if the journalist had merely shared two random results from the scientific community. This is because we generally expect evidence favoring the true belief to appear more often. Sharing equal proportions of results going in both directions puts a strong finger on the scale in the wrong direc- tion. Indeed, norms of fairness have long been recognized as a tool for propagandists: the tobacco industry, for instance, often invoked This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 159 the Fairness Doctrine to insist that its views be represented on tel- evision and in newspaper articles. 26 Ultimately, the mere existence of contrarians is not a good rea- son to share their views or give them a platform. And the sugges- tion that it would be unfair not to report contrarian views—or at least, not to report them specifically as contrarian views—especially when the scientific community has otherwise reached consensus, is wrong, at least if what we care about is successful action. This is so even when the contrarians have excellent credentials and legitimate results that do support their views. When we are trying to solve difficult problems, there will always be high-quality and convincing evidence that pushes us in both directions. You might worry about this: after all, throughout this book we have pointed to cases in which “consensus” views of experts have turned out to be false. What if journalists had reported only the views of those gentleman physicians who insisted that their hands could not possibly transmit disease, dismissing Semmelweis as a Figure 16. Communication in a model with a “fair” journalist who chooses two results to report to policy makers. Upon observing results from scientists, the journalist communicates one that supports theory B and one that spuriously supports A (bolded) to policy makers. This has the effect of biasing the set of data available to policy makers. Light nodes represent individuals who favor action A, and dark nodes, B. This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 160 habitual contrarian? Indeed, something very much like this occurred in the lead-up to the Iraq War in 2003: foreign-policy experts in the United States and the United Kingdom, along with politicians across the political spectrum, almost uniformly adopted the view that Saddam Hussein was developing weapons of mass destruction. (He was not.) The Bush and Blair administrations used these al- leged weapons to justify launching a war that ultimately led to more than a decade of severe civil strife and nearly two hundred thousand civilian deaths, according to the Iraq Body Count project. 27 The New York Times was widely criticized for presenting this consensus without adequate scrutiny or skepticism, and the editors took the highly unusual step of issuing an apology in 2004. 28 In this case, reporting only the consensus view and stories that were broadly in line with it had dire consequences. Fair enough. So how, then, are journalists to tell the difference— especially when they are not experts in the relevant material? For one, stories that are ultimately about current or historical events have a very different status from stories about science. It is not, and should not be, journalists’ role to referee scientific disagreements; that is what peer review and the scientific process are for, precisely because expert judgment is often essential. It is in those cases in which that process has already unfolded in the pages of scientific journals, and the losers of the debates wish to rehash them in public forums, that journalists should be most cautious. On the other hand, it most certainly is journalists’ job to investigate and question those purported matters of fact on which major domestic and for- eign policies are based—including determining whether there is a scientific consensus on an issue relevant to policy-making. Perhaps more important, it is essential to focus on the reasons for apparent controversy. We do not wish to rule out the possibility that today’s Semmelweis is laboring in obscurity, or that a talented reporter could not find him and bring his ideas to public attention. This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 161 This, after all, is precisely what 20/20 did with Hightower and methylmercury in fish. Our point, rather, is that the mere existence of contrarians or (apparent) controversy is not itself a story, nor does it justify equal time for all parties to a disagreement. And the publication of a surprising or contrary-to-expectation research ar- ticle is not in and of itself newsworthy. So we need journalists to avoid sensationalizing new findings and to report both that there is a consensus (when there is one) and the reasons for it. It is particularly important for journalists and politicians to carefully vet the sources of their information. It will invariably be the case that nonexperts need experts to aggregate evidence for them. This is what propagandists seek to exploit, by standing in for disinterested experts and aggregating evidence in a way favorable to their own interests. Often the groups doing this aggregation consciously attempt to mislead journalists about their independence and credentials. This is where institutions can play an important role. Journal- ists reporting on science need to rely not on individual scientists (even when they are well-credentialed or respected), but on the con- sensus views of established, independent organizations, such as the National Academy of Sciences, and on international bodies, such as the Intergovernmental Panel on Climate Change. Likewise for other matters of fact important to policy-making, such as tax and economic policy. In nations with reliably independent government record keepers—such as the Bureau of Labor Statistics in the United States, which measures economic indicators such as unemployment and inflation, or the Congressional Budget Office, which scores proposed legislation on expected budgetary impact—the findings of these organizations should be given special weight, as compared with self-appointed experts claiming that the “official” figures are faulty, misleading, or biased. Reports from the United Nations, par- ticularly when they involve serious peer review, as with the IPCC, This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 162 are also often more reliable than those from the governments of individual nations. Such institutions can certainly be manipulated for partisan ends,29 but they are far more likely to be reliable than individuals or organizations whose interests are tied to the issues at stake. In the predawn hours of July 10, 2016, a man named Seth Rich called his girlfriend as he walked home from a Washington, D.C., bar.30 The bar was only a few miles away from his home, but Rich walked slowly and they chatted for a few hours. And then he abruptly ended the call. Moments later, at about 4:20 a.m., Rich was shot twice in the back. Police responded immediately and took him to a local hospital, but he soon died of his wounds. Rich was twenty-seven years old. Just a few days earlier, he had accepted a job as a staffer for Hillary Clinton’s presidential cam- paign. The Democratic National Convention, at which Clinton would be officially nominated as the party’s presidential candidate, would open in about two weeks. For the previous two years, Rich had worked for the Democratic National Committee (DNC), tasked with increasing voter turnout during the contentious primary be- tween Clinton and Bernie Sanders. 31 The D.C. Metropolitan Police ruled the death a homicide, a robbery attempt gone wrong. But within a few days of Rich’s shoot- ing, conspiracy theories began to swirl, apparently initiated by a post on the internet discussion site Reddit. 32 The allegation was that Rich had been killed by the Clintons, or perhaps Debbie Wasserman Schultz, then head of the DNC, to cover up election fraud during the primary campaigns. Commentators on the thread speculated that through his job at the DNC, Rich had learned of efforts to suppress Sanders voters or otherwise tilt the primary toward Clin- This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 163 ton, and that this information had made him dangerous to the Clinton campaign. Rumors that a Clinton had murdered someone who presented a political liability were hardly new. In 1994, William E. Dannemeyer, a former Republican congressman from Southern California who had been voted out of office two years previously, sent a letter to congressional leaders asserting that then-President Bill Clinton had been responsible for dozens of murders in his rise to the presi- dency.33 Dannemeyer listed twenty-four people connected with the Clintons who he claimed had died “under other than natural circumstances.” His list appears to have originated with an activist named Linda Thompson, who the previous year had produced thirty-four names of people who she believed—admittedly with “no direct evidence”—had been killed by Clinton or his associates. A 1994 film called The Clinton Chronicles made similar charges. 34 These claims were investigated at length by law enforcement and various special prosecutors during Bill Clinton’s presidency. They have no substance. And yet they have persisted among con- spiracy theorists on both the far right and the far left, creating an ecosystem in which the allegations concerning Rich seemed unsur- prising, even plausible. Soon the story took on new texture. About a month before Rich was killed, the DNC admitted that its computer servers had been hacked and thousands of emails stolen. 35 The first indications of a hack had come nine months previously, when the FBI contacted the DNC to say that at least one of its computers had been com- promised by Russian operatives. A technician checked the system and found nothing amiss, and the DNC did not respond further. Two months later, the FBI again contacted the DNC, this time to say that one of its servers was routinely sending data to Russia. Once again, the DNC’s IT staff decided that the computers had not been This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 164 breached and did not communicate the FBI’s warnings to the DNC leadership. The DNC apparently did nothing to stop the hack. That Russian hackers had compromised DNC computers was not widely reported until June 14, 2016. The DNC acknowledged it and attributed it to Russian security services the following day. Meanwhile, WikiLeaks founder Julian Assange announced that he had thousands of pages of emails related to the Clinton campaign that he would soon release.36 WikiLeaks waited another month to release the emails, apparently to maximize damage to Clinton’s cam- paign: the messages appeared online on July 22, 2016, days before the Democratic convention opened. Some of the leaked emails sug- gested that the DNC had taken actions to support Clinton’s candi- dacy and to undermine that of Sanders. A hacker claiming to be Romanian and calling himself Guccifer 2.0 claimed responsibility for the hack. But the FBI, which launched an official investigation on July 25, 2016, indicated that it believed Russian hackers were behind the attacks—an accusation consistent with its earlier warnings to the DNC. Half a dozen private cyber- security firms came to the same conclusion, and by October 2016, there was a broad consensus among the US intelligence agencies that the hacks had been conducted by Russian intelligence services. 37 Still, WikiLeaks never confirmed Russian involvement. In Au- gust 2016, Assange implied that the messages had come from a dif- ferent source altogether: Seth Rich. He offered a twenty-thousand- dollar reward for information related to Rich’s death, and then, in an interview with the Dutch television program Nieuwsuur, he asserted, following a question about Rich, that “our sources take risks”—before adding that WikiLeaks has a policy of never reveal- ing or otherwise commenting on its sources. 38 The story’s lack of evidence, or even coherence, did not stop some media outlets from reporting these allegations. Some of this This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 165 coverage came from RT, the English-language Russian propaganda site, which seemed to like having Rich as an alternative culprit for the DNC hack.39 But more mainstream sources also reported on Assange’s remarks, including the Washington Post, BuzzFeed News, and Newsweek. The story got a further boost in May 2017 when Fox News and several Fox affiliates ran stories in which it was claimed that the hacked DNC emails had been discovered, by the FBI, on Rich’s computer.40 These allegations were attributed to a man named Rod Wheeler, a former Washington, D.C., homicide detective who had been paid to work on the Rich case by a Republican insider. Wheeler talked as if he had himself seen the messages on Rich’s computer and could speak directly to this new evidence. There was only one problem: the Fox News story was com- pletely fabricated. Shortly after it appeared, the FBI stated that it was not involved in the Rich investigation. Soon Wheeler admitted that he had not, after all, seen the emails on Rich’s computer. Rich’s parents produced a statement saying that they had not seen or heard of any evidence that Rich had ever possessed or transmitted any DNC emails—or that Rich’s death was anything other than a botched robbery. Either the evidence did not exist, or whoever had it was withholding it. About a week after first airing the stories, Fox and its affiliates retracted them; that August, Wheeler filed a lawsuit against Fox claiming that its reporters had knowingly fabricated quotations they attributed to him and that the entire story had been orchestrated in consultation with the White House. 41 It seems that there is not, and never has been, any reason to believe that Rich had any involve- ment in the DNC email hack. Yet at least some right-wing media personalities, including Fox’s own Sean Hannity, have continued to repeat variations on these claims. Hannity, in particular, has refused This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 166 to issue a retraction over the Wheeler remarks—even after Wheeler himself disavowed them. The Rich example shows how thin the line between “real news” and “fake news” can be. Of course, Fox News (like ETF News— and MSNBC) has a transparent political orientation; but by run- ning a story based on the remarks attributed to Wheeler, it veered from editorial slant into blatant falsehood. The example shows that even legitimate news sources can produce and spread fake news. And if the allegations in Wheeler’s lawsuit that the White House was in- volved in the story are true, the situation is even more troubling. Some readers will surely respond that Fox News, especially the commentary side that includes Hannity, is not a legitimate news source at all. But it is not the only mainstream source to have spread fake news since the 2016 election. MSNBC host Stephanie Ruhle, for instance, claimed on air that Fox News had held its 2016 Christ- mas parties at the Trump International Hotel in Washington, D.C. The claim was false, and she later apologized. 42 (The Republican National Committee, however, does seem to have held a Christmas party at the hotel that year.)43 And CNN Investigates, an elite CNN reporting team, was forced to retract two stories related to Trump during the summer of 2017: in one, they claimed that Anthony Scaramucci, who very briefly served as White House communica- tions director, had been connected to a Russian investment fund; in another, they claimed that former FBI director James Comey would offer testimony to Congress, which he never ultimately gave. 44 Fox News is in a different category from ETF News or other right-wing sources such as Breitbart or Infowars. So are CNN and MSNBC. What makes them different is that Fox, CNN, and MSNBC generally retract false stories in light of new evidence. A self-correcting editorial process is at work. 45 One can quibble about This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 167 how long it takes to make the retractions, and whether whatever damage was going to be done had already happened. But in the cases where the facts are simply and demonstrably wrong, these media sources have corrected the errors—and in many cases, they have done so because other news sources have policed them. 46 That these groups can nonetheless spread falsehood makes the problem of identifying fake news much more difficult. It is not as simple as pointing to reliable and unreliable sources. These sorts of blatant falsehoods, though, are not the only prob- lem, and maybe not the worst one. A deeper issue concerns a more subtle way in which fake news shades into real news: it sets a jour- nalistic agenda. 47 Unlike in the Pizzagate fiasco, real facts exist in the background of the Rich story. Rich really was murdered, there really have been allegations that he was involved in the DNC email leak, WikiLeaks really did offer a large reward for information re- lated to the case, and real investigations have been privately funded and conducted to identify his killers—who, by the way, have not been identified or arrested. It is not “fake” news to report on these facts, particularly given that a large readership wants to pore over such stories. But it is also hard to see it as real news if the only reason anyone was interested in the first place was that these facts are tied up with widespread fake news. Ultimately, fake news, unsubstantiated alle- gations, and innuendo can create interest in a story that then justi- fies investigations and coverage by more reliable sources. Even when these further investigations show the original allegations to be base- less, they spread the reach of the story—and create the sense that there is something to it. Were it not for the conspiracy theories, Assange would never have been asked about Seth Rich, the Wash- ington Post would not have covered his remarks (or refuted them), private investigators would not have been funded by Republican lobbyists for Fox News to quote, and so on. This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 168 Here is another manifestation of a theme that has come up throughout this book. Individual actions that, taken on their own, are justified, conducive to truth, and even rational, can have trou- bling consequences in a broader context. Individuals exercising judg- ment over whom to trust, and updating their beliefs in a way that is responsive to those judgments, ultimately contribute to polariza- tion and the breakdown of fruitful exchanges. Journalists looking for true stories that will have wide interest and readership can ul- timately spread misinformation. Stories in which every sentence is true and impeccably sourced can nonetheless contribute to fake news and false belief. These dynamics are troubling, but once we recognize them, it appears that small interventions could have a significant impact. In particular, it is important to distinguish two essentially different tasks that reliable news sources perform. One involves investigat- ing allegations, checking facts, and refuting false claims. This is an important activity—but it is also a risky one, because it can, coun- terintuitively, expand the reach of fake stories. In some cases, as with the Comet Ping Pong conspiracy, it can turn fake news into a real story. The other task involves identifying and reporting real stories of independent interest, relevant to readers’ lives and responsive to their demands. This is also an important activity, and although it requires judgment, it runs fewer risks of promoting falsehoods. We suggest that these activities need to be kept firmly separated— and that media which serve as primary news sources, such as the Wall Street Journal, New York Times, and Washington Post, should con- sider avoiding the first sort of activity altogether. Checking facts and policing the media, while extremely important, are best left to independent watchdogs such as PolitiFact and Snopes.com, which run less of a risk of driving further traffic and attention to false stories. • This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 169 There is overwhelming evidence that foreign actors, apparently as- sociated with Russian intelligence services, attempted to interfere in the 2016 US election—initially to weaken Hillary Clinton’s can- didacy, and later to promote Donald Trump. 48 As we noted, for in- stance, the US intelligence community has determined that Russian intelligence services hacked the DNC’s email servers and then re- leased embarrassing emails, particularly related to the treatment of Bernie Sanders. 49 Facebook, Twitter, and Google have revealed that accounts linked to the Russian government spent well over one hundred thousand dollars to purchase political ads, most of which seem to have been designed to create controversy and sow civil dis- cord. Facebook has subsequently revealed that Russian-produced political content reached as many as 126 million US users. 50 As of this writing, allegations of explicit coordination have not been fully settled one way or the other, but there is little doubt that the Trump campaign knew that the Russian government favored its candidate and was making efforts to influence the election in his favor. For our purposes, however, there are even more important ways in which Russia-backed groups appear to have influenced the elec- tion, and Western political discourse more generally. For instance, let us simply stipulate for the sake of argument that Russian agents did in fact hack the DNC servers and release some or all of the emails they stole. What was their purpose in doing so? If gathering intelligence were the whole goal, it would make little sense to release the hacked emails. The character and scale of the release suggest a different motive. Ultimately, the release of the DNC emails has led to lasting divisions and sustained controversies within the Democratic Party, which in turn have affected Demo- crats’ ability to effectively govern. In other words, the emails have produced discord and mistrust—and in doing so, they have eroded an American political institution. Perhaps this was the point. This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 170 The idea that Russia’s goal was to create social division and un- dermine democratic (small d) institutions is supported by the ways Russia-backed groups appear to have used social media during the campaign. Although it seems they aimed to help Trump’s candi- dacy, they did not target their efforts only at potential Trump voters or others on the political right. Russian organizations are reported to have developed a bestiary of personalities, voices, and positions crafted to influence various groups, 51 ranging from members of the LGBTQ community, to Black Lives Matter activists, to gun rights supporters, to anti-immigrant zealots, and even, bizarrely, to ani- mal lovers. One goal appears to have been to establish trust with a broad range of demographic and interest groups in order to influence them. The New York Times reported in October 2017 that the Russia- linked LGBT United Twitter account, for example, declared: “We speak for all fellow members of LGBT community across the na- tion. Gender preference does not define you. Your spirit defines you.” Even the dog-lover page, the Times suggests, was probably formed with the intention of developing a set of followers who could then be slowly exposed to political content. In other words, the goal was to get close, pose as a peer, and then exert influence. After convincing users that they shared core beliefs and values, Russians used these platforms to widen the gap between right and left. Their “Heart of Texas” Facebook page tried to persuade Texans to secede from the nation. Their “Blacktivist” group agitated for protests in Baltimore in response to the death of Freddie Gray, a twenty-five-year-old black man who died shortly after being trans- ported without safety restraints in a police van. One bizarre cam- paign included a contest titled “Don’t Shoot Us,” which encouraged Pokémon Go users to capture Pokémon near real-world sites of police brutality and name them after the victims. 52 In none of these cases were group members pushed to accept This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 171 views far from those they already held. The LGBTQ group, for instance, did not advocate for Trump; instead, social influence was used to push people to more extreme versions of the views they already held. The picture that emerges is one in which Russian propagandists were highly sensitive to the dynamics of social influence, in several ways. First, recall the role that network structure plays in the mod- els of conformity we presented in Chapter 2: agents in some posi- tions in a network are much more influential than those in other positions. For instance, in Chapter 3 we observed that the agent at the center of a star or wheel structure is more influential than those on the periphery. In part this is simply a matter of having a larger overall number of connections to other agents, but that is not the whole story. It also matters that the other agents are more weakly connected to one another than the central agent is to everyone. At least some of the Facebook content linked to Russian ac- counts were “pages” or “community pages” rather than “groups.” It is easy to use the terms indistinguishably, but the distinction matters: a Facebook “group” is primarily designed to facilitate dis- cussions among its members. A Facebook “page” is designed for an organization or celebrity to create a community of their followers. “Community pages” are somewhere in between: there, other users can sign up as “followers” of the page and can make posts directed to the community; but whoever created the page can make posts as well, which are then targeted at and shared with the whole com- munity. If you visit one of the Russian-linked community pages—say, the LGBT United page, which was active as of March 2018—the first things you see are the posts created by the group itself, identified as “LGBT United” posts; the posts directed at the group from other members of the community can be found only by following a fur- ther series of links. In other words, the community page mimics a This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 172 star network, with the page creator at the center. Rather than try to influence people already at the center of star networks, the Russian accounts appear to have created star networks by creating affinity groups structured so that they could communicate to the whole community more easily than members could communicate with one another. Of course, the mere ability to broadcast information is not suf- ficient to create influence. You also need those to whom you are broadcasting to listen. And here we see the value of creating and distributing content through groups defined by a subject of shared interest or agreement—be it the right to bear arms or the right to love kittens. Recall that the central assumption underlying our models of po- larization was that people are more susceptible to the influence of those they trust—in particular, those who they think have formed reliable beliefs in the past. This means that establishing connec- tions through affinity groups provides powerful tools for influence, especially when the influence tends to push them farther in direc- tions they are already inclined to go. And if the purpose is merely to drive polarization—as opposed to persuading everyone of any par- ticular claim—posing to people on both sides of an issue as someone who shares their opinions, and then presenting further evidence or arguments in support of those opinions, will be very successful. Structuring these interactions around affinity groups may have allowed the Russia-linked groups to exert even more influence. Re- call the modification to the polarization models that we described in Chapter 3, when we discussed reputation. There we considered the role that shared beliefs in one domain could play in influencing beliefs in other domains. We described how, in a model in which each agent considers agreement on a range of issues when deter- mining how much to trust another agent on any particular issue, we found that different beliefs influenced one another: populations This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 173 would become polarized on multiple different beliefs but would do so in a strongly correlated way. How might a propagandist use this fact to manipulate beliefs? In the examples we considered in Chapter 3, the strategy was to find someone who had demonstrated success in solving other problems— someone with a long track record of discovering the truth—and have that person act as a mouthpiece for the propagandist. But another strategy is available. If we consider beliefs across a range of issues in determining whom to trust, then establishing the existence of shared beliefs in one arena (opinions on gun laws or LGBTQ rights) provides grounds for trust on issues in other are- nas. So if someone wanted to convince you of something you are currently uncertain about, perhaps the best way for that person to do it is to first establish a commonality—a shared interest or belief. The affinity groups appear to have played just this role. One natural response to fake news is to say that social media sites, web search providers, and news aggregators have a responsibility to identify fake news and stop it. It was, after all, a Facebook algorithm that actively promoted a fake story, hosted on ETF News, about Megyn Kelly. In other cases, these sites merely provided vehicles for fake news stories to go viral. This is in part a matter of what people are interested in reading about—but it is amplified by algo- rithms that social media sites use to identify which stories are highly engaging so as to maximize their viewership. We fully endorse this solution: organizations like Facebook, Twit- ter, and Google are responsible for the rampant spread of fake news on their platforms over the past several years—and, ultimately, for the political, economic, and human costs that resulted. But assign- ing culpability is not the same as identifying solutions. If it was algorithms on social media sites that amplified and spread This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 174 fake news, one might hope that algorithms could also help to iden- tify fake news and prevent it from being amplified on these sites—or even prevent fake news from being shared on such sites at all. But many sites that produce or distribute fabricated stories—including ETF News—also produce stories that are true. Should it be possi- ble to share those true stories if they come from unreliable sources? And as we have just seen, media groups that typically produce stories that are true or essentially true can also sometimes produce false stories. Would those stories make it through the filter? If so, how many fake, or at least false, news stories should a given site be able to run before it gets flagged as a fake news provider? Since the 2016 election, a large number of academic articles have appeared that offer algorithmic solutions to the fake news prob- lem. 53 These efforts are surely worthwhile, but determining what is true is a difficult, time-consuming, and human-labor-intensive pro- cess. (The Kelly story was promoted by Facebook days after Face- book fired its human editors.) Algorithmic responses can help, but more is needed: ultimately, we need human editorial discretion, armies of fact checkers, and ideally, full financial and political inde- pendence between the groups whose actions are covered by news organizations, whose platforms are used to distribute news broadly, and who are responsible for evaluating whether claims are true. We need to recognize fake news as a profound problem that requires accountability and investment to solve. Perhaps more important, we need to recognize that fake news stories—and propaganda more generally—are not fixed targets. These problems cannot be solved once and for all. Economist Charles Goodhart is known for “Goodhart’s law,” which has been glossed by anthropologist Marilyn Strathern as, “When a measure becomes a target, it ceases to be a good measure.”54 In other words, whenever there are interests that would like to game an instrument of measurement, they will surely figure out how to do it—and once This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 175 they do, the measurement is useless. A classic example occurred in Hanoi, Vietnam, under French colonial rule. In the early 1900s, the city was so overrun with rats that the colonial government offered a bounty for every rat tail returned to them. The problem was that people began cutting off rats’ tails and then simply releasing the rats to breed and make more rats, with more tails. 55 We should expect a similar response from fake news sources. As soon as we develop algorithms that identify and block fake news sites, the creators of these sites will have a tremendous incentive to find creative ways to outwit the detectors. Whatever barriers we erect against the forces of propaganda will immediately become targets for these sources to overcome. Bennett Holman, for example, uses historical cases to illustrate how pharmaceutical companies con- stantly devise new strategies to get around reforms by consumer pro- tection groups, prompting further action on the part of these groups, and so on. He compares the process to an asymmetric arms race. 56 The term “arms race” will conjure, for most people, the nuclear arms race between the US and the USSR during the Cold War, where each side attempted to out-arm the other, leading to the proliferation of ever more dangerous and massive bombs. In biol- ogy, we see arms races between species with conflicting interests: cheetahs get faster to catch gazelles, who in turn evolve to outrun cheetahs; prey species concentrate neurotoxins while their preda- tors evolve an ever-increasing resistance to the poisons. This framework paints a dreary picture of our hopes for defeat- ing fake news. The better we get at detecting and stopping it, the better we should expect propagandists to get at producing and dis- seminating it. That said, the only solution is to keep trying. The idea that our search for truth in public discourse is an endless arms race between highly motivated, well-funded political and in- This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 176 dustrial forces attempting to protect or advance their interests, and a society trying to adapt to an ever-changing media and technolog- ical landscape, suggests that would-be propagandists and others who would seek to distort the facts will constantly invent new methods for doing so. If we hope to have a just and democratic society whose policies are responsive to the available evidence, then we must pay attention to the changing character of propaganda and influence, and develop suitable responses. The models and examples we have discussed suggest some inter- ventions that could help us fight fake news—and propaganda more generally. Just as we have focused on only one aspect of why false beliefs spread—social effects rather than individual psychological considerations, which surely also matter—our proposals on how we might best respond, as a society, to fake news and propaganda will also be only part of the story. But we do think that recognizing the importance of social effects on the spread and persistence of false beliefs, even if we assume that all individuals in a society are per- fectly rational (which, alas, they are not), shows that whatever else we do, we also need to think about interventions that take networks into account. One possible intervention concerns the relationship between local issues and issues that are more abstract, in the sense of being disconnected from individuals’ everyday experiences. The more local our politics is, the less chance for it to be dominated by distorting social effects of the sort that have emerged in recent years. This is  because policies with local ramifications give the world more chances to push back. Consider the difference between national legislation aimed at regulating emissions generally, and local legis- lation aimed at regulating emissions from the nearby coal plant. Or legislation on mercury contamination in a town’s fishing areas that has observable effects on the day-to-day lives of those who would vote on the bill. This generates a situation better matched by mod- This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 177 els in which there is a notable difference between the success rates of actions A and B, and where conformity is less significant. We should expect in such cases that social effects will not matter as much, and that it will be harder for industry to take advantage of these effects to obscure the truth. Of course, some pressing issues, such as economic policy and climate policy, are necessarily national or international problems that cannot be made local in the same way. But the more individual politicians can do to redirect political discourse toward issues of local significance, the more we should expect voters to be guided by the best evidence available. Ironically, this suggests that one of the drivers of political dysfunction in the United States may have been anticorruption efforts related to earmark spending—that is, to fed- eral appropriations used to support projects in individual congres- sional districts. (In 2011, President Obama declared in his State of the Union address that he would veto any bill that contained ear- marks, and the following month, the Republican-controlled Con- gress enacted a ban on such spending.) Voters who are worried about real, tangible consequences to their lives may be less likely to engage in conformist identity signaling during elections. A second possible intervention concerns our ability to construct social networks that minimize exposure to dissenting opinion and maximize positive feedback for holding certain beliefs, indepen- dent of their evidential support. Social media sites should change the algorithms by which information is shared among members so that all members of the site are exposed to material that is widely shared and discussed among subgroups (rather than limiting expo- sure to just those subgroups). 57 Remember that in our conformity models, dissenting opinions were bolstered and protected by cli- quish, clumpy social networks. When individuals are mostly con- nected within one small group, outside influences matter less than those within the clique, and conformity effects can buffer the group This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 178 from evidence that conflicts with their false beliefs. Even if some members gather this evidence, they are unwilling to share it for fear of social censure. Simply sharing more information with all members of such a group may not disrupt their conformity, but the more people in a network with access to reliable information, the more likely that someone will manage to buck the social trends. Of course, even if we could magically scramble all the social net- works in the United States, this would not solve the issue of dis- trust. As we have discussed, even if every member of a group can communicate, the group can still become polarized when people do not trust those with different beliefs. Perhaps ironically, propagan- dists have demonstrated remarkably effective interventions for such situations. To persuade a group to change beliefs, you need to find someone who shares their other core beliefs and values, and get that person to advocate your view. We do not recommend setting up fake Facebook interest groups to persuade, say, anti-vaxxers to change their minds. But we do recommend finding spokespeople whose shared values can ground trust with groups that are highly dubious of well-established facts. In an ideal world, trusted politicians might play this role. If Republican representatives who believe in anthropogenic climate change, for example, were willing to share this with their conserva- tive constituents, they might have a serious impact on public opin- ion. In fact, joint work by Aydin Mohseni and Cole Williams sug- gests that when individuals go against the trends in their own social networks to hold a minority belief, their statements of this belief can be especially powerful. Because other people expect them to conform, it is easy to infer that they must have good reasons for taking such a socially risky position.58 We might call this the “mav- erick effect”—such as when Arizona senator (and self-styled maver- ick) John McCain says that climate change is real, his statement This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 179 has much more impact on the right than the same one made by Al Gore. This same mechanism, of course, worked in the opposite di- rection in the case of Roger Revelle. The implication is that finding, targeting, and publicizing the views of a few individuals who are willing to go against the political and social consensus to spread true beliefs can have an outsized so- cial effect. It is even better if these individuals are (like politicians) highly connected. Those at the center of a star are under unusual pressure to conform, that is, not to play the maverick, but they also have considerable power to sway their peers when they decide to do so. We can keep hoping that politicians will do the right, rather than the expedient, thing. But there may be more hope for the rest of us. One general takeaway from this book is that we should stop thinking that the “marketplace of ideas” can effectively sort fact from fiction.59 In 1919, Justice Oliver Wendell Holmes dissented from the Supreme Court’s decision in Abrams v. United States to uphold the Sedition Act of 1918.60 The defendants had distributed leaflets denouncing US attempts to interfere in the Russian Revo- lution. While the court upheld their sentences, Holmes responded that “the ultimate good desired is better reached by free trade in ideas. . . . The best test of truth is the power of the thought to get itself accepted in the competition of the market.” Holmes’s admirable goal was to protect free speech, but the met- aphor of the marketplace of ideas, as an analogue to the free market in economics, has been widely adopted. Through discussion, one imagines, the wheat will be separated from the chaff, and the public will eventually adopt the best ideas and beliefs and discard the rest. Unfortunately, this marketplace is a fiction, and a dangerous one. We do not want to limit free speech, but we do want to strongly advocate that those in positions of power or influence see their speech for what it is—an exercise of power, capable of doing real This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 180 harm. It is irresponsible to advocate for unsupported views, and doing so needs to be thought of as a moral wrong, not just a harm- less addition to some kind of ideal “marketplace.” This is as true for scientists as it is for political and social leaders. Remember the propaganda models presented in Chapter 3. These showed that studies that erroneously support false beliefs are essen- tial tools for propagandists. This is not the fault of scientists, but on the (certain) assumption that industry interests are here to stay, it is still incumbent on scientists to take whatever measures they can to prevent their work from being used to do social damage. 61 This means, first, that scientific communities must adopt norms of publication that decrease the chances of spurious findings, espe- cially in cases when the public good is clearly on the line. Second, scientists need to consider inherent risks when they publish. Phi- losopher of science Heather Douglas has argued persuasively that it is the responsibility of scientists to take into account, throughout the scientific process, the social consequences of the work they do and to weigh these against the benefits of publishing—or at least, to hold their own research on socially sensitive topics to particularly high standards before they do choose to publish it.62 One might respond that the duties of scientists are just to do science. We side with Douglas in thinking that in areas where industrial propagan- dists are at work, the risks to society from publishing are sufficiently high that they must factor into scientists’ decisions. 63 There are other ways to do science that minimize the risks of playing into the hands of propagandists. Throughout the present book, we have pointed out that low-powered studies are especially likely to generate spurious results. One solution is for scientific communities to raise their standards. Another is for groups of scien- tists to band together, when public interest is on the line, and com- bine their results before publishing. This generates one extremely strong piece of research with a clear consensus, rather than many This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 181 disjointed, weaker studies with disparate results. This strategy has the downside of suppressing contrary views, but that is also the point. If scientists work out disagreements ahead of time, they protect the public from those who would use dissent to sow confusion. 64 Where it is impossible for scientists to work together in this way, independent bodies—possibly government organizations but ide- ally independent scholarly organizations such as the National Acad- emy of Sciences—should oversee the aggregation and presentation of disciplinary consensuses. Such a step does not avoid the problem of propagandists who promote research that aligns with their inter- ests, but it would make abundantly clear to anyone paying attention that the studies to which the propagandists want to direct our at- tention are not consistent with the state of the art. Another clear message is that we must abandon industry funding of research. It is easy to think, as a scientist, that one is incorrupti- ble. As we have discussed, though, even in cases in which scientists are not corrupt, industry funding can drastically alter the scientific process through industrial selection. Industry funding is tempting, since it is plentiful and research is expensive. But if we want good science that protects the interests of the public, this expense is a cost the public must bear. It is too costly to allow industry to interfere with science, and too easy for it to do so, even when scientists are individually trustworthy. Journalists, to minimize the social spread of false belief, need to hold themselves to different standards when writing about science and expert opinion. As we have argued, attempts at fairness often bias the scientific evidence seen by the public. Giving a “fair shake” to minority viewpoints in science can grant authority and power to fringe elements or downright bad actors. Everything we have seen in this book indicates that journalists should instead try to give to the public an unbiased sampling of available evidence. If there are ninety-nine studies indicating that smoking is dangerous for every This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 182 one study indicating the opposite, journalists should talk to ninety- nine scientists who think smoking is harmful for every one who does not. ( John Oliver, on his satirical news show, recently did just this in real time, by bringing ninety-seven climate scientists and three skeptics onto his stage at once—or at least actors who did a good job of posing as such.) 65 In this vein, Wikipedia has developed a commendable standard for writing about controversial scientific topics. 66 The Wikipedia “proper weighting” standard holds that if multiple opinions or views are expressed in peer-reviewed articles in journals that are indexed by reputable scientific databases, then it is appropriate to include all such opinions in a Wikipedia article. But the weight given to each such opinion—that is, the space allotted to the opinion relative to other such opinions—should be proportional both to the number of published articles in high-impact journals that support the view, and to the number of citations such articles have received, with more recent articles and citations given greater weight than older ones. This may sound like a complicated standard to meet, but in  fact, modern scholarly tools (including, for instance, Google Scholar) can make it very easy to identify which articles are highly cited and which opinions are widely defended in reputable journals. Of course, whatever respectable journalists do, their effect will be muted as long as other sources in the marketplace peddle false or misleading material. On this point, we have a controversial sug- gestion. We currently have a legislative framework that limits the ability of certain industries—tobacco and pharmaceuticals—to ad- vertise their products and to spread misinformation. This is be- cause there is a clear public health risk to allowing these industries to promote their products. We also have defamation and libel laws that prohibit certain forms of (inaccurate) claims about individuals. We think these legislative frameworks should be extended to cover This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 183 more general efforts to spread misinformation. In an era of global warming, websites like Breitbart News and Infowars are more dam- aging to public health than Joe Camel and the Marlboro Man were in the past, and they should be treated as such. 67 In many ways, the United States is behind Europe on this front. Like the US and UK, France, Germany, the Netherlands, and other European nations have been targeted by fake news—often with alleged links to Russia.68 In 2015, the European Union created a specialized task force called East StratCom, with the express pur- pose of identifying strategies to combat fake news and propa- ganda. 69 (The group is called “East” StratCom because the EU rec- ognized Russia as the source of troubling disinformation campaigns as early as 2015.) The group draws on a large network of volunteers to identify and debunk disinformation—a kind of institutionalized version of Snopes.com or PolitiFact. More recently, Germany has implemented new laws aimed at holding social media companies responsible for “unlawful” content that remains on their sites— including material deemed to be hate speech.70 At the time of writ- ing, French president Emmanuel Macron has endorsed a similar law against fake news more broadly.71 Some readers may consider this a form of censorship and coun- ter to the spirit of free speech.72 But the goal here is not to limit speech. It is to prevent speech from illegitimately posing as some- thing it is not, and to prevent damaging propaganda from getting amplified on social media sites. If principles of free speech are com- patible with laws against defamatory lies about individuals, surely they are also compatible with regulating damaging lies dressed up as reported fact on matters of public consequence. Lying media should be clearly labeled as such, for the same reason that we provide the number of calories on a package of Doritos or point out health ef- fects on a cigarette box. And social media sites should remain vigi- This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 184 lant about stopping the spread of fake news on their platforms or, at the very least, try to ensure that this “news” is clearly labeled as such. We expect these suggestions, if implemented, to be just another step in the long arms race with propagandists. For this reason, part of the picture will have to involve regulatory bodies in government as well as online sources whose entire purpose is to identify and block sources of misinformation. This will require significant social resources to do well. But when the safety and well-being of nations and the world depend on it, it seems like the least we can do. We conclude this book with what we expect will be the most contro- versial proposal of all. This suggestion goes beyond the core issues of truth, falsehood, science, and propaganda that we have focused on. We believe that any serious reflection on the social dynamics of false belief and propaganda raises an unsettling question: Is it time to reimagine democracy? We do not mean to express skepticism about the ideals of a dem- ocratic society—properly construed. (More on that in a moment.) But we do think that the political situation among Western de- mocracies suggests that the institutions that have served us well— institutions such as a free and independent press, publicly funded education and scientific research, the selection of leaders and legis- lators via free elections, individual civil rights and liberties—may no longer be adequate to the goal of realizing democratic ideals. In a pair of important books—Science, Truth, and Democracy (2001) and Science in a Democratic Society (2011)—the philosopher of sci- ence Philip Kitcher has presented a vision of what it means to do science in a way that is responsive to the needs of democracy—and also what it means to have a democracy that is suitably responsive to the facts uncovered by that science. We wish to extract from Kitcherism an idea about what it means This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 185 to have a democratic society that is responsive to fact. When it comes to decisions about and informed by science—which we may think of, broadly, as everything from clearly scientific subjects such as climate change to calculations of the actual flows of immigrants across the US-Mexico border—what he calls “vulgar democracy” is simply unacceptable. Vulgar democracy is the majority-rules picture of democracy, where we make decisions about what science to support, what constraints to place on it, and ultimately what pol- icies to adopt in light of that science by putting them to a vote. The problem, he argues, is simple: most of the people voting have no idea what they are talking about. Vulgar democracy is a “tyranny of ignorance”—or, given what we have argued here, a tyranny of pro- paganda. Public beliefs are often worse than ignorant: they are ac- tively misinformed and manipulated. 73 As we have argued throughout this book, it is essential that our policy decisions be informed by the best available evidence. What this evidence says is simply not up for a vote. 74 There is an obvious alternative to vulgar democracy that is equally unacceptable. Decisions about science and policy informed by science could be made by expert opinion alone, without input from those whose lives would be affected by the polices. As Kitcher points out, this would push onto scientific elites decisions that they are not qualified to make, because they, too, are substantially igno- rant: not about the science, but about what matters to the people whose lives would be affected by policy based on that science. Kitcher proposes a “well-ordered science” meant to navigate be- tween vulgar democracy and technocracy in a way that rises to the ideals of democracy. Well-ordered science is the science we would have if decisions about research priorities, methodological proto- cols, and ethical constraints on science were made via considered and informed deliberation among ideal and representative citizens able to adequately communicate and understand both the relevant This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms The Social Network 186 science and their own preferences, values, and priorities. 75 But as Kitcher is the first to admit, there is a strong dose of utopianism here: well-ordered science is what we get in an ideal society, free of the corrupting forces of self-interest, ignorance, and manipulation. The world we live in is far from this ideal. We may strive for well- ordered science, but it is not what we have. As it stands, matters of crucial public interest—the habitability of huge swaths of our planet; the population’s susceptibility to dis- ease and its exposure to pollutants, toxins, and radiation—are de- cided in a way that mimics the mechanisms of vulgar democracy without realizing any of its ideals. Before it can influence policy, hard-won knowledge is filtered through a population that cannot evaluate it—and which is easily manipulated. There is no sense in which the people’s preferences and values are well-represented by this system, and no sense in which it is responsive to facts. It is a caricature of democracy. Of course, replacing this system with well-ordered science is beyond impractical. What we need to do instead is to recognize how badly our current institutions fail at even approximating well- ordered science and begin reinventing those institutions to better match the needs of a scientifically advanced, technologically so- phisticated democracy: one that faces internal and external adver- saries who are equally advanced and constantly evolving. We need to develop a practical and dynamic form of Kitcherism. Proposing our own form of government is, of course, beyond the scope of this book. But we want to emphasize that that is the logical conclusion of the ideas we have discussed. And the first step in that process is to abandon the notion of a popular vote as the proper way to adjudicate issues that require expert knowledge. The challenge is to find new mechanisms for aggregating values that capture the ideals of democracy, without holding us all hostage to ignorance and manipulation. This content downloaded from 142.103.160.110 on Wed, 08 Apr 2020 15:24:59 UTC All use subject to https://about.jstor.org/terms","libVersion":"0.3.2","langs":""}